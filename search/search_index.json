{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"An open, automated, and frictionless machine learning environment. <p> \u00a0 1000s of data sets, uniformly formatted, easy to load, organized online</p> <p> \u00a0Models and pipelines automatically uploaded from machine learning libraries</p> <p> Extensive APIs to integrate OpenML into your tools and scripts</p> <p>  Easily reproducible results (e.g. models, evaluations) for comparison and reuse</p> <p>\u00a0 Stand on the shoulders of giants, and collaborate in real time</p> <p>\u00a0 Make your work more visible and reusable</p> <p>\u00a0 Built for automation: streamline your experiments and model building</p>"},{"location":"#installation","title":"Installation","text":"<p>The OpenML package is available in many languages and across libraries. For more information about them, see the Integrations page.</p> Python/sklearnPytorchKerasTensorFlowRJuliaRUST.Net <ul> <li>Python/sklearn repository</li> <li><code>pip install openml</code></li> </ul> <ul> <li>Pytorch repository</li> <li><code>pip install openml-pytorch</code></li> </ul> <ul> <li>Keras repository</li> <li><code>pip install openml-keras</code></li> </ul> <ul> <li>TensorFlow repository</li> <li><code>pip install openml-tensorflow</code></li> </ul> <ul> <li>R repository</li> <li><code>install.packages(\"mlr3oml\")</code></li> </ul> <ul> <li>Julia repository</li> <li><code>using Pkg;Pkg.add(\"OpenML\")</code></li> </ul> <ul> <li>RUST repository</li> <li>Install from source</li> </ul> <ul> <li>.Net repository</li> <li><code>Install-Package openMl</code></li> </ul> <p>You might also need to set up the API key. For more information, see the API key page</p>"},{"location":"#learning-openml","title":"Learning OpenML","text":"<p>Aside from the individual package documentations, you can learn more about OpenML through the following resources: The core concepts of OpenML are explained in the Concepts page. These concepts include the principle behind using Datasets, Runs, Tasks, Flows, Benchmarking and much more. Going through them will help you leverage OpenML even better in your work.</p>"},{"location":"#contributing-to-openml","title":"Contributing to OpenML","text":"<p>OpenML is an open source project, hosted on GitHub. We welcome everybody to help improve OpenML, and make it more useful for everyone. For more information on how to contribute, see the Contributing page.</p> <p>We want to make machine learning and data analysis simple, accessible, collaborative and open with an optimal division of labour between computers and humans.</p>"},{"location":"#want-to-get-involved","title":"Want to get involved?","text":"<p>Awesome, we're happy to have you! </p> <p>OpenML is dependent on the community. If you want to help, please email us (openmlHQ@googlegroups.com). If you feel already comfortable you can help by opening issues or make a pull request on GitHub. We also have regular workshops you can join (they are announced on openml.org).</p>"},{"location":"APIs/","title":"APIs","text":"<p>This page has moved. Please see the new API documentation.</p>"},{"location":"Basic-Concepts/","title":"Basic Concepts","text":"<p>Researchers are encouraged to upload their experimental results on OpenML, so that these can be reused by anyone. Various high level papers  have been published that overview the design goals, benefits and opportunities (for example, at ECML/PKDD 2013, SIGKDD Explorations and JMLR). However, there is no clear overview of the basic concepts upon which the platform is build. In this blog post I will review these, and discuss some best practices.\u00a0This page is a slightly updated version of this blogpost</p>"},{"location":"Basic-Concepts/#data","title":"Data","text":"<p>One of the core components of OpenML are datasets. People can upload their datasets, and the system automatically organises these on line. An example of a dataset is the well-known Iris dataset. It shows all features, once of these is identified as the 'default target attribute', although this concept is flexible. It also shows some automatically computed data qualities (or, meta-features). Each dataset has its own unique ID.</p> <p>Information about the dataset, the data features and the data qualities can be obtained automatically by means of the following API functions:</p> <ul> <li>Get all available datasets</li> <li>Get dataset (required the data id)</li> <li>Get data features (requires the data id)</li> <li>Get data qualities (requires the data id)</li> </ul>"},{"location":"Basic-Concepts/#task-types-and-tasks","title":"Task types and\u00a0tasks","text":"<p>A dataset alone does not constitute a scientific task. We must first agree on what types of results are expected to be shared. This is expressed in task types: they define what types of inputs are given, which types of output are expected to be returned, and what protocols should be used. For instance, classification tasks should include well-defined cross-validation procedures, labelled input data, and require predictions as outputs. The collection of all this information together is called a task. The Iris dataset has various tasks defined on it, for example this one. Although the web-interface does not show it, this task formally describes the target attribute that should be modelled (in this case the same as the default target attribute of the dataset, but this is flexible), the quality estimation procedure (10-fold cross-validation), the evaluation measure (predictive accuracy) and the cross-validation folds.</p> <p>Useful API operations include:</p> <ul> <li>Get all available tasks</li> <li>Get all available tasks of a given type (e.g. get all Classification tasks, requires the id of the task type)</li> <li>Get the details of a task (requires task id)</li> </ul> <p>Currently, there are a wide range of task types defined on OpenML, including classification, regression, on line learning, clustering and subgroup discovery. Although this set can be extended, this is currently not a supported API operation (meaning that we will add them by hand). If you interested in task types that are currently not supported, please contact us.\u00a0</p>"},{"location":"Basic-Concepts/#flows","title":"Flows","text":"<p>Tasks can be 'solved' by classifiers (or algorithms, workflows, flows). OpenML stores references to these flows. It is important to stress that flows are actually ran on the computer of the user, only meta-information about the flow is stored on OpenML. This  information includes basic trivialities such as the creator, toolbox and compilation instructions, but also more formal description about hyper parameter. A flow can also contain subflows, for example, the flow Bagging can have a subflow 'Decision Tree' which would make the flow 'Bagging of Decision Trees'. A flow is distinguished by its name and 'external version', which are both provided by the uploader. When uploading a flow, it is important to think about a good naming convention for the both, for example, the git commit number could be used as external version, as this uniquely identifies a state of the code. Ideally, when two persons are using the same flow, they will use the same name and external version, so that results of the flows can be compared across tasks. (This is ensured when using the toolboxed in which OpenML is integrated, such as Weka, Scikit Learn and MLR).</p> <p>Useful API functions are:</p> <ul> <li>List all flows </li> <li>List all my flows</li> <li>Give details about a given flow (requires flow id)</li> </ul>"},{"location":"Basic-Concepts/#runs","title":"Runs","text":"<p>Whenever a flow executes a task, this is called a run. The existence of runs is actually the main contribution of OpenML. Some experiments take weeks to complete, and having the results stored on OpenML helps other researchers reuse the experiments. The task description specifies which information should be uploaded in order to have a valid run, in most cases, for each cross-validation fold the predictions on the test set. This allows OpenML to calculate basic evaluation measures, such as predictive accuracy, ROC curves and many more. Also information about the flow and hyper parameter settings should be provided.</p> <p>Some useful API functions:</p> <ul> <li>List all runs performed on a given task (requires task id, e.g., the iris task is 59)</li> <li>Compare two flows on all tasks (requires a comma separated list of flow ids, e.g., 1720, 1721 for comparing k-nn with a decision tree)</li> <li>And many more\u00a0...</li> </ul> <p>Usually, the result is in some XML or JSON format (depending on the preference of the user), linking together various task ids, flow ids, etc. In order for this to become meaningful, the user needs to perform other API tasks to get information about what flows were executed, what tasks and datasets were used, etc. Details about this will be provided in another post.</p>"},{"location":"Basic-Concepts/#setups","title":"Setups","text":"<p>Every run that is executed by a flow, contains information about the hyper parameter settings of the flow. A setup is the combination of all parameter settings of a given flow. OpenML internally links the result of a given run to a setup id. This way, experiments can be done across hyper parameter settings.</p> <p>For example,  </p> <ul> <li>Compare two setups on all tasks (requires a comma separated list of setup ids, e.g., 8994, 8995, 8996 for comparing multiple MLP configurations)</li> </ul> <p>As setups constitute a complex concept, most of the operations concerning setups are hidden from the user. Hence, not all setup functions are properly documented yet. For example, these do not contain a page on the webinterface.</p>"},{"location":"Data-collections/","title":"Data collections","text":"<p>This website is supposed to gather and explain curated lists of OpenML datasets.</p>"},{"location":"Data-collections/#efficient-and-robust-automated-machine-learning-feurer-et-al-nips-2015","title":"Efficient and Robust Automated Machine Learning - Feurer et al. - NIPS 2015","text":"<p>Contact: @mfeurer</p> <p>Used in: Efficient and Robust Automated Machine Learning</p> <p>Datasets:</p> <pre><code>1000,1002,1018,1019,1020,1021,1036,1040,1041,1049,1050,1053,1056,1067,1068,1069,1111,1112,1114,1116,1119,1120,1128,1130,1134,1138,1139,1142,1146,1161,1166,12,14,16,179,180,181,182,184,185,18,21,22,23,24,26,273,28,293,300,30,31,32,351,354,357,36,389,38,390,391,392,393,395,396,398,399,3,401,44,46,554,57,60,679,6,715,718,720,722,723,727,728,734,735,737,740,741,743,751,752,761,772,797,799,803,806,807,813,816,819,821,822,823,833,837,843,845,846,847,849,866,871,881,897,901,903,904,910,912,913,914,917,923,930,934,953,958,959,962,966,971,976,977,978,979,980,991,993,995\n</code></pre> did name n p p.num p.syms p.bin n.class minclass maxclass n.miss 29 31 credit-g 1000 21 7 13 2 2 300 700 0 570 715 fri_c3_1000_25 1000 26 25 0 0 2 443 557 0 573 718 fri_c4_1000_100 1000 101 100 0 0 2 436 564 0 578 723 fri_c4_1000_25 1000 26 25 0 0 2 453 547 0 595 740 fri_c3_1000_10 1000 11 10 0 0 2 440 560 0 598 743 fri_c1_1000_5 1000 6 5 0 0 2 457 543 0 606 751 fri_c4_1000_10 1000 11 10 0 0 2 440 560 0 651 797 fri_c4_1000_50 1000 51 50 0 0 2 440 560 0 653 799 fri_c0_1000_5 1000 6 5 0 0 2 497 503 0 660 806 fri_c3_1000_50 1000 51 50 0 0 2 445 555 0 667 813 fri_c3_1000_5 1000 6 5 0 0 2 437 563 0 691 837 fri_c1_1000_50 1000 51 50 0 0 2 453 547 0 699 845 fri_c0_1000_10 1000 11 10 0 0 2 491 509 0 703 849 fri_c0_1000_25 1000 26 25 0 0 2 497 503 0 719 866 fri_c2_1000_50 1000 51 50 0 0 2 418 582 0 755 903 fri_c2_1000_25 1000 26 25 0 0 2 437 563 0 756 904 fri_c0_1000_50 1000 51 50 0 0 2 490 510 0 762 910 fri_c1_1000_10 1000 11 10 0 0 2 436 564 0 764 912 fri_c2_1000_5 1000 6 5 0 0 2 416 584 0 765 913 fri_c2_1000_10 1000 11 10 0 0 2 420 580 0 769 917 fri_c1_1000_25 1000 26 25 0 0 2 454 546 0 262 392 oh0.wc 1003 3183 3182 0 0 10 51 194 0 535 679 rmftsa_sleepdata 1024 3 2 1 0 4 94 404 0 596 741 rmftsa_sleepdata 1024 3 1 1 0 2 509 515 0 271 401 oh10.wc 1050 3239 3238 0 0 10 52 165 0 914 1068 pc1 1109 22 21 0 0 2 77 1032 0 786 934 socmob 1156 6 1 4 2 2 256 900 0 749 897 colleges_aaup 1161 17 13 3 0 2 348 813 87 782 930 colleges_usnews 1302 35 32 2 0 2 614 688 1144 123 185 baseball 1340 18 15 2 0 3 57 1215 20 818 966 analcatdata_halloffame 1340 18 15 2 0 2 125 1215 20 896 1049 pc4 1458 38 37 0 0 2 178 1280 0 21 23 cmc 1473 10 2 7 3 3 333 629 0 119 181 yeast 1484 9 8 0 0 10 5 463 0 261 391 re0.wc 1504 2887 2886 0 0 13 11 608 0 972 1128 OVA_Breast 1545 10937 10936 1 0 2 344 1201 0 974 1130 OVA_Lung 1545 10937 10936 0 0 2 126 1419 0 978 1134 OVA_Kidney 1545 10937 10936 0 0 2 260 1285 0 982 1138 OVA_Uterus 1545 10937 10936 1 0 2 124 1421 0 983 1139 OVA_Omentum 1545 10937 10936 0 0 2 77 1468 0 986 1142 OVA_Endometrium 1545 10937 10936 0 0 2 61 1484 0 990 1146 OVA_Prostate 1545 10937 10936 0 0 2 69 1476 0 1005 1161 OVA_Colon 1545 10937 10936 0 0 2 286 1259 0 1010 1166 OVA_Ovary 1545 10937 10936 0 0 2 198 1347 0 268 398 wap.wc 1560 8461 8460 0 0 20 5 341 0 897 1050 pc3 1563 38 37 0 0 2 160 1403 0 265 395 re1.wc 1657 3759 3758 1 0 25 10 371 0 19 21 car 1728 7 0 6 0 4 65 1210 0 843 991 car 1728 7 0 6 0 2 518 1210 0 12 12 mfeat-factors 2000 217 216 0 0 10 200 200 0 14 14 mfeat-fourier 2000 77 76 0 0 10 200 200 0 16 16 mfeat-karhunen 2000 65 64 0 0 10 200 200 0 17 18 mfeat-morphological 2000 7 6 0 0 10 200 200 0 20 22 mfeat-zernike 2000 48 47 0 0 10 200 200 0 814 962 mfeat-morphological 2000 7 6 0 0 2 200 1800 0 823 971 mfeat-fourier 2000 77 76 0 0 2 200 1800 0 830 978 mfeat-factors 2000 217 216 0 0 2 200 1800 0 847 995 mfeat-zernike 2000 48 47 0 0 2 200 1800 0 872 1020 mfeat-karhunen 2000 65 64 0 0 2 200 1800 0 766 914 balloon 2001 3 2 0 0 2 482 1519 0 913 1067 kc1 2109 22 21 0 0 2 326 1783 0 627 772 quake 2178 4 3 0 0 2 969 1209 0 33 36 segment 2310 20 19 0 0 7 330 330 0 810 958 segment 2310 20 19 0 0 2 330 1980 0 259 389 fbis.wc 2463 2001 2000 0 0 17 38 506 0 263 393 la2s.wc 3075 12433 12432 1 0 6 248 905 0 592 737 space_ga 3107 7 6 0 0 2 1541 1566 0 42 46 splice 3190 62 0 61 0 3 767 1655 0 805 953 splice 3190 62 0 61 0 2 1535 1655 0 3 3 kr-vs-kp 3196 37 0 36 34 2 1527 1669 0 266 396 la1s.wc 3204 13196 13195 1 NA 6 273 943 0 888 1041 gina_prior2 3468 785 784 0 0 10 315 383 0 35 38 sick 3772 30 7 22 20 2 231 3541 3772 52 57 hypothyroid 3772 30 7 22 20 4 2 3481 3772 852 1000 hypothyroid 3772 30 7 22 20 2 291 3481 3772 724 871 pollen 3848 6 5 0 0 2 1924 1924 0 583 728 analcatdata_supreme 4052 8 7 0 0 2 971 3081 0 575 720 abalone 4177 9 7 1 0 2 2081 2096 0 41 44 spambase 4601 58 57 0 0 2 1813 2788 0 54 60 waveform-5000 5000 41 40 0 0 3 1653 1692 0 831 979 waveform-5000 5000 41 40 0 0 2 1692 3308 0 28 30 page-blocks 5473 11 10 0 0 5 28 4913 0 873 1021 page-blocks 5473 11 10 0 0 2 560 4913 0 915 1069 pc2 5589 37 36 0 0 2 23 5566 0 26 28 optdigits 5620 65 64 0 0 10 554 572 0 832 980 optdigits 5620 65 64 0 0 2 572 5048 0 120 182 satimage 6430 37 36 0 0 6 625 1531 0 701 847 wind 6574 15 14 0 0 2 3073 3501 0 960 1116 musk 6598 170 167 2 0 2 1017 5581 0 845 993 kdd_ipums_la_97-small 7019 61 33 27 8 2 2594 4425 7019 657 803 delta_ailerons 7129 6 5 0 0 2 3346 3783 0 854 1002 kdd_ipums_la_98-small 7485 56 16 39 8 2 791 6694 7369 211 300 isolet 7797 618 617 0 0 26 298 300 0 22 24 mushroom 8124 23 0 22 4 2 3916 4208 2480 590 735 cpu_small 8192 13 12 0 0 2 2477 5715 0 607 752 puma32H 8192 33 32 0 0 2 4064 4128 0 616 761 cpu_act 8192 22 21 0 0 2 2477 5715 0 661 807 kin8nm 8192 9 8 0 0 2 4024 4168 0 670 816 puma8NH 8192 9 8 0 0 2 4078 4114 0 687 833 bank32nh 8192 33 32 0 0 2 2543 5649 0 775 923 visualizing_soil 8641 5 3 1 1 2 3888 4753 0 870 1018 kdd_ipums_la_99-small 8844 57 15 41 9 2 568 8276 8844 902 1056 mc1 9466 39 38 0 0 2 68 9398 0 673 819 delta_elevators 9517 7 6 0 0 2 4732 4785 0 260 390 new3s.wc 9558 26833 26832 1 0 44 104 696 0 828 976 kdd_JapaneseVowels 9961 15 14 0 0 2 1614 8347 0 899 1053 jm1 10885 22 21 0 0 2 2106 8779 5 30 32 pendigits 10992 17 16 0 0 10 1055 1144 0 871 1019 pendigits 10992 17 16 0 0 2 1144 9848 0 269 399 ohscal.wc 11162 11466 11465 0 0 10 709 1621 0 24 26 nursery 12960 9 0 8 1 5 2 4320 0 811 959 nursery 12960 9 0 8 1 2 4320 8640 0 589 734 ailerons 13750 41 40 0 0 2 5828 7922 0 883 1036 sylva_agnostic 14395 217 216 0 0 2 886 13509 0 887 1040 sylva_prior 14395 109 108 0 0 2 886 13509 0 577 722 pol 15000 49 48 0 0 2 5041 9959 0 700 846 elevators 16599 19 18 0 0 2 5130 11469 0 964 1120 MagicTelescope 19020 12 11 0 0 2 6688 12332 0 6 6 letter 20000 17 16 0 0 26 734 813 0 829 977 letter 20000 17 16 0 0 2 813 19187 0 676 822 cal_housing 20640 9 8 0 0 2 8385 12255 0 677 823 houses 20640 9 8 0 0 2 8914 11726 0 675 821 house_16H 22784 17 16 0 0 2 6744 16040 0 697 843 house_8L 22784 9 8 0 0 2 6744 16040 0 122 184 kropt 28056 7 0 6 0 18 27 4553 0 963 1119 adult-census 32561 16 7 8 1 2 7841 24720 2399 582 727 2dplanes 40768 11 10 0 0 2 20348 20420 0 734 881 mv 40768 11 7 3 2 2 16447 24321 0 753 901 fried 40768 11 10 0 0 2 20341 20427 0 117 179 adult 48842 15 2 12 1 2 11687 37155 3620 955 1111 KDDCup09_appetency 50000 231 192 38 4 2 890 49110 50000 956 1112 KDDCup09_churn 50000 231 192 39 NA 2 3672 46328 50000 958 1114 KDDCup09_upselling 50000 231 192 38 4 2 3682 46318 50000 414 554 mnist_784 70000 785 784 0 0 10 6313 7877 0 241 357 vehicle_sensIT 98528 101 100 1 1 2 49264 49264 0 118 180 covertype 110393 55 14 40 40 7 1339 51682 0 196 273 IMDB.drama 120919 1002 1001 1 1 2 43779 77140 0 239 351 codrna 488565 9 8 1 1 2 162855 325710 0 206 293 covertype 581012 55 54 1 1 2 283301 297711 0 240 354 poker 1025010 11 10 1 1 2 511308 513702 0"},{"location":"Gamification/","title":"Gamification","text":"<p>Gamification is the use of game thinking and game mechanics in non-game contexts to engage users in solving problems and increase users' self contributions (definition from Wikipedia).</p> <p>Examples: * Foursquare badge list * Class badges</p> <p>In order to increase user participation and loyalty, we can include some badges to user profile. Here is a list of possible badges:</p>"},{"location":"Gamification/#datasets","title":"Datasets","text":"<ul> <li>1 dataset:  </li> <li>10 datasets:</li> <li>100 datasets:</li> <li>Submitting a dataset bigger than 1GB:</li> </ul>"},{"location":"Gamification/#tasks","title":"Tasks","text":"<ul> <li>1 task:</li> <li>10 tasks:</li> <li>100 tasks:</li> <li>100 tasks of the same type:</li> <li>Supervised Classification:</li> <li>Supervised Data Stream Classification:</li> <li>Supervised Regression: </li> <li>Clustering:</li> <li>Learning Curve:</li> <li>Machine Learning Challenge:</li> <li>Survival Analysis:</li> </ul>"},{"location":"Gamification/#flows","title":"Flows","text":"<ul> <li>1 flow:</li> <li>10 flows:</li> <li>100 flows:</li> </ul>"},{"location":"Gamification/#runs","title":"Runs","text":"<ul> <li>1 run:</li> <li>10 runs:</li> <li>100 runs:</li> <li>100 Weka flows:</li> <li>100 R flows:</li> <li>1000 runs:</li> <li>10,000 runs:</li> <li>Submitting runs during 4 consecutive days: </li> </ul>"},{"location":"Governance/","title":"Governance","text":"<p>The purpose of this document is to formalize the governance process used by the OpenML project (the OpenML GitHub organization which contains all code and projects related to OpenML.org), to clarify how decisions are made and how the various elements of our community interact. This document establishes a decision-making structure that takes into account feedback from all members of the community and strives to find consensus, while avoiding any deadlocks.</p> <p>The OpenML project is an independent open source project that is legally represented by the Open Machine Learning Foundation. The Open Machine Learning Foundation is a not-for-profit organization supporting, but not controlling, the OpenML project. The Foundation is open to engage with universities, companies, or anyone sharing the same goals. The OpenML project has a separate governance model described in this document.</p> <p>This is a meritocratic, consensus-based community project. Anyone with an interest in the project can join the community, contribute to the project design, and participate in the decision making process. This document describes how that participation takes place and how to set about earning merit within the project community.</p>"},{"location":"Governance/#roles-and-responsibilities","title":"Roles And Responsibilities","text":""},{"location":"Governance/#contributors","title":"Contributors","text":"<p>Contributors are community members who contribute in concrete ways to the project. Anyone can become a contributor, and contributions can take many forms \u2013 not only code \u2013 as detailed in the contributors guide. Contributors need to create pull requests to contribute to the code or documentation.</p>"},{"location":"Governance/#core-contributors","title":"Core contributors","text":"<p>Core contributors are community members who have shown that they are dedicated to the continued development of the project through ongoing engagement with the community. They have shown they can be trusted to maintain OpenML with care. Being a core contributor allows contributors to more easily carry on with their project related activities by giving them write access to the project\u2019s repository (abiding by the decision making process described below, e.g. merging pull requests that obey the decision making procedure described below) and is represented as being an organization member on the OpenML GitHub organization. Core contributors are expected to review code contributions, can merge approved pull requests, can cast votes for and against merging a pull-request, and can be involved in deciding major changes to the API.</p> <p>New core contributors can be nominated by any existing core contributors. Once they have been nominated, there will be a vote in the private OpenML core email list by the current core contributors. While it is expected that most votes will be unanimous, a two-thirds majority of the cast votes is enough. The vote needs to be open for at least 1 week.</p> <p>Core contributors that have not contributed to the project (commits or GitHub comments) in the past 12 months will become emeritus core contributors and recant their commit and voting rights until they become active again. The list of core contributors, active and emeritus (with dates at which they became active) is public on the OpenML website.</p>"},{"location":"Governance/#steering-committee","title":"Steering Committee","text":"<p>The Steering Committee (SC) members are core contributors who have additional responsibilities to ensure the smooth running of the project. SC members are expected to participate in strategic planning, join monthly meetings, and approve changes to the governance model. The purpose of the SC is to ensure a smooth progress from the big-picture perspective. Indeed, changes that impact the full project require a synthetic analysis and a consensus that is both explicit and informed. In cases that the core contributor community (which includes the SC members) fails to reach such a consensus in the required time frame, the SC is the entity to resolve the issue.</p> <p>The SC consists of community representatives and partner representatives. Community representatives of the SC are nominated by a core contributor. A nomination will result in a discussion that cannot take more than a month and then a vote by the core contributors which will stay open for a week. SC membership votes are subject to a two-third majority of all cast votes as well as a simple majority approval of all the current SC members.</p> <p>Partner institutions who enter a collaboration agreement or sponsorship agreement with the OpenML Foundation can nominate a representative on the Steering Committee, if so agreed in the agreement. Such a collaboration should in principle include one full-time developer to work on OpenML (in cash or in kind) for the duration of the agreement. New partner representatives have to be confirmed by the SC following the same voting rules above.</p> <p>The OpenML community must have at least equal footing in the steering committee. Additional SC members may be nominated to ensure this, following the membership voting rules described above.</p> <p>When decisions are escalated to the steering committee (see the decision making process below), and no consensus can be found within a month, the SC can meet and decide by consensus or with a simple majority of all cast votes.</p> <p>SC members who do not actively engage with the SC duties are expected to resign.</p> <p>The initial Steering Committee of OpenML consists of Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Heidi Seibold, Jan van Rijn, and Joaquin Vanschoren. They all represent the OpenML community.</p>"},{"location":"Governance/#decision-making-process","title":"Decision Making Process","text":"<p>Decisions about the future of the project are made through discussion with all members of the community. All non-sensitive project management discussion takes place on the project contributors\u2019 mailing list and the issue trackers of the sub-projects. Occasionally, sensitive discussion occurs on the private core developer email list (see below). This includes voting on core/SC membership or discussion of internal disputes. All discussions must follow the OpenML honor code.</p> <p>OpenML uses a \u201cconsensus seeking\u201d process for making decisions. The group tries to find a resolution that has no open objections among core contributors. At any point during the discussion, any core contributors can call for a vote, which will conclude one month from the call for the vote, or when two thirds of all votes are in favor.</p> <p>If no option can gather two thirds of the votes cast (ignoring abstentions), the decision is escalated to the SC, which in turn will use consensus seeking with the fallback option of a simple majority vote if no consensus can be found within a month. This is what we hereafter may refer to as \u201cthe decision making process\u201d. It applies to all core OpenML repositories.</p> <p>Decisions (in addition to adding core contributors and SC membership as above) are made according to the following rules:</p> <p>Normal changes:  </p> <ul> <li>Minor Documentation changes, such as typo fixes, or addition / correction of a sentence: requires one approved review by a core contributor, and no objections in the comments (lazy consensus). Core contributors are expected to give \u201creasonable time\u201d to others to give their opinion on the pull request if they\u2019re not confident others would agree.  </li> <li>Non-server packages that only have one core contributor are not subject to the ruling in the bullet point above (i.e. a sole core developer can make decisions on their own).</li> </ul> <p>Major changes:  </p> <ul> <li>Major changes to the API principles and metadata schema require a concrete proposal outlined in an OpenML Request for Comments (RfC), which has to be opened for public consultation for at least 1 month. The final version has to be approved using the decision-making process outlined above (two-third of the cast vote by core contributors or simple majority if escalated to the SC). Voting is typically done as a comment in the pull request (+1, -1, or 0 to abstain).  </li> <li>RfCs must be announced and shared via the public mailing list and may link additional content (such as blog posts or google docs etc. detailing the changes).  </li> <li>Changes to the governance model use the same decision process outlined above.  </li> </ul> <p>If a veto -1 vote is cast on a lazy consensus, the proposer can appeal to the community and core contributors and the change can be approved or rejected using the decision making procedure outlined above.</p>"},{"location":"Governance/#communication-channels","title":"Communication channels","text":"<p>OpenML uses the following communication channels:  </p> <ul> <li>An open contributor mailing list and the GitHub issue trackers.  </li> <li>A chat application for daily interaction with the community (currently Slack).  </li> <li>Private email lists (without archive) for the core developers (core@openml.org) and steering committee (steering@openml.org), for membership voting and sensitive discussions.  </li> <li>Biyearly Steering Committee meeting at predefined times, listed on the website, and asynchronous discussions on a discussion board. They are open to all steering committee members and core contributors, and they can all request discussion on a topic. Closed meetings for SC members only can be called in if there are sensitive discussions or other valid reasons.  </li> <li>A monthly Engineering meeting at predefined times, listed on the website. The meeting is open to all. Discussion points are put on the [project roadmap]  (https://github.com/orgs/openml/projects/2).</li> </ul>"},{"location":"Helper-functions/","title":"Helper functions","text":"<p>Mostly written in Java, these functions build search indexes, compute dataset characteristics, generate tasks and evaluate the results of certain tasks.</p>"},{"location":"Helper-functions/#code","title":"Code","text":"<p>The Java code is available in the 'OpenML' repository: https://github.com/openml/OpenML/tree/master/Java</p>"},{"location":"Helper-functions/#components","title":"Components","text":"<p>General: - OpenML: Building Lucene search index and smaller tools, e.g. extracting documentation from WEKA source files and ARFF files - generateApiDocs: Generates API HTML Documentation - http_post_file: Example how to post files to the api using Java.</p> <p>Support for tasks: - foldgeneration: Java code for generating cross-validation folds. Can be used from command line. - splitgeneration: Split generator for cross validation and holdout. Unsure what's the difference with the previous? - generate_predictions: Helper class to build prediction files based on WEKA output. Move to WEKA repository? - evaluate_predictions: The evaluation engine computing evaluation scores based on submitted predictions</p>"},{"location":"Home/","title":"Home","text":""},{"location":"Home/#openml-components","title":"OpenML Components","text":"<p>To make development easier, OpenML has been subdivided into several subprojects with their own repositories, wikis, and issue trackers: * Website itself and API services: https://github.com/openml/website * Java library for interfacing with the OpenML API: https://github.com/openml/java * R package for interfacing with the OpenML API: https://github.com/openml/r * Python module for interfacing with the OpenML API (stub): https://github.com/openml/python * WEKA plugin: https://github.com/openml/weka * RapidMiner plugin: https://github.com/openml/rapidminer * KNIME plugin: https://github.com/openml/knime</p>"},{"location":"Home/#suggestions-for-further-integrations","title":"Suggestions for further integrations","text":"<ul> <li>We need more data. Other people made efforts for hosting and selecting ML data already.  [[Data-Repositories]] lists them. List must be extended and we need to check how much we already have integrated.</li> </ul>"},{"location":"Home/#local-installation-of-openml","title":"Local installation of OpenML","text":"<p>Developers who are working on new features may need a [[Local Installation]] for testing purposes. </p>"},{"location":"Home/#backend-development","title":"Backend development","text":"<p>The website is built using a PHP/Java backend and a PHP/javascript frontend. </p> <p>An overview: * [[Web APP|WebApp-(PHP)]]: The high-level architecture of the website, including the controllers for different parts of the website (REST API, html, ...) and connections to the database. * [[Helper functions]]: Mostly written in Java, these functions build search indexes, compute dataset characteristics, generate tasks and evaluate the results of certain tasks. * [[URL Mapping]] A guide to the basics how a URL maps to internal files. </p>"},{"location":"IM-accounts/","title":"IM accounts","text":""},{"location":"IM-accounts/#skype","title":"Skype","text":"<p>Joaquin: joaquin.vanschoren Jan: Zwaaikom  Venkatesh: lilacsunbird Luis Torgo: ltorgo Bernd Bischl: bernd.bischl</p>"},{"location":"IM-accounts/#google","title":"Google","text":"<p>Joaquin: joaquin.vanschoren@gmail.com Jan: janvanrijn@gmail.com  Venkatesh: lilacsunbird Luis Torgo: ltorgo@gmail.com Bernd Bischl: bernd.bischl@gmail.com</p>"},{"location":"Job-Scheduling/","title":"Job Scheduling","text":"<p>In many cases, a scientist could be interested in running a wide range of flows over a range of datasets (or tasks). For this purpose, a job scheduler has been implemented. OpenML maintains a list of (setup,task) tuples, that users requested to run. A setup is a flow with information about all parameter settings.</p> <p>Flows can be linked to a specific workbench. For example, the flows weka.J48(1) (which has id 60) and weka.SMO_PolyKernel(1) (which has id 70) are linked to Weka_3.7.10. When the same algorithm is uploaded from out another Weka version, a new version of the implementation is registered at OpenML. For example, if we would upload weka.J48 from the old version weka_3.7.5, OpenML would register the implementation as weka.J48(2). (In fact, this is what actually happend. See implementation id 100.)</p> <p>In order to schedule jobs, go to the job scheduler (Alpha version, no guarantees). Select a task type, and give the experiment a name. It is important to filter the tasks and setups using the menu on the left, since the overview can be huge. Note that there are a some tasks on \"big datasets\", which can slow down the experimentation proces. Be considerate including those. Also make sure to filter on setups with flows of your own workbench version. If these are not in the system yet, register these on the share page. </p> <p>[OpenML Weka] The OpenML weka package can be used to automatically execute the scheduled jobs. Run it with the following command: </p> <p>java -cp openmlweka.jar org.openml.weka.experiment.RunJob -T 3 -N 1000</p> <p>With T being the task type to execute, and N the number of runs you want to perform. (You can set this number as high as you like, the experimenter will stop as soon as there are no jobs left.) Note that this experimenter will only execute jobs that are of this Weka version. </p> <p>The OpenML Weka package can automatically register implementations on OpenML. Using the GUI, execute an OpenML task with all the implementations that you want to be registered. Before running those, these will automatically be registered on OpenML. </p>"},{"location":"OpenML-Docs/","title":"OpenML Docs","text":""},{"location":"OpenML-Docs/#general-documentation","title":"General Documentation","text":"<p>The general documentation (the one you are reading now) in written in MarkDown, can be easily edited by clicking the edit button (the pencil icon) on the top of every page. It will open up an editing page on GitHub (you do need to be logged in on GitHub). When you are done, add a small message explaining the change and click 'commit changes'. On the next page, just launch the pull request. We will then review it and approve the changes, or discuss them if necessary.</p> <p>The sources are generated by MkDocs, using the Material theme. Check these docs to see what is possible in terms of styling.</p> <p>Deployment</p> <p>To deploy the documentation manually, you need to have MkDocs and MkDocs-Material installed: <pre><code>pip install mkdocs\npip install mkdocs-material\npip install fontawesome_markdown\n</code></pre> To deploy the documentation locally, run <code>mkdocs serve</code> in the top directory (with the <code>mkdocs.yml</code> file). Any changes made after that will be hot-loaded.</p> <p>The documentation will be auto-deployed with every push or merge with the master branch of <code>https://www.github.com/openml/docs/</code>. In the background, a CI job will run <code>mkdocs gh-deploy</code>, which will build the HTML files and push them to the gh-pages branch of openml/docs. <code>https://docs.openml.org</code> is just a reverse proxy for <code>https://openml.github.io/docs/</code>.</p>"},{"location":"OpenML-Docs/#rest-api","title":"REST API","text":"<p>The REST API is documented using Swagger.io, in YAML. This generates a nice web interface that also allows trying out the API calls using your own API key (when you are logged in).</p> <p>You can edit the sources on SwaggerHub. When you are done, export to json and replace the downloads/swagger.json file in the OpenML main GitHub repository. You need to do a pull request that is then reviewed by us. When we merge the new file the changes are immediately available.</p> <p>The data API can be edited in the same way.</p>"},{"location":"OpenML-Docs/#python-api","title":"Python API","text":"<p>To edit the tutorial, you have to edit the <code>reStructuredText</code> files on openml-python/doc. When done, you can do a pull request.</p> <p>To edit the documentation of the python functions, edit the docstrings in the Python code. When done, you can do a pull request.</p> <p>Note</p> <p>Developers: A CircleCI job will automatically render the documentation on every GitHub commit, using Sphinx.</p>"},{"location":"OpenML-Docs/#r-api","title":"R API","text":"<p>To edit the tutorial, you have to edit the <code>Rmarkdown</code> files on openml-r/vignettes.</p> <p>To edit the documentation of the R functions, edit the Roxygen documention next to the functions in the R code.</p> <p>Note</p> <p>Developers: A Travis job will automatically render the documentation on every GitHub commit, using knitr. The Roxygen documentation is updated every time a new version is released on CRAN.</p>"},{"location":"OpenML-Docs/#java-api","title":"Java API","text":"<p>The Java Tutorial is written in markdown and can be edited the usual way (see above).</p> <p>To edit the documentation of the Java functions, edit the documentation next to the functions in the Java code.</p> <ul> <li>Javadocs: https://www.openml.org/docs/</li> </ul> <p>Note</p> <p>Developers: A Travis job will automatically render the documentation on every GitHub commit, using Javadoc.</p>"},{"location":"REST-tutorial/","title":"REST tutorial","text":"<p>OpenML offers a RESTful Web API, with predictive URLs, for uploading and downloading machine learning resources. Try the API Documentation to see examples of all calls, and test them right in your browser.</p>"},{"location":"REST-tutorial/#getting-started","title":"Getting started","text":"<ul> <li>REST services can be called using simple HTTP GET or POST actions.</li> <li>The REST Endpoint URL is <code>https://www.openml.org/api/v1/</code></li> <li>The default endpoint returns data in XML. If you prefer JSON, use the endpoint <code>https://www.openml.org/api/v1/json/</code>. Note that, to upload content, you still need to use XML (at least for now).</li> </ul>"},{"location":"REST-tutorial/#testing","title":"Testing","text":"<p>For continuous integration and testing purposes, we have a test server offering the same API, but which does not affect the production server.</p> <ul> <li>The test server REST Endpoint URL is <code>https://test.openml.org/api/v1/</code></li> </ul>"},{"location":"REST-tutorial/#error-messages","title":"Error messages","text":"<p>Error messages will look like this:</p> <pre><code>&lt;oml:error xmlns:oml=\"http://openml.org/error\"&gt;\n&lt;oml:code&gt;100&lt;/oml:code&gt;\n&lt;oml:message&gt;Please invoke legal function&lt;/oml:message&gt;\n&lt;oml:additional_information&gt;Additional information, not always available.&lt;/oml:additional_information&gt;\n&lt;/oml:error&gt;\n</code></pre> <p>All error messages are listed in the API documentation. E.g. try to get a non-existing dataset:</p> <ul> <li>in XML: https://www.openml.org/api_new/v1/data/99999</li> <li>in JSON: https://www.openml.org/api_new/v1/json/data/99999</li> </ul>"},{"location":"REST-tutorial/#examples","title":"Examples","text":"<p>You need to be logged in for these examples to work.</p>"},{"location":"REST-tutorial/#download-a-dataset","title":"Download a dataset","text":"<ul> <li>User asks for a dataset using the /data/{id} service. The <code>dataset id</code> is typically part of a task, or can be found on OpenML.org.</li> <li>OpenML returns a description of the dataset as an XML file (or JSON). Try it now</li> <li>The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.</li> <li>The dataset is returned by the server hosting the dataset. This can be OpenML, but also any other data repository. Try it now</li> </ul>"},{"location":"REST-tutorial/#download-a-flow","title":"Download a flow","text":"<ul> <li>User asks for a flow using the /flow/{id} service and a <code>flow id</code>. The <code>flow id</code> can be found on OpenML.org.</li> <li>OpenML returns a description of the flow as an XML file (or JSON). Try it now</li> <li>The flow description contains the URL where the flow can be downloaded (e.g. GitHub), either as source, binary or both, as well as additional information on history, dependencies and licence. The user calls the right URL to download it.</li> <li>The flow is returned by the server hosting it. This can be OpenML, but also any other code repository. Try it now</li> </ul>"},{"location":"REST-tutorial/#download-a-task","title":"Download a task","text":"<ul> <li>User asks for a task using the /task/{id} service and a <code>task id</code>. The <code>task id</code> is typically returned when searching for tasks.</li> <li>OpenML returns a description of the task as an XML file (or JSON). Try it now</li> <li>The task description contains the <code>dataset id</code>(s) of the datasets involved in this task. The user asks for the dataset using the /data/{id} service and the <code>dataset id</code>.</li> <li>OpenML returns a description of the dataset as an XML file (or JSON). Try it now</li> <li>The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.</li> <li>The dataset is returned by the server hosting it. This can be OpenML, but also any other data repository. Try it now</li> <li>The task description may also contain links to other resources, such as the train-test splits to be used in cross-validation. The user calls that URL to download the train-test splits.</li> <li>The train-test splits are returned by OpenML. Try it now</li> </ul>"},{"location":"altmetrics/","title":"Altmetrics","text":"<p>To encourage open science, OpenML now includes a score system to track and reward scientific activity, reach and impact, and in the future will include further gamification features such as badges. Because the system is still experimental and very much in development, the details are subject to change. Below, the score system is described in more detailed followed by our rationale for this system for those interested. If anything is unclear or you have any feedback of the system do not hesitate to let us know.</p>"},{"location":"altmetrics/#the-scores","title":"The scores","text":"<p>All scores are awarded to users and involve datasets, flows, tasks and runs, or knowledge pieces in short.</p> Activity  <p>Activity score is awarded to users for contributing to the knowledge base of OpenML. This includes uploading knowledge pieces, leaving likes and downloading new knowledge pieces. Uploads are rewarded strongest, with 3 activity, followed by likes, with 2 activity, and downloads are rewarded the least, with 1 activity.</p> Reach  <p>Reach score is awarded to knowledge pieces and by extension their uploaders for the expressed interest of other users. It is increased by 2 for every user that leaves a like on a knowledge piece and increased by 1 for every user that downloads it for the first time.</p> Impact  <p>Impact score is awarded to knowledge pieces and by extension their uploaders for the reuse of these knowledge pieces. A dataset is reused if when it is used as input in a task while flows and tasks are reused in runs. 1 Impact is awarded for every reuse by a user that is not the uploader. Impact of a reused knowledge piece is further increased by half of the acquired reach and half of the acquired impact of a reuse, usually rounded down. So the impact of a dataset that is used in a single task with reach 10 and impact 5, is 8 (\u230a1+0.5*10+0.5*5 \u230b).</p>"},{"location":"altmetrics/#the-rationale","title":"The rationale","text":"<p>One of OpenML's core ideas is to create an open science environment for sharing and exploration of knowledge while getting credit for your work. The activity score serves the encouragement of sharing and exploration. Reach makes exploration easier (by finding well liked, and/or often downloaded knowledge pieces), while also providing a form of credit to the user. Impact is another form of credit that is closer in concept to citation scores.</p>"},{"location":"altmetrics/#where-to-find-it","title":"Where to find it","text":"<p>The number of likes and downloads as well as the reach and impact of knowledge pieces can be found on the top of their respective pages, for example the Iris data set. In the top right you will also find the new Like button next to the already familiar download button.</p> <p>When searching for knowledge pieces on the search page, you will now be able to see the statistics mentioned above as well. In addition you can sort the search results on their downloads, likes, reach or impact.</p> <p>On user profiles you will find all statistics relevant to that user, as well as graphs of their progress on the three scores.</p>"},{"location":"altmetrics/#badges","title":"Badges","text":"<p>Badges are intended to provide discrete goals for users to aim for. They are only in a conceptual phase, depending on the community's reaction they will be further developed.</p> <p>The badges a user has acquired can be found on their user profile below the score graphs. The currently implemented badges are:</p> Clockwork Scientist  For being active every day for a period of time. Team Player  For collaborating with other users; reusing a knowledge piece of someone who has reused a knowledge piece of yours. Good News Everyone  For achieving a high reach on singular knowledge piece you uploaded."},{"location":"altmetrics/#downvotes","title":"Downvotes","text":"<p>Although not part of the scores, downvotes have also been introduced. They are intended to indicate a flaw of a data set, flow, task or run that can be fixed, for example a missing description.</p> <p>If you want to indicate something is wrong with a knowledge piece, click the number of issues statistic at the top the page. A panel will open where you either agree with an already raised issue anonymously or submit your own issue (not anonymously).</p> <p>You can also sort search results by the number of downvotes, or issues on the search page.</p>"},{"location":"altmetrics/#opting-out","title":"Opting out","text":"<p>If you really do not like the altmetrics you can opt-out by changing the setting on your profile. This hides your scores and badges from other users and hides their scores and badges from you. You will still be able to see the number of likes, downloads and downvotes on knowledge pieces, and your likes, downloads and downvotes will still be counted.</p>"},{"location":"benchmark/","title":"Benchmarking suites","text":"<p>Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. OpenML aims to facilitate the creation of curated, comprehensive suites of machine learning tasks, covering precise sets of conditions.</p> <p>Seamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier:  </p> <ul> <li> <p>all datasets are uniformly formatted in standardized data formats </p> </li> <li> <p>they can be easily downloaded programmatically through APIs and client libraries</p> </li> <li> <p>they come with machine-readable meta-information, such as the occurrence of missing values, to train algorithms correctly</p> </li> <li> <p>standardized train-test splits are provided to ensure that results can be objectively compared</p> </li> <li> <p>results can be shared in a reproducible way through the APIs</p> </li> <li> <p>results from other users can be easily downloaded and reused</p> </li> </ul>"},{"location":"benchmark/#software-interfaces","title":"Software interfaces","text":"<p>To use OpenML Benchmark suites, you can use bindings in several programming languages. These all interface with the OpenML REST API. The default endpoint for this is <code>https://www.openml.org/api/v1/</code>, but this can change when later versions of the API are released. To use the code examples below, you only need a recent version of one of the following libraries:</p> <ul> <li>OpenML Java ApiConnector (version <code>1.0.22</code> and up).</li> <li>OpenML Weka (version <code>0.9.6</code> and up). This package adds a Weka Integration.</li> <li>OpenML Python (version <code>0.9.0</code> and up)</li> <li>OpenML R (version <code>1.8</code> and up)</li> </ul>"},{"location":"benchmark/#using-openml-benchmark-suites","title":"Using OpenML Benchmark Suites","text":"<p>Below are walk-through instructions for common use cases, as well as code examples. These illustrations use the reference OpenML-CC18 benchmark suite, but you can replace it with any other benchmark suite. Note that a benchmark suite is a set of OpenML <code>tasks</code>, which envelop not only a specific dataset, but also the train-test splits and (for predictive tasks) the target feature.</p> Terminology and current status <p>Benchmark suites are sets of OpenML tasks that you can create and manage yourself. At the same time, it is often useful to also share the set of experiments (runs) with the ensuing benchmarking results. For legacy reasons, such sets of tasks or runs are called <code>studies</code> in the OpenML REST API. In the OpenML bindings (Python, R, Java,...) these are called either <code>sets</code> or <code>studies</code>.</p> <p>When benchmarking, you will probably use two types of sets:</p> <ul> <li>Sets of tasks. These can be created, edited, downloaded or deleted via the OpenML API. Website forms will be added soon. Also the set of underlying datasets can be easily retrieved via the API.</li> <li>Sets of runs. Likewise, these can be created, edited, downloaded or deleted via the OpenML API. On the website, these are currently simply called 'studies'. Also the set of underlying tasks, datasets and flows can be easily retrieved. It is possible to link a set of runs to a benchmark study, aimed to collect future runs on that specific set of tasks. Additional information on these will be provided in a separate page.</li> </ul>"},{"location":"benchmark/#listing-the-benchmark-suites","title":"Listing the benchmark suites","text":"<p>The current list of benchmark suites is explicitly listed on the bottom of this page. The list of all sets of tasks can also be fetched programmatically. This list includes the suite's ID (and optionally an alias), which can be used to fetch further details.</p> <p>Via the REST API, the list is returned in XML or JSON</p> REST <p>https://www.openml.org/api/v1/xml/study/list/main_entity_type/task/status/all</p> <p>Check out the API docs</p> Python example <pre><code>import openml\n\n# using the main entity type task, only benchmark suites are returned\n# each benchmark suite has an ID, some also have an alias. These can be\n# used to obtain the full details. \nstudies = openml.study.list_suites(status = 'all')\n</code></pre> Java example <pre><code>public void listBenchmarksuites() throws Exception {\n    OpenmlConnector openml = new OpenmlConnector();\n    Map&lt;String, String&gt; filters = new TreeMap&lt;String, String&gt;();\n    filters.put(\"status\", \"all\");\n    filters.put(\"main_entity_type\", \"task\");\n    filters.put(\"limit\", \"20\");\n    StudyList list = openml.studyList(filters);\n}\n</code></pre> R example <pre><code>studies = listOMLStudies()\n</code></pre>"},{"location":"benchmark/#fetching-details","title":"Fetching details","text":"<p>Using the ID or alias of a benchmark suite, you can retrieve a description and the full list of tasks and the underlying datasets.</p> <p>Via the REST API, a list of all tasks and dataset IDs is returned in XML or JSON</p> REST <p>https://www.openml.org/api/v1/xml/study/OpenML-CC18</p> <p>Check out the API docs</p> <p>In Python, the data is returned as <code>features, targets</code> numpy arrays:</p> Python example <pre><code>import openml\n\nbenchmark_suite = openml.study.get_suite('OpenML-CC18') # obtain the benchmark suite\n\nfor task_id in benchmark_suite.tasks:  # iterate over all tasks\n    task = openml.tasks.get_task(task_id)  # download the OpenML task\n    features, targets = task.get_X_and_y()  # get the data\n</code></pre> <p>In Java, the data is returned as a WEKA Instances object:</p> Java example <pre><code>public void downloadDatasets() throws Exception {\n    OpenmlConnector openml = new OpenmlConnector();\n    Study benchmarksuite = openml.studyGet(\"OpenML-CC18\", \"tasks\");\n    for (Integer taskId : benchmarksuite.getTasks()) { // iterate over all tasks\n        Task t = openml.taskGet(taskId); // download the OpenML task\n        // note that InstanceHelper is part of the OpenML-weka package\n        Instances d = InstancesHelper.getDatasetFromTask(openml, t); // obtain the dataset\n    }\n}\n</code></pre> <p>In R, the data is returned as an R dataframe:</p> R example <pre><code>library(OpenML)\ntask.ids = getOMLStudy('OpenML-CC18')$tasks$task.id # obtain the list of suggested tasks\nfor (task.id in task.ids) { # iterate over all tasks\n  task = getOMLTask(task.id) # download single OML task\n  data = as.data.frame(task) # obtain raw data set\n</code></pre>"},{"location":"benchmark/#running-and-sharing-benchmarks","title":"Running and sharing benchmarks","text":"<p>The code below demonstrates how OpenML benchmarking suites can be conveniently imported for benchmarking using the Python, Java and R APIs.</p> <p>First, the list of tasks is downloaded as already illustrated above. Next, a specific algorithm (or pipeline) can be run on each of them. The OpenML API will automatically evaluate the algorithm using the pre-set train-test splits and store the predictions and scores in a run object. This run object can then be immediately published, pushing the results to the OpenML server, so that they can be compared against all others on the same benchmark set. Uploading results requires an OpenML API key, which can be found in your account details after logging into the OpenML website.</p> REST <p>Requires POST requests: Attaching a new run to a benchmark_study Detaching a run from benchmark_study </p> Python example <pre><code>import openml\nimport sklearn\n\nopenml.config.apikey = 'FILL_IN_OPENML_API_KEY'  # set the OpenML Api Key\nbenchmark_suite = openml.study.get_suite('OpenML-CC18')  # obtain the benchmark suite\n\n# build a scikit-learn classifier\nclf = sklearn.pipeline.make_pipeline(sklearn.preprocessing.Imputer(),\n                                     sklearn.tree.DecisionTreeClassifier())\n\nfor task_id in benchmark_suite.tasks:  # iterate over all tasks\n\n    task = openml.tasks.get_task(task_id)  # download the OpenML task\n    run = openml.runs.run_model_on_task(clf, task)  # run the classifier on the task\n    score = run.get_metric_fn(sklearn.metrics.accuracy_score)  # print accuracy score\n    print('Data set: %s; Accuracy: %0.2f' % (task.get_dataset().name,score.mean()))\n    run.publish()  # publish the experiment on OpenML (optional, requires internet and an API key)\n    print('URL for run: %s/run/%d' %(openml.config.server,run.run_id))\n</code></pre> Java example <pre><code>public static void runTasksAndUpload() throws Exception {\n  OpenmlConnector openml = new OpenmlConnector();\n  openml.setApiKey(\"FILL_IN_OPENML_API_KEY\");\n  // obtain the benchmark suite\n  Study benchmarksuite = openml.studyGet(\"OpenML-CC18\", \"tasks\");\n  Classifier tree = new REPTree(); // build a Weka classifier\n  for (Integer taskId : benchmarksuite.getTasks()) { // iterate over all tasks\n    Task t = openml.taskGet(taskId); // download the OpenML task\n    Instances d = InstancesHelper.getDatasetFromTask(openml, t); // obtain the dataset\n    int runId = RunOpenmlJob.executeTask(openml, new WekaConfig(), taskId, tree);\n    Run run = openml.runGet(runId);   // retrieve the uploaded run\n  }\n}\n</code></pre> R example <pre><code>library(OpenML)\nsetOMLConfig(apikey = 'FILL_IN_OPENML_API_KEY')\nlrn = makeLearner('classif.rpart') # construct a simple CART classifier\ntask.ids = getOMLStudy('OpenML-CC18')$tasks$task.id # obtain the list of suggested tasks\nfor (task.id in task.ids) { # iterate over all tasks\n  task = getOMLTask(task.id) # download single OML task\n  data = as.data.frame(task) # obtain raw data set\n  run = runTaskMlr(task, learner = lrn) # run constructed learner\n  upload = uploadOMLRun(run) # upload and tag the run\n}\n</code></pre>"},{"location":"benchmark/#retrieving-runs-on-a-benchmarking-suites","title":"Retrieving runs on a benchmarking suites:","text":"<p>Once a benchmark suite has been created, the listing functions can be used to  obtain all results on the benchmark suite. Note that there are several other ways to select and bundle runs together. This will be featured in  a separate article on reproducible benchmarks. </p> REST (TODO) <p>https://www.openml.org/api/v1/xml/run/list/study/OpenML-CC18</p> <p>Check out the API docs</p> Python example <pre><code>benchmark_suite = openml.study.get_suite('OpenML-CC18')\nruns = openml.runs.list_runs(task=benchmark_suite.tasks, limit=1000)\n</code></pre> Java example <pre><code>public void downloadResultsBenchmarkSuite()  throws Exception {\n    Study benchmarkSuite = openml.studyGet(\"OpenML100\", \"tasks\");\n\n    Map&lt;String, List&lt;Integer&gt;&gt; filters = new TreeMap&lt;String, List&lt;Integer&gt;&gt;();\n    filters.put(\"task\", Arrays.asList(benchmarkSuite.getTasks()));\n    RunList rl = openml.runList(filters, 200, null);\n\n    assertTrue(rl.getRuns().length &gt; 0); \n}\n</code></pre> R example <pre><code>benchmark.suite = getOMLStudy(study = \"OpenML-CC18\")\nrun.ids = extractOMLStudyIds(benchmark.suite, \"run.id\")\nruns = rbindlist(lapply(run.ids, function(id) listOMLRuns(run.id = id)))\n# TODO waiting for REST API\n</code></pre>"},{"location":"benchmark/#creating-new-benchmark-suites","title":"Creating new benchmark suites","text":"<p>Additional OpenML benchmark suites can be created by defining the precise set of tasks, as well as a textual description. New datasets first need to be registered on OpenML and tasks need to be created on them.</p> <p>We have provided a GitHub repository with additional tools and scripts to build new benchmark studies, e.g. to select all datasets adhering to strict conditions, and to analyse bencharking results.</p> REST <p>Requires POST requests: Creating a benchmark suite </p> Python example <pre><code>import openml\n\n# find 250 tasks that we are interested in, e.g., the tasks that have between\n# 100 and 10000 instances and between 4 and 20 attributes\ntasks = openml.tasks.list_tasks(number_instances='100..10000', number_features='4..20', size=250)\ntask_ids = list(tasks.keys())\n\n# create the benchmark suite\n# the arguments are the alias, name, description, and list of task_ids, respectively.\nstudy = openml.study.create_benchmark_suite( \n    name=\"MidSize Suite\", \n    alias=None,\n    description=\"illustrating how to create a benchmark suite\", \n    task_ids=task_ids,\n)\nstudy_id = study.publish()\n</code></pre> Java example <pre><code>public void createBenchmarkSuite() throws Exception {\n    OpenmlConnector openml = new OpenmlConnector(\"FILL_IN_OPENML_API_KEY\");\n    // find 250 tasks that we are interested in, e.g., the tasks that have between\n    // 100 and 10000 instances and between 4 and 20 attributes\n    Map&lt;String, String&gt; filtersOrig = new TreeMap&lt;String, String&gt;();\n    filtersOrig.put(\"number_instances\", \"100..10000\");\n    filtersOrig.put(\"number_features\", \"4..20\");\n    filtersOrig.put(\"limit\", \"250\");\n    Tasks tasksOrig = client_write_test.taskList(filtersOrig);\n\n    // create the study\n    Study study = new Study(null, \"test\", \"test\", null, tasksOrig.getTaskIds(), null);\n    int studyId = openml.studyUpload(study);\n}\n</code></pre> R example <pre><code># find 250 tasks with 100 and 10000 instances and between 4 and 20 attributes\ntid = listOMLTasks(number.of.instances = c(100, 10000), number.of.features = c(4, 20), limit = 250)\nstudy = makeOMLStudy(alias = \"test_alias\", name = \"Test Upload from R\", description = \"Just testing\", task.id = tid$task.id)\nid = uploadOMLStudy(study)\n</code></pre>"},{"location":"benchmark/#updating-a-benchmark-suite","title":"Updating a benchmark suite","text":"<p>You can add tasks to a benchmark suite, or remove them.</p> REST <p>Requires POST requests: Attaching a new task Detaching a task </p> Python example <pre><code>import openml\n\n# find 250 tasks that we are interested in, e.g., the tasks that have between\n# 100 and 10000 instances and between 4 and 20 attributes\ntasks = openml.tasks.list_tasks(number_instances='100..10000', number_features='4..20', size=250)\ntask_ids = list(tasks.keys())\n\n# create the benchmark suite\nstudy = openml.study.create_benchmark_suite( \n    name=\"MidSize Suite\", \n    alias=None,\n    description=\"illustrating how to create a benchmark suite\", \n    task_ids=task_ids,\n)\nstudy_id = study.publish()\n\n# download the study from the server, for verification purposes\nstudy = openml.study.get_study(study_id)\n\n# until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset:\ntasks_new = openml.tasks.list_tasks(data_name='letter', size=1)\ntask_ids_new = list(tasks_new.keys())\nopenml.study.attach_to_study(study_id, task_ids_new)\n\n# or even remove these again\nopenml.study.detach_from_study(study_id, task_ids_new)\n\n# redownload the study\nstudy_prime = openml.study.get_study(study_id)\n\nassert(study.tasks == study_prime.tasks)\nassert(study.data == study_prime.data)\n</code></pre> Java example <pre><code>public void attachDetachStudy()  throws Exception {\n    OpenmlConnector openml = new OpenmlConnector(\"FILL_IN_OPENML_API_KEY\");\n    // find 250 tasks that we are interested in, e.g., the tasks that have between\n    // 100 and 10000 instances and between 4 and 20 attributes\n    Map&lt;String, String&gt; filtersOrig = new TreeMap&lt;String, String&gt;();\n    filtersOrig.put(\"number_instances\", \"100..10000\");\n    filtersOrig.put(\"number_features\", \"4..20\");\n    filtersOrig.put(\"limit\", \"250\");\n    Tasks tasksOrig = openml.taskList(filtersOrig);\n\n    // create the study\n    Study study = new Study(null, \"test\", \"test\", null, tasksOrig.getTaskIds(), null);\n    int studyId = openml.studyUpload(study);\n\n    // until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset:\n    Map&lt;String, String&gt; filtersAdd = new TreeMap&lt;String, String&gt;();\n    filtersAdd.put(\"data_name\", \"letter\");\n    filtersAdd.put(\"limit\", \"1\");\n    Tasks tasksAdd = openml.taskList(filtersAdd);\n    openml.studyAttach(studyId, Arrays.asList(tasksAdd.getTaskIds()));\n\n    // or even remove these again\n    openml.studyDetach(studyId, Arrays.asList(tasksAdd.getTaskIds()));\n\n    // download the study\n    Study studyDownloaded = openml.studyGet(studyId);\n    assertArrayEquals(tasksOrig.getTaskIds(), studyDownloaded.getTasks());\n}\n</code></pre> R example <pre><code>TODO\n</code></pre>"},{"location":"benchmark/#further-code-examples-and-use-cases","title":"Further code examples and use cases","text":"<p>As mentioned above, we host a GitHub repository with additional tools and scripts to easily create and use new benchmark studies. It includes:</p> <ul> <li>A Jupyter Notebook that builds a new benchmark suite with datasets that adhere to strict and complex conditions, as well as automated tests to remove tasks that are too easy for proper benchmarking.</li> <li>A Jupyter Notebook that shows how to pull in the latest state-of-the-art results for any of the benchmark suites</li> <li>A Jupyter Notebook that does a detailed analysis of all results in a benchmark suite, and an example run on the OpenML-CC18. It includes a wide range of plots and rankings to get a deeper insight into the benchmark results.</li> <li>Scripts in Python and R to facilitate common subtasks.</li> </ul> <p>We very much welcome new scripts and notebooks, or improvements to the existing ones, that help others to create benchmark suites and analyse benchmarking results.</p>"},{"location":"benchmark/#list-of-benchmarking-suites","title":"List of benchmarking suites","text":""},{"location":"benchmark/#openml-cc18","title":"OpenML-CC18","text":"<p>The OpenML-CC18 suite contains all OpenML datasets from mid-2018 that satisfy a large set of clear requirements for thorough yet practical benchmarking. It includes datasets frequently used in benchmarks published over the last years, so it can be used as a drop-in replacement for many benchmarking setups.</p> <p>List of datasets and properties</p> <p>The suite is defined as the set of all verified OpenML datasets that satisfy the following requirements:</p> <ul> <li>the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small and not too big,</li> <li>the number of features does not exceed 5000 features to keep the runtime of algorithms low,</li> <li>the target attribute has at least two classes</li> <li>the ratio of the minority class and the majority class is above 0.05, to eliminate highly imbalanced datasets which require special treatment for both algorithms and evaluation measures.</li> </ul> <p>We excluded datasets which:</p> <ul> <li>are artificially generated (not to confuse with simulated)</li> <li>cannot be randomized via a 10-fold cross-validation due to grouped samples or because they are time series or data streams</li> <li>are a subset of a larger dataset</li> <li>have classes with less than 20 observations</li> <li>have no source or reference available</li> <li>can be perfectly classified by a single attribute or a decision stump</li> <li>allow a decision tree to achieve 100% accuracy on a 10-fold cross-validation task</li> <li>have more than 5000 features after one-hot-encoding categorical features</li> <li>are created by binarization of regression tasks or multiclass classification tasks, or</li> <li>are sparse data (e.g., text mining data sets)</li> </ul> Detailed motivation of these decisions <p>We chose the CC18 datasets to allow for practical benchmarking based on the characteristics that might be problematic based on our experience, and to avoid common pitfalls that may invalidate benchmark studies:  </p> <ul> <li>We used at least 500 data points to allow performing cross-validation while still having a large-enough test split.</li> <li>We limited the datasets to 100.000 data points to allow the algorithms to train machine learning models in a reasonable amount of time.</li> <li>We limited the number of features to 5000 to allow the usage of algorithms which scale unfavourably in the number of features. This limitation, together with the two limitations above aims to allow running all \u201cstandard\u201d machine learning algorithms (naive bayes, linear models, support vector machines, tree-based ensemble methods and neural networks) on the benchmark suite.</li> <li>We required each dataset to have at least two classes to be able to work in a supervised classification setting.</li> <li>We require each class to have at least 20 observations to be able to perform stratified cross-validation where there is at least one observation from each class in each split. We have found that not having all classes present in all training and test sets can make several machine learning packages fail.</li> <li>We require a certain balancedness (ratio of minority class to majority class) to prevent cases where only predicting the majority class would be beneficial. This is most likely the restriction which is most debatable, but we found it very helpful to apply a large set of machine learning algorithms across several libraries to the study. We expect that future studies focus more on imbalanced datasets. </li> </ul> <p>Furthermore, we aimed to have the dataset collection as general as possible, rule out as few algorithms as possible and have it usable as easily as possible:</p> <ul> <li>We strived to remove artificial datasets as they, for example, come from textbooks and it is hard to reliably assess their difficulty. We admit that there is a blurred line between artificial and simulated datasets and do not have a perfect distinction between them (for example, a lot of phenomena can be simulated, but the outcome might be like a simple, artificial dataset). Therefore, we removed datasets if we were in doubt of whether they are simulated or artificial. </li> <li>We removed datasets which require grouped sampling because they are time series or data streams which should be treated with special care by machine learning algorithms (i.e., taking the time aspect into account). To be on the safe side, we also removed datasets where each sample constitutes a single data stream.</li> <li>We removed datasets which are a subset of larger datasets. Allowing subsets would be very subjective as there is no objective choice of a dataset subset size or a subset of the variables or classes. Therefore, creating dataset subsets would open a Pandora\u2019s Box.</li> <li>We removed datasets which have no source or reference available to potentially learn more about these datasets if we observe unexpected behavior in future studies. In contrast, we would not be able to learn more about the background of a dataset which has no description and publication attached, leaving us with a complete black box.</li> <li>We removed datasets which can be perfectly classified by a single attribute or a decision stump as they do not allow to meaningfully compare machine learning algorithms (they all achieve 100% accuracy unless the hyperparameters are set in a bogus way).</li> <li>We removed datasets where a decision tree could achieve 100% accuracy on a 10-fold cross-validation task to remove datasets which can be solved by a simple algorithm which is prone to overfitting training data. We found that this is a good indicator of too easy datasets. Obviously, other datasets will appear easy for several algorithms, and we aim to learn more about the characteristics of such datasets in future studies.</li> <li>We removed datasets which have more than 5000 features after one-hot-encoding categorical features. One-hot-encoding is the most frequent way to deal with categorical variables across the different machine learning libraries MLR, scikit-learn and WEKA. In order to limit the number of features to 5000 as explained above, we imposed the additional constraint that this should be counted after one-hot-encoding to allow wide applicability of the benchmark suite.</li> <li>We removed datasets which were created by binarization of regression tasks or multiclass classification task for similar reasons as for forbidding dataset subsets.</li> <li>We did not include sparse datasets because not all machine learning libraries (i.e., all machine learning models) can handle them gracefully, which is in contrast to our goal which is wide applicability.</li> </ul>"},{"location":"benchmark/#citing-the-openml-cc18","title":"Citing the OpenML-CC18","text":"<p>If you have used the OpenML-CC18 in a scientific publication, we would appreciate citations of core OpenML packages as well as a citation of the following paper:</p> <p>Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin. OpenML Benchmarking Suites. arXiv 1708.0373v2 (2019): 1-6</p>"},{"location":"benchmark/#openml100","title":"OpenML100","text":"<p>The OpenML100 was a predecessor of the OpenML-CC18, consisting of 100 classification datasets. We recommend that you use the OpenML-CC18 instead, because the OpenML100 suffers from some teething issues in the design of benchmark suites. For instance, it contains several datasets that are too easy to model with today's machine learning algorithms, as well as datasets that represent time series analysis problems. These do not invalidate benchmarks run on the OpenML100, but may obfuscate the interpretation of results. The 'OpenML-CC18' handle is also more descriptive and allows easier versioning. The OpenML100 was first published in the Arxiv preprint OpenML Benchmarking Suites and the OpenML100.</p> <p>List of datasets and properties</p> <p>For reference, the OpenML100 included datasets satisfying the following requirements:</p> <ul> <li>the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small for proper training and not too big for practical experimentation</li> <li>the number of features does not exceed 5000 features to keep the runtime of algorithms low</li> <li>the target attribute has at least two classes</li> <li>he ratio of the minority class and the majority class is above 0.05 to eliminate highly imbalanced datasets that would obfuscate a clear analysis</li> </ul> <p>It excluded datasets which:</p> <ul> <li>cannot be randomized via a 10-fold cross-validation due to grouped samples</li> <li>have an unknown origin or no clearly defined task</li> <li>are variants of other datasets (e.g. binarized regression tasks)</li> <li>include sparse data (e.g., text mining data sets)</li> </ul>"},{"location":"benchmark/#citing-the-openml100","title":"Citing the OpenML100","text":"<p>If you have used the OpenML100 in a scientific publication, we would appreciate citations of core OpenML packages as well as a citation of the following paper:</p> <p>Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin. OpenML Benchmarking Suites and the OpenML100. arXiv 1708.0373v1 (2017): 1-6</p>"},{"location":"benchmark/#need-help","title":"Need help?","text":"<p>We are happy to answer to any suggestion or question you may have. For general questions or issues, please open an issue in the benchmarking issue tracker. If the issue lies with one of the language-specific bindings, please post an issue in the appropriate issue tracker.</p>"},{"location":"automl/AutoML-Benchmark/","title":"Getting Started","text":"<p>The AutoML Benchmark is a tool for benchmarking AutoML frameworks on tabular data. It automates the installation of AutoML frameworks, passing it data, and evaluating their predictions.  Our paper describes the design and showcases  results from an evaluation using the benchmark.  This guide goes over the minimum steps needed to evaluate an AutoML framework on a toy dataset.</p> <p>Full instructions can be found in the API Documentation.</p>"},{"location":"automl/AutoML-Benchmark/#installation","title":"Installation","text":"<p>These instructions assume that Python 3.9 (or higher)  and git are installed, and are available under the alias <code>python</code> and <code>git</code>, respectively. We recommend Pyenv for managing multiple Python installations, if applicable. We support Ubuntu 22.04, but many linux and MacOS versions likely work (for MacOS, it may be necessary to have <code>brew</code> installed).</p> <p>First, clone the repository:</p> <pre><code>git clone https://github.com/openml/automlbenchmark.git --branch stable --depth 1\ncd automlbenchmark\n</code></pre> <p>Create a virtual environments to install the dependencies in:</p>"},{"location":"automl/AutoML-Benchmark/#linux","title":"Linux","text":"<pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"automl/AutoML-Benchmark/#macos","title":"MacOS","text":"<pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"automl/AutoML-Benchmark/#windows","title":"Windows","text":"<pre><code>python -m venv ./venv\nvenv/Scripts/activate\n</code></pre> <p>Then install the dependencies:</p> <pre><code>python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\n</code></pre> Note for Windows users <p>The automated installation of AutoML frameworks is done using shell script, which doesn't work on Windows. We recommend you use Docker to run the examples below. First, install and run <code>docker</code>.  Then, whenever there is a <code>python runbenchmark.py ...</code>  command in the tutorial, add <code>-m docker</code> to it (<code>python runbenchmark.py ... -m docker</code>).</p> Problem with the installation? <p>On some platforms, we need to ensure that requirements are installed sequentially. Use <code>xargs -L 1 python -m pip install &lt; requirements.txt</code> to do so. If problems  persist, open an issue with the error and information about your environment (OS, Python version, pip version).</p>"},{"location":"automl/AutoML-Benchmark/#running-the-benchmark","title":"Running the Benchmark","text":"<p>To run a benchmark call the <code>runbenchmark.py</code> script specifying the framework to evaluate.</p> <p>See the API Documentation. for more information on the parameters available.</p>"},{"location":"automl/basic_example/","title":"Random Forest Baseline","text":"<p>Let's try evaluating the <code>RandomForest</code> baseline, which uses scikit-learn's random forest:</p>"},{"location":"automl/basic_example/#running-the-benchmark","title":"Running the Benchmark","text":""},{"location":"automl/basic_example/#linux","title":"Linux","text":"<pre><code>python runbenchmark.py randomforest \n</code></pre>"},{"location":"automl/basic_example/#macos","title":"MacOS","text":"<pre><code>python runbenchmark.py randomforest \n</code></pre>"},{"location":"automl/basic_example/#windows","title":"Windows","text":"<p>As noted above, we need to install the AutoML frameworks (and baselines) in a container. Add <code>-m docker</code> to the command as shown: <pre><code>python runbenchmark.py randomforest -m docker\n</code></pre></p> <p>Important</p> <p>Future example usages will only show invocations without <code>-m docker</code> mode, but Windows users will need to run in some non-local mode.</p>"},{"location":"automl/basic_example/#results","title":"Results","text":"<p>After running the command, there will be a lot of output to the screen that reports on what is currently happening. After a few minutes final results are shown and should  look similar to this:</p> <pre><code>Summing up scores for current run:\n               id        task  fold    framework constraint     result      metric  duration      seed\nopenml.org/t/3913         kc2     0 RandomForest       test   0.865801         auc      11.1 851722466\nopenml.org/t/3913         kc2     1 RandomForest       test   0.857143         auc       9.1 851722467\n  openml.org/t/59        iris     0 RandomForest       test  -0.120755 neg_logloss       8.7 851722466\n  openml.org/t/59        iris     1 RandomForest       test  -0.027781 neg_logloss       8.5 851722467\nopenml.org/t/2295 cholesterol     0 RandomForest       test -44.220800    neg_rmse       8.7 851722466\nopenml.org/t/2295 cholesterol     1 RandomForest       test -55.216500    neg_rmse       8.7 851722467\n</code></pre> <p>The result denotes the performance of the framework on the test data as measured by the metric listed in the metric column. The result column always denotes performance  in a way where higher is better (metrics which normally observe \"lower is better\" are converted, which can be observed from the <code>neg_</code> prefix).</p> <p>While running the command, the AutoML benchmark performed the following steps:</p> <ol> <li>Create a new virtual environment for the Random Forest experiment.      This environment can be found in <code>frameworks/randomforest/venv</code> and will be re-used      when you perform other experiments with <code>RandomForest</code>.</li> <li>It downloaded datasets from OpenML complete with a      \"task definition\" which specifies cross-validation folds.</li> <li>It evaluated <code>RandomForest</code> on each (task, fold)-combination in a separate subprocess, where:<ol> <li>The framework (<code>RandomForest</code>) is initialized.</li> <li>The training data is passed to the framework for training.</li> <li>The test data is passed to the framework to make predictions on.</li> <li>It passes the predictions back to the main process</li> </ol> </li> <li>The predictions are evaluated and reported on. They are printed to the console and      are stored in the <code>results</code> directory. There you will find:<ol> <li><code>results/results.csv</code>: a file with all results from all benchmarks conducted on your machine.</li> <li><code>results/randomforest.test.test.local.TIMESTAMP</code>: a directory with more information about the run,     such as logs, predictions, and possibly other artifacts.</li> </ol> </li> </ol> <p>Docker Mode</p> <p>When using docker mode (with <code>-m docker</code>) a docker image will be made that contains the virtual environment. Otherwise, it functions much the same way.</p>"},{"location":"automl/basic_example/#important-parameters","title":"Important Parameters","text":"<p>As you can see from the results above, the  default behavior is to execute a short test benchmark. However, we can specify a different benchmark, provide different constraints, and even run the experiment in a container or on AWS. There are many parameters for the <code>runbenchmark.py</code> script, but the most important ones are:</p>"},{"location":"automl/basic_example/#framework-required","title":"Framework (required)","text":"<ul> <li>The AutoML framework or baseline to evaluate and is not case-sensitive. See   integrated frameworks for a list of supported frameworks.    In the above example, this benchmarked framework <code>randomforest</code>.</li> </ul>"},{"location":"automl/basic_example/#benchmark-optional-defaulttest","title":"Benchmark (optional, default='test')","text":"<ul> <li>The benchmark suite is the dataset or set of datasets to evaluate the framework on.   These can be defined as on OpenML as a study or task    (formatted as <code>openml/s/X</code> or <code>openml/t/Y</code> respectively) or in a local file.   The default is a short evaluation on two folds of <code>iris</code>, <code>kc2</code>, and <code>cholesterol</code>.</li> </ul>"},{"location":"automl/basic_example/#constraints-optional-defaulttest","title":"Constraints (optional, default='test')","text":"<ul> <li> <p>The constraints applied to the benchmark as defined by default in constraints.yaml.   These include time constraints, memory constrains, the number of available cpu cores, and more.   Default constraint is <code>test</code> (2 folds for 10 min each). </p> <p>Constraints are not enforced!</p> <p>These constraints are forwarded to the AutoML framework if possible but, except for runtime constraints, are generally not enforced. It is advised when benchmarking to use an environment that mimics the given constraints.</p> Constraints can be overriden by <code>benchmark</code> <p>A benchmark definition can override constraints on a task level. This is useful if you want to define a benchmark which has different constraints for different tasks. The default \"test\" benchmark does this to limit runtime to 60 seconds instead of 600 seconds, which is useful to get quick results for its small datasets. For more information, see defining a benchmark.</p> </li> </ul>"},{"location":"automl/basic_example/#mode-optional-defaultlocal","title":"Mode (optional, default='local')","text":"<ul> <li> <p>The benchmark can be run in four modes:</p> <ul> <li><code>local</code>: install a local virtual environment and run the benchmark on your machine.</li> <li><code>docker</code>: create a docker image with the virtual environment and run the benchmark in a container on your machine.               If a local or remote image already exists, that will be used instead. Requires Docker.</li> <li><code>singularity</code>: create a singularity image with the virtual environment and run the benchmark in a container on your machine. Requires Singularity.</li> <li><code>aws</code>: run the benchmark on AWS EC2 instances.           It is possible to run directly on the instance or have the EC2 instance run in <code>docker</code> mode.           Requires valid AWS credentials to be configured, for more information see Running on AWS.</li> </ul> </li> </ul> <p>For a full list of parameters available, run:</p> <pre><code>python runbenchmark.py --help\n</code></pre>"},{"location":"automl/benchmark_on_openml/","title":"Example: Benchmarks on OpenML","text":"<p>In the previous examples, we used benchmarks which were defined in a local file (test.yaml and  validation.yaml, respectively).  However, we can also use tasks and benchmarking suites defined on OpenML directly from the command line. When referencing an OpenML task or suite, we can use <code>openml/t/ID</code> or <code>openml/s/ID</code> respectively as  argument for the benchmark parameter. Running on the iris task:</p> <pre><code>python runbenchmark.py randomforest openml/t/59\n</code></pre> <p>or on the entire AutoML benchmark classification suite (this will take hours!):</p> <pre><code>python runbenchmark.py randomforest openml/s/271\n</code></pre> <p>Large-scale Benchmarking</p> <p>For large scale benchmarking it is advised to parallelize your experiments, as otherwise it may take months to run the experiments. The benchmark currently only supports native parallelization in <code>aws</code> mode (by using the <code>--parallel</code> parameter), but using the <code>--task</code> and <code>--fold</code> parameters  it is easy to generate scripts that invoke individual jobs on e.g., a SLURM cluster. When you run in any parallelized fashion, it is advised to run each process on separate hardware to ensure experiments can not interfere with each other.</p>"},{"location":"automl/important_params/","title":"Important Parameters","text":"<p>As you can see from the results above, the  default behavior is to execute a short test benchmark. However, we can specify a different benchmark, provide different constraints, and even run the experiment in a container or on AWS. There are many parameters for the <code>runbenchmark.py</code> script, but the most important ones are:</p> <p><code>Framework (required)</code></p> <ul> <li>The AutoML framework or baseline to evaluate and is not case-sensitive. See   integrated frameworks for a list of supported frameworks.    In the above example, this benchmarked framework <code>randomforest</code>.</li> </ul> <p><code>Benchmark (optional, default='test')</code></p> <ul> <li>The benchmark suite is the dataset or set of datasets to evaluate the framework on.   These can be defined as on OpenML as a study or task    (formatted as <code>openml/s/X</code> or <code>openml/t/Y</code> respectively) or in a local file.   The default is a short evaluation on two folds of <code>iris</code>, <code>kc2</code>, and <code>cholesterol</code>.</li> </ul> <p><code>Constraints (optional, default='test')</code></p> <ul> <li> <p>The constraints applied to the benchmark as defined by default in constraints.yaml.   These include time constraints, memory constrains, the number of available cpu cores, and more.   Default constraint is <code>test</code> (2 folds for 10 min each). </p> <p>Constraints are not enforced!</p> <p>These constraints are forwarded to the AutoML framework if possible but, except for runtime constraints, are generally not enforced. It is advised when benchmarking to use an environment that mimics the given constraints.</p> Constraints can be overriden by <code>benchmark</code> <p>A benchmark definition can override constraints on a task level. This is useful if you want to define a benchmark which has different constraints for different tasks. The default \"test\" benchmark does this to limit runtime to 60 seconds instead of 600 seconds, which is useful to get quick results for its small datasets. For more information, see defining a benchmark.</p> </li> </ul> <p><code>Mode (optional, default='local')</code></p> <ul> <li> <p>The benchmark can be run in four modes:</p> <ul> <li><code>local</code>: install a local virtual environment and run the benchmark on your machine.</li> <li><code>docker</code>: create a docker image with the virtual environment and run the benchmark in a container on your machine.               If a local or remote image already exists, that will be used instead. Requires Docker.</li> <li><code>singularity</code>: create a singularity image with the virtual environment and run the benchmark in a container on your machine. Requires Singularity.</li> <li><code>aws</code>: run the benchmark on AWS EC2 instances.           It is possible to run directly on the instance or have the EC2 instance run in <code>docker</code> mode.           Requires valid AWS credentials to be configured, for more information see Running on AWS.</li> </ul> </li> </ul> <p>For a full list of parameters available, run:</p> <pre><code>python runbenchmark.py --help\n</code></pre>"},{"location":"automl/specific_task_fold_example/","title":"Example: AutoML on a specific task and fold","text":"<p>The defaults are very useful for performing a quick test, as the datasets are small and cover different task types (binary classification, multiclass classification, and  regression). We also have a \"validation\" benchmark suite for more elaborate testing that also includes missing data, categorical data,  wide data, and more. The benchmark defines 9 tasks, and evaluating two folds with a 10-minute time constraint would take roughly 3 hours (=9 tasks * 2 folds * 10 minutes, plus overhead). Let's instead use the <code>--task</code> and <code>--fold</code> parameters to run only a specific task and fold in the <code>benchmark</code> when evaluating the  flaml AutoML framework:</p> <pre><code>python runbenchmark.py flaml validation test -t eucalyptus -f 0\n</code></pre> <p>This should take about 10 minutes plus the time it takes to install <code>flaml</code>. Results should look roughly like this:</p> <pre><code>Processing results for flaml.validation.test.local.20230711T122823\nSumming up scores for current run:\n               id       task  fold framework constraint    result      metric  duration       seed\nopenml.org/t/2079 eucalyptus     0     flaml       test -0.702976 neg_logloss     611.0 1385946458\n</code></pre> <p>Similarly to the test run, you will find additional files in the <code>results</code> directory.</p>"},{"location":"concepts/","title":"Concepts","text":"<p>OpenML operates on a number of core concepts which are important to understand:  </p> <p>:fa-database: Datasets Datasets are pretty straight-forward. Tabular datasets are self-contained, consisting of a number of rows (instances) and columns (features), including their data types. Other  modalities (e.g. images) are included via paths to files stored within the same folder. Datasets are uniformly formatted (S3 buckets with Parquet tables, JSON metadata, and media files), and are auto-converted and auto-loaded in your desired format by the APIs (e.g. in Python) in a single line of code. Example: The Iris dataset or the Plankton dataset</p> <p>:fa-trophy: Tasks A task consists of a dataset, together with a machine learning task to perform, such as classification or clustering and an evaluation method. For supervised tasks, this also specifies the target column in the data. Example: Classifying different iris species from other attributes and evaluate using 10-fold cross-validation.</p> <p>:fa-cogs: Flows A flow identifies a particular machine learning algorithm (a pipeline or untrained model) from a particular library or framework, such as scikit-learn, pyTorch, or MLR. It contains details about the structure of the model/pipeline, dependencies (e.g. the library and its version) and a list of settable hyperparameters. In short, it is a serialized description of the algorithm that in many cases can also be deserialized to reinstantiate the exact same algorithm in a particular library.  Example: scikit-learn's RandomForest or a simple TensorFlow model</p> <p>:fa-star: Runs A run is an experiment - it evaluates a particular flow (pipeline/model) with particular hyperparameter settings, on a particular task. Depending on the task it will include certain results, such as model evaluations (e.g. accuracies), model predictions, and other output files (e.g. the trained model). Example: Classifying Gamma rays with scikit-learn's RandomForest</p>"},{"location":"concepts/benchmarking/","title":"Collections and benchmarks","text":"<p>You can combine tasks and runs into collections, to run experiments across many tasks at once and collect all results. Each collection gets its own page, which can be linked to publications so that others can find all the details online.</p>"},{"location":"concepts/benchmarking/#benchmarking-suites","title":"Benchmarking suites","text":"<p>Collections of tasks can be published as benchmarking suites. Seamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier: - all datasets are uniformly formatted in standardized data formats - they can be easily downloaded programmatically through APIs and client libraries - they come with machine-readable meta-information, such as the occurrence of missing  values, to train algorithms correctly - standardized train-test splits are provided to ensure that results can be objectively compared - results can be shared in a reproducible way through the APIs - results from other users can be easily downloaded and reused </p> <p>You can search for all existing benchmarking suites or create your own. For all further details, see the benchmarking guide.</p> <p></p>"},{"location":"concepts/benchmarking/#benchmark-studies","title":"Benchmark studies","text":"<p>Collections of runs can be published as benchmarking studies. They contain the results of all runs (possibly millions) executed on a specific benchmarking suite. OpenML allows you to easily download all such results at once via the APIs, but also visualized them online in the Analysis tab (next to the complete list of included tasks and runs). Below is an example of a benchmark study for AutoML algorithms.</p> <p></p>"},{"location":"concepts/data/","title":"Data","text":""},{"location":"concepts/data/#discovery","title":"Discovery","text":"<p>OpenML allows fine-grained search over thousands of machine learning datasets. Via the website, you can filter by many dataset properties, such as size, type, format, and many more. Via the APIs you have access to many more filters, and you can download a complete table with statistics of all datasest. Via the APIs you can also load datasets directly into your preferred data structures such as numpy (example in Python). We are also working on better organization of all datasets by topic </p> <p></p>"},{"location":"concepts/data/#sharing","title":"Sharing","text":"<p>You can upload and download datasets through the website or though the APIs (recommended). You can share data directly from common data science libraries, e.g. from Python or R dataframes, in a few lines of code. The OpenML APIs will automatically extract lots of meta-data and store all datasets in a uniform format.</p> <pre><code>    import pandas as pd\n    import openml as oml\n\n    # Create an OpenML dataset from a pandas dataframe\n    df = pd.DataFrame(data, columns=attribute_names)\n    my_data = oml.datasets.functions.create_dataset(\n        name=\"covertype\", description=\"Predicting forest cover ...\",\n        licence=\"CC0\", data=df\n    )\n\n    # Share the dataset on OpenML\n    my_data.publish()\n</code></pre> <p>Every dataset gets a dedicated page on OpenML with all known information, and can be edited further online.</p> <p></p> <p>Data hosted elsewhere can be referenced by URL. We are also working on interconnecting OpenML with other machine learning data set repositories </p>"},{"location":"concepts/data/#automated-analysis","title":"Automated analysis","text":"<p>OpenML will automatically analyze the data and compute a range of data quality characteristics. These include simple statistics such as the number of examples and features, but also potential quality issues (e.g. missing values) and more advanced statistics (e.g. the mutual information in the features and benchmark performances of simple models). These can be useful to find, filter and compare datasets, or to automate data preprocessing.  We are also working on simple metrics and automated dataset quality reports </p> <p>The Analysis tab (see image below, or try it live) also shows an automated and interactive analysis of all datasets. This runs on open-source Python code via Dash and we welcome all contributions </p> <p></p> <p>The third tab, 'Tasks', lists all tasks created on the dataset. More on that below.</p>"},{"location":"concepts/data/#dataset-id-and-versions","title":"Dataset ID and versions","text":"<p>A dataset can be uniquely identified by its dataset ID, which is shown on the website and returned by the API. It's <code>1596</code> in the <code>covertype</code> example above. They can also be referenced by name and ID. OpenML assigns incremental version numbers per upload with the same name. You can also add a free-form <code>version_label</code> with every upload.</p>"},{"location":"concepts/data/#dataset-status","title":"Dataset status","text":"<p>When you upload a dataset, it will be marked <code>in_preparation</code> until it is (automatically) verified. Once approved, the dataset will become <code>active</code> (or <code>verified</code>). If a severe issue has been found with a dataset, it can become <code>deactivated</code> (or <code>deprecated</code>) signaling that it should not be used. By default, dataset search only returns verified datasets, but you can access and download datasets with any status.</p>"},{"location":"concepts/data/#special-attributes","title":"Special attributes","text":"<p>Machine learning datasets often have special attributes that require special handling in order to build useful models. OpenML marks these as special attributes.</p> <p>A <code>target</code> attribute is the column that is to be predicted, also known as dependent variable. Datasets can have a default target attribute set by the author, but OpenML tasks can also overrule this. Example: The default target variable for the MNIST dataset is to predict the class from pixel values, and most supervised tasks will have the class as their target. However, one can also create a task aimed at predicting the value of pixel257 given all the other pixel values and the class column.</p> <p><code>Row id</code> attributes indicate externally defined row IDs (e.g. <code>instance</code> in dataset 164). <code>Ignore</code> attributes are other columns that should not be included in training data (e.g. <code>Player</code> in dataset 185). OpenML will clearly mark these, and will (by default) drop these columns when constructing training sets.</p>"},{"location":"concepts/flows/","title":"Flows","text":"<p>Flows are machine learning pipelines, models, or scripts. They are typically uploaded directly from machine learning libraries (e.g. scikit-learn, pyTorch, TensorFlow, MLR, WEKA,...) via the corresponding APIs. Associated code (e.g., on GitHub) can be referenced by URL.</p>"},{"location":"concepts/flows/#analysing-algorithm-performance","title":"Analysing algorithm performance","text":"<p>Every flow gets a dedicated page with all known information. The Analysis tab shows an automated interactive analysis of all collected results. For instance, below are the results of a scikit-learn pipeline including missing value imputation, feature encoding, and a RandomForest model. It shows the results across multiple tasks, and how the AUC score is affected by certain hyperparameters.</p> <p></p> <p>This helps to better understand specific models, as well as their strengths and weaknesses.</p>"},{"location":"concepts/flows/#automated-sharing","title":"Automated sharing","text":"<p>When you evaluate algorithms and share the results, OpenML will automatically extract all the details of the algorithm (dependencies, structure, and all hyperparameters), and upload them in the background.</p> <pre><code>    from sklearn import ensemble\n    from openml import tasks, runs\n\n    # Build any model you like.\n    clf = ensemble.RandomForestClassifier()\n\n    # Evaluate the model on a task\n    run = runs.run_model_on_task(clf, task)\n\n    # Share the results, including the flow and all its details.\n    run.publish()\n</code></pre>"},{"location":"concepts/flows/#reproducing-algorithms-and-experiments","title":"Reproducing algorithms and experiments","text":"<p>Given an OpenML run, the exact same algorithm or model, with exactly the same hyperparameters, can be reconstructed within the same machine learning library to easily reproduce earlier results. </p> <pre><code>    from openml import runs\n\n    # Rebuild the (scikit-learn) pipeline from run 9864498\n    model = openml.runs.initialize_model_from_run(9864498)\n</code></pre> <p>Note</p> <p>You may need the exact same library version to reconstruct flows. The API will always state the required version. We aim to add support for VMs so that flows can be easily (re)run in any environment </p>"},{"location":"concepts/openness/","title":"Openness and Authentication","text":"<p>You can download and inspect all datasets, tasks, flows and runs through the website or the API without creating an account. However, if you want to upload datasets or experiments, you need to create an account, sign in, and find your API key on your profile page.</p> <p>This key can then be used with any of the OpenML APIs.</p>"},{"location":"concepts/runs/","title":"Runs","text":""},{"location":"concepts/runs/#automated-reproducible-evaluations","title":"Automated reproducible evaluations","text":"<p>Runs are experiments (benchmarks) evaluating a specific flows on a specific task. As shown above, they are typically submitted automatically by machine learning libraries through the OpenML APIs), including lots of automatically extracted meta-data, to create reproducible experiments. With a few for-loops you can easily run (and share) millions of experiments.</p>"},{"location":"concepts/runs/#online-organization","title":"Online organization","text":"<p>OpenML organizes all runs online, linked to the underlying data, flows, parameter settings, people, and other details. See the many examples above, where every dot in the scatterplots is a single OpenML run.</p>"},{"location":"concepts/runs/#independent-server-side-evaluation","title":"Independent (server-side) evaluation","text":"<p>OpenML runs include all information needed to independently evaluate models. For most tasks, this includes all predictions, for all train-test splits, for all instances in the dataset, including all class confidences. When a run is uploaded, OpenML automatically evaluates every run using a wide array of evaluation metrics. This makes them directly comparable with all other runs shared on OpenML. For completeness, OpenML will also upload locally computed evaluation metrics and runtimes. </p> <p>New metrics can also be added to OpenML's evaluation engine, and computed for all runs afterwards. Or, you can download OpenML runs and analyse the results any way you like.</p> <p>Note</p> <p>Please note that while OpenML tries to maximise reproducibility, exactly reproducing all results may not always be possible because of changes in numeric libraries,  operating systems, and hardware.</p>"},{"location":"concepts/sharing/","title":"Sharing (under construction)","text":"<p>Currently, anything on OpenML can be shared publicly or kept private to a single user. We are working on sharing features that allow you to share your materials with other users without making them entirely public. Watch this space </p>"},{"location":"concepts/tagging/","title":"Tagging","text":"<p>Datasets, tasks, runs and flows can be assigned tags, either via the web interface or the API. These tags can be used to search and annotate datasets, or simply to better organize your own datasets and experiments.</p> <p>For example, the tag OpenML-CC18 refers to all tasks included in the OpenML-CC18 benchmarkign suite.</p>"},{"location":"concepts/tasks/","title":"Tasks","text":"<p>Tasks describe what to do with the data. OpenML covers several task types, such as classification and clustering. Tasks are containers including the data and other information such as train/test splits, and define what needs to be returned. They are machine-readable so that you can automate machine learning experiments, and easily compare algorithms evaluations (using the exact same train-test splits) against all other benchmarks shared by others on OpenML.</p>"},{"location":"concepts/tasks/#collaborative-benchmarks","title":"Collaborative benchmarks","text":"<p>Tasks are real-time, collaborative benchmarks (e.g. see MNIST below). In the Analysis tab, you can view timelines and leaderboards, and learn from all prior submissions to design even better algorithms.</p> <p></p>"},{"location":"concepts/tasks/#discover-the-best-algorithms","title":"Discover the best algorithms","text":"<p>All algorithms evaluated on the same task (with the same train-test splits) can be directly compared to each other, so you can easily look up which algorithms perform best overall, and download their exact configurations. Likewise, you can look up the best algorithms for similar tasks to know what to try first.</p> <p></p>"},{"location":"concepts/tasks/#automating-benchmarks","title":"Automating benchmarks","text":"<p>You can search and download existing tasks, evaluate your algorithms, and automatically share the results (which are stored in a run). Here's what this looks like in the Python API. You can do the same across hundreds of tasks at once.</p> <pre><code>    from sklearn import ensemble\n    from openml import tasks, runs\n\n    # Build any model you like\n    clf = ensemble.RandomForestClassifier()\n\n    # Download any OpenML task (includes the datasets)\n    task = tasks.get_task(3954)\n\n    # Automatically evaluate your model on the task\n    run = runs.run_model_on_task(clf, task)\n\n    # Share the results on OpenML.\n    run.publish()\n</code></pre> <p>You can create new tasks via the website or via the APIs as well.</p>"},{"location":"contributing/Client-API-Standards/","title":"Client Development","text":"<p>This page defines a minimal standard to adhere in programming APIs.</p>"},{"location":"contributing/Client-API-Standards/#configuration-file","title":"Configuration file","text":"<p>The configuration file resides in a directory <code>.openml</code> in the home directory of the user and is called config. It consists of <code>key = value</code> pairs which are seperated by newlines. The following keys are defined:</p> <ul> <li>apikey:<ul> <li>required to access the server</li> </ul> </li> <li>server:<ul> <li>default: <code>http://www.openml.org</code></li> </ul> </li> <li>verbosity:<ul> <li>0: normal output</li> <li>1: info output</li> <li>2: debug output</li> </ul> </li> <li>cachedir:<ul> <li>if not given, will default to <code>file.path(tempdir(), \"cache\")</code>.</li> </ul> </li> <li>arff.reader:<ul> <li><code>RWeka</code>: This is the standard Java parser used in Weka.</li> <li><code>farff</code>: The farff package lives below the mlr-org and is a newer, faster parser without Java.</li> </ul> </li> </ul>"},{"location":"contributing/Client-API-Standards/#caching","title":"Caching","text":""},{"location":"contributing/Client-API-Standards/#cache-invalidation","title":"Cache invalidation","text":"<p>All parts of the entities which affect experiments are immutable. The entities dataset and task have a flag <code>status</code> which tells the user whether they can be used safely.</p>"},{"location":"contributing/Client-API-Standards/#file-structure","title":"File structure","text":"<p>Caching should be implemented for</p> <ul> <li>datasets</li> <li>tasks</li> <li>splits</li> <li>predictions</li> </ul> <p>and further entities might follow in the future. The cache directory <code>$cache</code> should be specified by the user when invoking the API. The structure in the cache directory should be as following:</p> <ul> <li>One directory for the following entities:<ul> <li><code>$cache/datasets</code></li> <li><code>$cache/tasks</code></li> <li><code>$cache/runs</code></li> </ul> </li> <li>For every dataset there is an extra directory for which the name is the dataset ID, e.g. <code>$cache/datasets/2</code> for the dataset anneal.ORIG<ul> <li>The dataset should be called <code>dataset.arff</code></li> <li>Every other file should be named by the API call which was used to obtain it. The XML returned by invoking <code>openml.data.qualities</code> should therefore be called qualities.xml.</li> </ul> </li> <li>For every task there is an extra directory for which the name is the task ID, e.g. <code>$cache/tasks/1</code><ul> <li>The task file should be called <code>task.xml</code>.</li> <li>The splits accompanying a task are stored in a file <code>datasplits.arff</code>.</li> </ul> </li> <li>For every run there is an extra directory for which the name is the run ID, e.g. <code>$cache/run/1</code><ul> <li>The predictions should be called <code>predictions.arff</code>.</li> </ul> </li> </ul>"},{"location":"contributing/Communication-Channels/","title":"Communication Channels","text":"<p>We have several communication channels set up for different purposes:</p>"},{"location":"contributing/Communication-Channels/#github","title":"GitHub","text":"<p>https://github.com/openml</p> <ul> <li>Issues (members and users can complain)  </li> <li>Request new features  </li> </ul> <p>Anyone with a GitHub account can write issues. We are happy if people get involved by writing issues, so don't be shy </p>"},{"location":"contributing/Communication-Channels/#slack","title":"Slack","text":"<p>https://openml.slack.com</p> <ul> <li>Informal communication  </li> </ul> <p>We use slack for day to day discussions and news. If you want to join the OpenML slack chat, please message us (openmlHQ@googlegroups.com).</p>"},{"location":"contributing/Communication-Channels/#mailing-list","title":"Mailing List","text":"<p>https://groups.google.com/forum/#!forum/openml</p> <ul> <li>Information on upcoming workshop  </li> <li>Other major information  </li> <li>Urgent or important issues  </li> </ul> <p>If you want to receive information on major news or upcoming events, sign up for the mailing list. There is a private mailing list for OpenML core members which you can contact by sending an e-mail to openmlHQ@googlegroups.com.</p>"},{"location":"contributing/Communication-Channels/#twitter-open_ml","title":"Twitter (@open_ml)","text":"<p>https://twitter.com/open_ml</p> <ul> <li>News  </li> <li>Publicly relevant information  </li> </ul>"},{"location":"contributing/Communication-Channels/#blog","title":"Blog","text":"<p>https://medium.com/open-machine-learning/archive</p> <ul> <li>Tutorials  </li> <li>News  </li> <li>Info about papers  </li> </ul>"},{"location":"contributing/Contributing/","title":"How to Contribute","text":"<p>OpenML is an open source project, hosted on GitHub. We welcome everybody to help improve OpenML, and make it more useful for everyone.</p> <p>We want to make machine learning and data analysis simple, accessible, collaborative and open with an optimal division of labour between computers and humans.</p>"},{"location":"contributing/Contributing/#want-to-get-involved","title":"Want to get involved?","text":"<p>Awesome, we're happy to have you! </p> <p>OpenML is dependent on the community. If you want to help, please email us (openmlHQ@googlegroups.com). If you feel already comfortable you can help by opening issues or make a pull request on GitHub. We also have regular workshops you can join (they are announced on openml.org).</p>"},{"location":"contributing/Contributing/#who-are-we","title":"Who are we?","text":"<p>We are a group of friendly people who are excited about open science and machine learning. A list of people currently involved can be found here.</p>"},{"location":"contributing/Contributing/#we-need-help","title":"We need help!","text":"<p>We are currently looking for help with:</p> <ul> <li>User feedback (best via GitHub issues, but email is also fine)</li> <li>Frontend / UX / Design of the website</li> <li>Backend / API</li> <li>Outreach / making OpenML better known (especially in non-ML-communities, where people have data but no analysis experise)</li> <li>Helping with the interfaces (Python, WEKA, MOA, RapidMiner, Java, R; find the links to GitHub repos here)</li> <li>Helping with documenting the interfaces or the API</li> <li>What could we do better to get new users started? Help us to figure out what is difficult to understand about OpenML. If you are a new user, you are the perfect person for this!</li> </ul>"},{"location":"contributing/Contributing/#beginner-issues","title":"Beginner issues","text":"<p>Check out the issues labeled Good first issue or help wanted (you need to be logged into GitHub to see these)</p>"},{"location":"contributing/Contributing/#change-the-world","title":"Change the world","text":"<p>If you have your own ideas on how you want to contribute, please get in touch! We are very friendly and open to new ideas </p>"},{"location":"contributing/Contributing/#communication-channels","title":"Communication channels:","text":"<p>We have several communication channels set up for different purposes:</p>"},{"location":"contributing/Contributing/#github","title":"GitHub","text":"<p>https://github.com/openml</p> <ul> <li>Issues (members and users can complain)</li> <li>Request new features</li> </ul> <p>Anyone with a GitHub account can write issues. We are happy if people get involved by writing issues, so don't be shy </p> <p>Please post issues in the relevant issue tracker.</p> <ul> <li>:fa-github: OpenML Core - Web services and API</li> <li>:fa-github: Website - The (new) OpenML website</li> <li>:fa-github: Docs - The documentation pages</li> <li>:fa-github: Python API - The Python API</li> <li>:fa-github: R API - The OpenML R package</li> <li>:fa-github: Java API - The Java API and Java-based plugins</li> <li>:fa-github: Datasets - For issues about datasets</li> <li>:fa-github: Blog - The OpenML Blog</li> </ul>"},{"location":"contributing/Contributing/#slack","title":"Slack","text":"<p>https://openml.slack.com</p> <ul> <li>Informal communication</li> </ul> <p>We use slack for day to day discussions and news. If you want to join the OpenML slack chat, please message us (openmlHQ@googlegroups.com).</p>"},{"location":"contributing/Contributing/#mailing-list","title":"Mailing List","text":"<p>https://groups.google.com/forum/#!forum/openml</p> <ul> <li>Information on upcoming workshop</li> <li>Other major information</li> <li>Urgent or important issues</li> </ul> <p>If you want to receive information on major news or upcoming events, sign up for the mailing list. There is a privat mailing list for OpenML core members which you can contact by sending an e-mail to openmlHQ@googlegroups.com.</p>"},{"location":"contributing/Contributing/#twitter-open_ml","title":"Twitter (@open_ml)","text":"<p>https://twitter.com/open_ml</p> <ul> <li>News</li> <li>Publicly relevant information</li> </ul>"},{"location":"contributing/Contributing/#blog","title":"Blog","text":"<p>https://blog.openml.org</p> <ul> <li>Tutorials</li> <li>News</li> <li>Open discussions</li> </ul>"},{"location":"contributing/Contributing/#contributors-bot","title":"Contributors bot","text":"<p>We use all contributors bot to add contributors to the repository README. You can check how to use this here. You can contribute in a lot of ways including code, blogs, content, design and talks. You can find the emoji key here .</p>"},{"location":"contributing/Core-team/","title":"Core team","text":"<p>OpenML has many amazing contributors, which you can find on out team website. Should you be a contributor, but not on this page, let us know!</p> <p>Current members of the core team are:</p> <ul> <li>Joaquin Vanschoren</li> <li>Jan van Rijn</li> <li>Bernd Bischl</li> <li>Giuseppe Casaliccio</li> <li>Matthias Feurer</li> <li>Heidi Seibold</li> </ul> <p>You can contact us by emailing to openmlHQ@googlegroups.com.</p> <p>To get in touch with the broader community check out our communication channels.</p>"},{"location":"contributing/Datasets/","title":"Datasets","text":""},{"location":"contributing/Datasets/#data-formats","title":"Data Formats","text":"<p>To guarantee interoperability, we focus on a limited set of data formats. We aim to support all sorts of data, but for the moment we only fully support tabular data in the ARFF format. We are currently working on supporting a much wider range of formats.</p> <p>ARFF definition. Also check that attribute definitions do not mix spaces and tabs, and do not include end-of-line comments.</p>"},{"location":"contributing/Datasets/#data-repositories","title":"Data repositories","text":"<p>This is a list of public dataset repositories that offer additional useful machine learning datasets. These have widely varying data formats, so they require manual selection, parsing and meta-data extraction.</p> <p>A collection of sources made by different users</p> <ul> <li>https://github.com/caesar0301/awesome-public-datasets</li> <li>https://dreamtolearn.com/ryan/1001_datasets</li> <li>https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research</li> <li>https://pathmind.com/wiki/open-datasets</li> <li>https://paperswithcode.com/</li> <li>https://medium.com/towards-artificial-intelligence/best-datasets-for-machine-learning-data-science-computer-vision-nlp-ai-c9541058cf4f</li> <li>https://lionbridge.ai/datasets/the-50-best-free-datasets-for-machine-learning/</li> <li>https://www.v7labs.com/open-datasets?utm_source=v7&amp;utm_medium=email&amp;utm_campaign=edu_outreach</li> </ul> <p>Machine learning dataset repositories (mostly already in OpenML)</p> <ul> <li>UCI: https://archive.ics.uci.edu/ml/index.html</li> <li>KEEL: http://sci2s.ugr.es/keel/datasets.php</li> <li>LIBSVM: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/</li> <li>AutoWEKA datasets: http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/</li> <li>skData package: https://github.com/jaberg/skdata/tree/master/skdata</li> <li>Rdatasets: http://vincentarelbundock.github.io/Rdatasets/datasets.html</li> <li>DataBrewer: https://github.com/rmax/databrewer</li> <li>liac-arff: https://github.com/renatopp/arff-datasets</li> </ul> <p>MS Open datasets:</p> <ul> <li>https://azure.microsoft.com/en-us/services/open-datasets/catalog/</li> </ul> <p>APIs (mostly defunct):</p> <ul> <li>databrewer (Python): https://pypi.org/project/databrewer/</li> <li>PyDataset (Python): https://github.com/iamaziz/PyDataset (wrapper for Rdatasets?)</li> <li>RDatasets (R): https://github.com/vincentarelbundock/Rdatasets</li> </ul> <p>Time series / Geo data:</p> <ul> <li>Data commons: https://datacommons.org/</li> <li>UCR: http://timeseriesclassification.com/</li> <li>Older version: http://www.cs.ucr.edu/~eamonn/time_series_data/</li> </ul> <p>Deep learning datasets (mostly image data)</p> <ul> <li>https://www.tensorflow.org/datasets/catalog/overview</li> <li>http://deeplearning.net/datasets/</li> <li>https://deeplearning4j.org/opendata</li> <li>http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html</li> <li>https://paperswithcode.com/datasets</li> </ul> <p>Extreme classification:</p> <ul> <li>http://manikvarma.org/downloads/XC/XMLRepository.html</li> </ul> <p>MLData (down)</p> <ul> <li>http://mldata.org/</li> </ul> <p>AutoWEKA datasets:</p> <ul> <li>http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/</li> </ul> <p>Kaggle public datasets</p> <ul> <li>https://www.kaggle.com/datasets</li> </ul> <p>RAMP Challenge datasets</p> <ul> <li>http://www.ramp.studio/data_domains</li> </ul> <p>Wolfram data repository</p> <ul> <li>http://datarepository.wolframcloud.com/</li> </ul> <p>Data.world</p> <ul> <li>https://data.world/</li> </ul> <p>Figshare (needs digging, lots of Excel files)</p> <ul> <li>https://figshare.com/search?q=dataset&amp;quick=1</li> </ul> <p>KDNuggets list of data sets (meta-list, lots of stuff here):</p> <ul> <li>http://www.kdnuggets.com/datasets/index.html</li> </ul> <p>Benchmark Data Sets for Highly Imbalanced Binary Classification</p> <ul> <li>http://www.cs.gsu.edu/~zding/research/imbalance-data/x19data.txt</li> </ul> <p>Feature Selection Challenge Datasets</p> <ul> <li>http://www.nipsfsc.ecs.soton.ac.uk/datasets/</li> <li>http://featureselection.asu.edu/datasets.php</li> </ul> <p>BigML's list of 1000+ data sources</p> <ul> <li>http://blog.bigml.com/2013/02/28/data-data-data-thousands-of-public-data-sources/</li> </ul> <p>Massive list from Data Science Central.</p> <ul> <li>http://www.datasciencecentral.com/profiles/blogs/data-sources-for-cool-data-science-projects</li> </ul> <p>R packages (also see https://github.com/openml/openml-r/issues/185)</p> <ul> <li>http://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html</li> <li>mlbench</li> <li>Stata datasets: http://www.stata-press.com/data/r13/r.html</li> </ul> <p>UTwente Activity recognition datasets:</p> <ul> <li>http://ps.ewi.utwente.nl/Datasets.php</li> </ul> <p>Vanderbilt:</p> <ul> <li>http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets</li> </ul> <p>Quandl</p> <ul> <li>https://www.quandl.com</li> </ul> <p>Microarray data:</p> <ul> <li>http://genomics-pubs.princeton.edu/oncology/</li> <li>http://svitsrv25.epfl.ch/R-doc/library/multtest/html/golub.html</li> </ul> <p>Medical data:</p> <ul> <li>http://www.healthdata.gov/</li> <li>http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/PPRPAGES/pprdat.htm</li> <li>http://hcup-us.ahrq.gov/</li> <li>https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html</li> <li>https://nsduhweb.rti.org/respweb/homepage.cfm</li> <li>http://orwh.od.nih.gov/resources/policyreports/womenofcolor.asp</li> </ul> <p>Nature.com Scientific data repositories list</p> <ul> <li>https://www.nature.com/sdata/policies/repositories</li> </ul>"},{"location":"contributing/OpenML-Docs/","title":"Documenting","text":""},{"location":"contributing/OpenML-Docs/#general-documentation","title":"General Documentation","text":"<p>The general documentation (the one you are reading now) in written in MarkDown, can be easily edited by clicking the edit button (the pencil icon) on the top of every page. It will open up an editing page on GitHub (you do need to be logged in on GitHub). When you are done, add a small message explaining the change and click 'commit changes'. On the next page, just launch the pull request. We will then review it and approve the changes, or discuss them if necessary.</p> <p>The sources are generated by MkDocs, using the Material theme. Check these docs to see what is possible in terms of styling.</p> <p>Deployment</p> <p>To deploy the documentation manually, you need to have MkDocs and MkDocs-Material installed: <pre><code>pip install mkdocs\npip install mkdocs-material\npip install fontawesome_markdown\n</code></pre> To deploy the documentation locally, run <code>mkdocs serve</code> in the top directory (with the <code>mkdocs.yml</code> file). Any changes made after that will be hot-loaded.</p> <p>The documentation will be auto-deployed with every push or merge with the master branch of <code>https://www.github.com/openml/docs/</code>. In the background, a CI job will run <code>mkdocs gh-deploy</code>, which will build the HTML files and push them to the gh-pages branch of openml/docs. <code>https://docs.openml.org</code> is just a reverse proxy for <code>https://openml.github.io/docs/</code>.</p>"},{"location":"contributing/OpenML-Docs/#rest-api","title":"REST API","text":"<p>The REST API is documented using Swagger.io, in YAML. This generates a nice web interface that also allows trying out the API calls using your own API key (when you are logged in).</p> <p>You can edit the sources on SwaggerHub. When you are done, export to json and replace the downloads/swagger.json file in the OpenML main GitHub repository. You need to do a pull request that is then reviewed by us. When we merge the new file the changes are immediately available.</p> <p>The data API can be edited in the same way.</p>"},{"location":"contributing/OpenML-Docs/#python-api","title":"Python API","text":"<p>To edit the tutorial, you have to edit the <code>reStructuredText</code> files on openml-python/doc. When done, you can do a pull request.</p> <p>To edit the documentation of the python functions, edit the docstrings in the Python code. When done, you can do a pull request.</p> <p>Note</p> <p>Developers: A CircleCI job will automatically render the documentation on every GitHub commit, using Sphinx.</p>"},{"location":"contributing/OpenML-Docs/#r-api","title":"R API","text":"<p>To edit the tutorial, you have to edit the <code>Rmarkdown</code> files on openml-r/vignettes.</p> <p>To edit the documentation of the R functions, edit the Roxygen documention next to the functions in the R code.</p> <p>Note</p> <p>Developers: A Travis job will automatically render the documentation on every GitHub commit, using knitr. The Roxygen documentation is updated every time a new version is released on CRAN.</p>"},{"location":"contributing/OpenML-Docs/#java-api","title":"Java API","text":"<p>The Java Tutorial is written in markdown and can be edited the usual way (see above).</p> <p>To edit the documentation of the Java functions, edit the documentation next to the functions in the Java code.</p> <ul> <li>Javadocs: https://www.openml.org/docs/</li> </ul> <p>Note</p> <p>Developers: A Travis job will automatically render the documentation on every GitHub commit, using Javadoc.</p>"},{"location":"contributing/OpenML_definition/","title":"OpenML Definition","text":"<p>OpenML is at its core a database, from which entities can be downloaded and to which entities can be uploaded. Although there are various interfaces for these, at the core all communication with the database goes through the API. In this document, we describe the standard how to upload entities to OpenML and what the resulting database state will be.</p>"},{"location":"contributing/OpenML_definition/#data","title":"Data","text":"<p>Data is uploaded through the function post data. The following files are needed:</p> <ul> <li><code>description</code>: An XML adhiring to the XSD schema.</li> <li><code>dataset</code>: An ARFF file containing the data (optional, if not set, there should be an URL in the description, pointing to this file).   Uploading any other files will result in an error.</li> </ul>"},{"location":"contributing/OpenML_definition/#tasks","title":"Tasks","text":"<p>Tasks are uploaded through the function post task. The following files are needed:</p> <ul> <li><code>description</code>: An XML adhering to the XSD schema.   Uploading any other files will result in an error.</li> </ul> <p>The task file should contain several input fields. These are a name and value combination of fields that are marked to be relevant by the task type definition. There are several task type definitions, e.g.:</p> <ul> <li>Supervised Classification</li> <li>Supervised Regression</li> <li>Learning Curve</li> <li>Data Stream Classification</li> </ul> <p>Note that the task types themselves are flexible content (ideally users can contribute task types) and therefore the documents are not part of the OpenML definition. The task types define which input fields should be set, when creating a task.</p> <p>Duplicate tasks (i.e., same value for <code>task_type_id</code> and all <code>input</code> fields equal) will be rejected.</p> <p>When creating a task, the API checks for all of the input fields whether the input is legitimate. (Todo: describe the checks and what they depend on).</p>"},{"location":"contributing/OpenML_definition/#flow","title":"FLow","text":"<p>Flows are uploaded through the function post flow. The following file is needed:</p> <ul> <li><code>description</code>: An XML adhering to the XSD schema.   Uploading any other files will result in an error.</li> </ul> <p>Duplicate flows (i.e., same values for <code>name</code> and <code>external_version</code>) will be rejected.</p>"},{"location":"contributing/OpenML_definition/#runs","title":"Runs","text":"<p>Runs are uploaded through the function post run. The following files are needed:</p> <ul> <li><code>description</code>: An XML adhering to the XSD schema.</li> <li><code>predictions</code>: An ARFF file containing the predictions (optional, depending on the task).</li> <li><code>trace</code>: An ARFF file containing the run trace (optional, depending on the flow).   Uploading any other files will result in an error.</li> </ul>"},{"location":"contributing/OpenML_definition/#predictions","title":"Predictions","text":"<p>The contents of the prediction file depends on the task type.</p>"},{"location":"contributing/OpenML_definition/#task-type-supervised-classification","title":"Task type: Supervised classification","text":"<p>Example predictions file</p> <ul> <li>repeat NUMERIC</li> <li>fold NUMERIC</li> <li>row_id NUMERIC</li> <li>confidence.{$classname}: optional. various columns, describing the confidence per class. The values of these columns should add to 1 (precision 1e-6).</li> <li>(proposal) decision_function.{$classname}: optional. various columns, describing decision function per class.</li> <li>prediction {$classname}   Runs that have a different set of columns will be rejected.</li> </ul>"},{"location":"contributing/OpenML_definition/#trace","title":"Trace","text":"<p>Example trace file</p> <ul> <li>repeat: cross-validation repeat</li> <li>fold: cross-validation fold</li> <li>iteration: the index order within this repeat/fold combination</li> <li>evaluation (float): the evaluation score that was attached based on the validation set</li> <li>selected {True, False}: Whether in this repeat/run combination this was the selected hyperparameter configuration (exactly one should be tagged with True)</li> <li>Per optimized parameter a column that has the name of the parameter and the prefix \"parameter_\"</li> <li>setup_string: Due to legacy reasons accepted, but will be ignored by the default evaluation engine</li> </ul> <p>(open question) what is in the same fold/repeat combination the same config is ran multiple times with same evaluation? Traces that have a different set of columns will be rejected.</p>"},{"location":"contributing/OpenML_definition/#data-features","title":"Data Features","text":"<p>Data features are uploaded by the Java Evaluation Engine and will be documented later.</p>"},{"location":"contributing/OpenML_definition/#data-qualities","title":"Data Qualities","text":"<p>Data qualities are uploaded by the Java Evaluation Engine and will be documented later.</p>"},{"location":"contributing/OpenML_definition/#evaluations","title":"Evaluations","text":"<p>Evaluations are uploaded by Java Evaluation Engine and will be documented later.</p>"},{"location":"contributing/OpenML_definition/#trace-iterations","title":"Trace Iterations","text":"<p>Trace Iterations are uploaded by Java Evaluation Engine and will be documented later.</p>"},{"location":"contributing/Visual-Guidelines/","title":"Visual Guidelines","text":"<p>This page contains some visual guidelines that might be useful for you, dear contributor. While these guidelines are not mandatory, they would make the OpenML experience more pleasant and consistent for everyone.</p>"},{"location":"contributing/Visual-Guidelines/#colors","title":"Colors","text":"<ul> <li><p>Primary color: #1E88E5</p></li> <li><p>Primary color (dark): #000482</p></li> <li><p>Primary color (light): #b5b7ff</p></li> </ul>"},{"location":"contributing/Visual-Guidelines/#logos","title":"Logos","text":""},{"location":"contributing/resources/","title":"Resources","text":""},{"location":"contributing/resources/#resources","title":"Resources","text":""},{"location":"contributing/resources/#database-snapshots","title":"Database snapshots","text":"<p>Everything uploaded to OpenML is available to the community. The nightly snapshot of the public database contains all experiment runs, evaluations and links to datasets, implementations and result files. In SQL format (gzipped). You can also download the Database schema.</p> <p> Nightly database SNAPSHOT</p> <p>If you want to work on the website locally, you'll also need the schema for the 'private' database with non-public information.</p> <p> Private database schema</p>"},{"location":"contributing/resources/#legacy-resources","title":"Legacy Resources","text":"<p>OpenML is always evolving, but we keep hosting the resources that were used in prior publications so that others may still build on them.</p> <ul> <li> <p>:fa-database: The experiment database used in Vanschoren et al. (2012) Experiment databases. Machine Learning 87(2), pp 127-158. You'll need to import this database (we used MySQL) to run queries. The database structure is described in the paper. Note that most of the experiments in this database have been rerun using OpenML, using newer algorithm implementations and stored in much more detail.</p> </li> <li> <p>:fa-share-alt: The Expos\u00e9 ontology used in the same paper, and described in more detail here and here. Expos\u00e9 is used in designing our databases, and we aim to use it to export all OpenML data as Linked Open Data.</p> </li> </ul>"},{"location":"contributing/terms/","title":"Terms","text":""},{"location":"contributing/terms/#honor-code","title":"Honor Code","text":"<p>By joining OpenML, you join a special worldwide community of data scientists building on each other's results and connecting their minds as efficiently as possible. This community depends on your motivation to share data, tools and ideas, and to do so with honesty. In return, you will gain trust, visibility and reputation, igniting online collaborations and studies that otherwise may not have happened.</p> <p>By using any part of OpenML, you agree to:</p> <ul> <li>Give credit where credit is due. Cite the authors whose work you are building on, or build collaborations where appropriate.</li> <li>Give back to the community by sharing your own data as openly and as soon as possible, or by helping the community in other ways. In doing so, you gain visibility and impact (citations).</li> <li>Share data according to your best efforts. Everybody make mistakes, but we trust you to correct them as soon as possible. Remove or flag data that cannot be trusted.</li> <li>Be polite and constructive in all discussions. Criticism of methods is welcomed, but personal criticisms should be avoided.</li> <li>Respect circles of trust. OpenML allows you to collaborate in 'circles' of trusted people to share unpublished results. Be considerate in sharing data with people outside this circle.</li> <li>Do not steal the work of people who openly share it. OpenML makes it easy to find all shared data (and when it was shared), thus everybody will know if you do this.</li> </ul>"},{"location":"contributing/terms/#terms-of-use","title":"Terms of Use","text":"<p>You agree that you are responsible for your own use of OpenML.org and all content submitted by you, in accordance with the Honor Code and all applicable local, state, national and international laws.</p> <p>By submitting or distributing content from OpenML.org, you affirm that you have the necessary rights, licenses, consents and/or permissions to reproduce and publish this content. You cannot upload sensitive or confidential data. You, and not the developers of OpenML.org, are solely responsible for your submissions.</p> <p>By submitting content to OpenML.org, you grant OpenML.org the right to host, transfer, display and use this content, in accordance with your sharing settings and any licences granted by you. You also grant to each user a non-exclusive license to access and use this content for their own research purposes, in accordance with any licences granted by you.</p> <p>You may maintain one user account and not let anyone else use your username and/or password. You may not impersonate other persons.</p> <p>You will not intend to damage, disable, or impair any OpenML server or interfere with any other party's use and enjoyment of the service. You may not attempt to gain unauthorized access to the Site, other accounts, computer systems or networks connected to any OpenML server. You may not obtain or attempt to obtain any materials or information not intentionally made available through OpenML.</p> <p>Strictly prohibited are content that defames, harasses or threatens others, that infringes another's intellectual property, as well as indecent or unlawful content, advertising, or intentionally inaccurate information posted with the intent of misleading others. It is also prohibited to post code containing viruses, malware, spyware or any other similar software that may damage the operation of another's computer or property.</p>"},{"location":"contributing/backend/API-development/","title":"API Development","text":""},{"location":"contributing/backend/API-development/#golden-rules-for-development","title":"Golden Rules for Development","text":"<ol> <li>Code Maintainability before anything else. The code has to be understandable, and if not conflicting with that, short. Avoid code duplications as much as possible.</li> <li>The API controller is the only entity giving access to the API models. Therefore, the responsibility for API access can be handled by the controller</li> <li>Read-Only operations are of the type GET. Operations that make changes in the database are of type POST or DELETE. Important, because this is the way the controller determines to allow users with a given set of privileges to access functions.</li> <li>Try to avoid direct queries to the database. Instead, use the respective models functions: 'get()', 'getWhere()', 'getById()', insert(), etc (Please make yourself familiar with the basic model: read-only and write)</li> <li>No external program/script execution during API calls (with one exception: data split generation). This makes the API unnecessarily slow, hard to debug and vulnerable to crashes. If necessary, make a cronjob that executes the program / script</li> </ol>"},{"location":"contributing/backend/API-development/#important-resources","title":"Important resources","text":"<p>API docs: www.openml.org/api_docs</p> <p>Controller: https://github.com/openml/OpenML/blob/master/openml_OS/controllers/Api_new.php</p> <p>Models: https://github.com/openml/OpenML/tree/master/openml_OS/models/api/v1</p> <p>Templates: https://github.com/openml/OpenML/tree/master/openml_OS/views/pages/api_new/v1</p>"},{"location":"contributing/backend/API-development/#backend-code-structure","title":"Backend code structure","text":"<p>The high-level architecture of the website, including the controllers for different parts of the website (REST API, html, ...) and connections to the database.</p>"},{"location":"contributing/backend/API-development/#code","title":"Code","text":"<p>The source code is available in the 'website' repository: https://github.com/openml/website</p>"},{"location":"contributing/backend/API-development/#important-files-and-folders","title":"Important files and folders","text":"<p>In this section we go through all important files and folder of the system.</p>"},{"location":"contributing/backend/API-development/#root-directory","title":"Root directory","text":"<p>The root directory of OpenML contains the following files and folders.</p> <ul> <li> <p>system: This folder contains all files provided by   CodeIgniter 2.1.3. The contents of this folder is   beyond the scope of this document, and not relevant for extending   OpenML. All the files in this folder are in the same state as they   were provided by Ellislabs, and none of these files should ever be   changed.</p> </li> <li> <p>sparks: Sparks is a package management system for   Codeigniter that allows for instant installation of libraries into   the application. This folder contains two libraries provided by   third party software developers, oauth1 (based on version 1 the   oauth protocol) and oauth2 (similarly, based on version 2 of the   oauth protocol). The exact contents of this folder is beyond the   scope of this document and not relevant for extending OpenML.</p> </li> <li> <p>openml_OS: All files in this folder are written specifically   for OpenML. When extending the functionality OpenML, usually one of   the files in this folder needs to be adjusted. As a thorough   understanding of the contents of this folder is vital for extending   OpenML, we will discuss the contents of this folder in   [[URL Mapping]] in more detail.</p> </li> <li> <p>index.php: This is the \u201cbootstrap\u201d file of the system.   Basically, every page request on OpenML goes through this file (with   the css, images and javascript files as only exception). It then   determines which CodeIgniter and OpenML files need to be included.   This file should not be edited.</p> </li> <li> <p>.htaccess: This file (which configures the Apache Rewrite   Engine) makes sure that all URL requests will be directed to   <code>index.php</code>. Without this file, we would need to include <code>index.php</code>   explicitly in every URL request. This file makes sure that all other   URL requests without <code>index.php</code> embedded in it automatically will   be transformed to <code>index.php</code>. Eg.,   http://www.openml.org/frontend/page/home will be rewritten to   http://www.openml.org/index.php/frontend/page/home. This will be   explained in detail in [[URL Mapping]].</p> </li> <li> <p>css: A folder containing all stylesheets. These are important   for the layout of OpenML.</p> </li> <li> <p>data: A folder containing data files, e.g., datasets,   implementation files, uploaded content. Please note that this folder   does not necessarily needs to be present in the root directory. The   OpenML Base Config file determines the   exact location of this folder.</p> </li> <li> <p>downloads: Another data folder, containing files like the most   recent database snapshot.</p> </li> <li> <p>img: A folder containing all static images shown on the webpage.</p> </li> <li> <p>js: A folder containing all used Javascript files and libraries,   including third party libraries like jQuery and datatables.</p> </li> <li> <p>Various other files, like .gitignore, favicon.ico, etc.</p> </li> </ul>"},{"location":"contributing/backend/API-development/#openml_os","title":"openml_OS","text":"<p>This folder is (in CodeIgniter jargon) the \u201cApplication folder\u201d, and contains all files relevant to OpenML. Within this folder, the following folders should be present: (And also some other folders, but these are not used by OpenML)</p> <ul> <li> <p>config: A folder containing all config files. Most notably, it   contains the file BASE_CONFIG.php, in which all system   specific variables are set; the config items within this file   differs over various installations (e.g., on localhost,   <code>openml.org</code>). Most other config files, like   database.php, will receive their values from   BASE_CONFIG.php. Other important config files are   autoload.php, determining which CodeIgniter / OpenML   files will be loaded on any request, openML.php,   containing config items specific to OpenML, and   routes.php, which will be explained in   [[URL Mapping]].</p> </li> <li> <p>controllers: In the Model/View/Controller design pattern, all   user interaction goes through controllers. In a webapplication   setting this means that every time a URL gets requested, exactly one   controller gets invoked. The exact dynamics of this will be   explained in [[URL Mapping]].</p> </li> <li> <p>core: A folder that contains CodeIgniter specific files. These   are not relevant for the understanding of OpenML.</p> </li> <li> <p>helpers: This folder contains many convenience functions.   Wikipedia states: \u201cA convenience function is a non-essential   subroutine in a programming library or framework which is intended   to ease commonly performed tasks\u201d. For example the   file_upload_helper.php contains many functions that   assist with uploading of files. Please note that a helper function   must be explicitly loaded in either the autoload config or the files   that uses its functions.</p> </li> <li> <p>libraries: Similar to sparks, this folder contains libraries   specifically written for CodeIgniter. For example, the library used   for all user management routines is in this folder.</p> </li> <li> <p>models: In the Model/View/Controller design pattern, models   represent the state of the system. In a webapplication setting, you   could say that a model is the link to the database. In OpenML,   almost all tables of the database are represented by a model. Each   model has general functionality applicable to all models (e.g.,   retrieve all records, retrieve record with constraints, insert   record) and functionality specific to that model (e.g., retrieve a   dataset that has certain data properties). Most models extend an   (abstract) base class, located in the abstract folder.   This way, all general functionality is programmed and maintained in   one place.</p> </li> <li> <p>third_party: Although the name might suggests differently, this   folder contains all OpenML Java libraries.</p> </li> <li> <p>views: In the Model/View/Controller design pattern, the views   are the way information is presented on the screen. In a   webapplication setting, a view usually is a block of (PHP generated)   HTML code. The most notable view is frontend_main.php,   which is the template file determining the main look and feel of   OpenML. Every single page also has its own specific view (which is   parsed within frontend_main.php). These pages can be   found (categorized by controller and name) in the pages   folder. More about this structure is explained in   [[URL Mapping]].</p> </li> </ul>"},{"location":"contributing/backend/API-development/#frontend-code-structure","title":"Frontend code structure","text":"<p>Architecture and libraries involved in generating the frontend functions.</p> <p>Code: https://github.com/openml/website/tree/master/openml_OS/views</p>"},{"location":"contributing/backend/API-development/#high-level","title":"High-level","text":"<p>All pages are generated by first loading frontend_main.php. This creates the 'shell' in which the content is loaded. It loads all css and javascript libraries, and contains the html for displaying headers and footers.</p>"},{"location":"contributing/backend/API-development/#create-new-page","title":"Create new page","text":"<p>The preferred method is creating a new folder into the folder <code>&lt;root_directory&gt;/openml_OS/views/pages/frontend</code> This page can be requested by <code>http://www.openml.org/frontend/page/&lt;folder_name&gt;</code> or just <code>http://www.openml.org/&lt;folder_name&gt;</code> This method is preferred for human readable webpages, where the internal actions are simple, and the output is complex. We will describe the files that can be in this folder.</p> <ul> <li> <p>pre.php: Mandatory file. Will be executed first. Do not make   this file produce any output! Can be used to pre-render data, or set   some variables that are used in other files.</p> </li> <li> <p>body.php: Highly recommended file. Intended for displaying the   main content of this file. Will be rendered at the right location   within the template file (<code>frontend_main.php</code>).</p> </li> <li> <p>javascript.php: Non-mandatory file. Intended for javascript   function on which <code>body.php</code> relies. Will be rendered within a   javascript block in the header of the page.</p> </li> <li> <p>post.php: Non mandatory file. Will only be executed when a POST   request is done (e.g., when a HTML form was send using the POST   protocol). Will be executed after <code>pre.php</code>, but before the   rendering process (and thus, before <code>body.php</code> and   <code>javascript.php</code>). Should handle the posted input, e.g., file   uploads.</p> </li> </ul> <p>It is also recommended to add the newly created folder to the mapping in the <code>routes.php</code> config file. This way it can also be requested by the shortened version of the URL. (Note that we deliberately avoided to auto-load all pages into this file using a directory scan, as this makes the webplatform slow. )</p>"},{"location":"contributing/backend/API-development/#url-to-page-mapping","title":"URL to Page Mapping","text":"<p>Most pages in OpenML are represented by a folder in /openml_OS/views/pages/frontend The contents of this folder will be parsed in the template <code>frontend_main.php</code> template, as described in [[backend]]. In this section we explain the way an URL is mapped to a certain OpenML page."},{"location":"contributing/backend/API-development/#url-anatomy","title":"URL Anatomy","text":"<p>By default, CodeIgniter (and OpenML) accepts a URL in the following form: <code>http://www.openml.org/index.php/&lt;controller&gt;/&lt;function&gt;/&lt;p1&gt;/&lt;pN&gt;/&lt;free&gt;</code> The various parts in the URL are divided by slashes. Every URL starts with the protocol and server name (in the case of OpenML this is <code>http://www.openml.org/</code>). This is followed by the bootstrap file, which is always the same, i.e., <code>index.php</code>. The next part indicates the controller that needs to be invoked; typically this is <code>frontend</code>, <code>rest_api</code> or <code>data</code>, but it can be any file from the <code>openml_OS</code> folder <code>controllers</code>. Note that the suffix <code>.php</code> should not be included in the URL.</p> <p>The next part indicates which function of the controller should be invoked. This should be a existing, public function from the controller that is indicated in the controller part. These functions might have one or more parameters that need to be set. This is the following part of the URL (indicated by <code>p1</code> and <code>pN</code>). The parameters can be followed by anything in free format. Typically, this free format is used to pass on additional parameters in <code>name</code> - <code>value</code> format, or just a way of adding a human readable string to the URL for SEO purposes.</p> <p>For example, the following URL <code>http://www.openml.org/index.php/frontend/page/home</code> invokes the function <code>page</code> from the <code>frontend</code> controller and sets the only parameter of this function, <code>$indicator</code>, to value <code>home</code>. The function <code>page</code> loads the content of the specified folder (<code>$indicator</code>) into the main template. In this sense, the function <code>page</code> can be seen as some sort of specialized page loader.</p>"},{"location":"contributing/backend/API-development/#url-shortening","title":"URL Shortening","text":"<p>Since it is good practice to have URL\u2019s as short as possible, we have introduced some logic that shortens the URL\u2019s. Most importantly, the URL part that invokes <code>index.php</code> can be removed at no cost, since this file is always invoked. For this, we use Apache\u2019s rewrite engine. Rules for rewriting URL\u2019s can be found in the <code>.htaccess</code> file, but is suffices to say that any URL in the following format <code>http://www.openml.org/index.php/&lt;controller&gt;/&lt;function&gt;/&lt;params&gt;</code> can due to the rewrite engine also be requested with <code>http://www.openml.org/&lt;controller&gt;/&lt;function&gt;/&lt;params&gt;</code></p> <p>Furthermore, since most of the pages are invoked by the function <code>page</code> of the <code>frontend</code> controller (hence, they come with the suffix <code>frontend/page/page_name</code>) we also created a mapping that maps URL\u2019s in the following form <code>http://www.openml.org/&lt;page_name&gt;</code> to <code>http://www.openml.org/frontend/page/&lt;page_name&gt;</code> Note that Apache\u2019s rewrite engine will also add <code>index.php</code> to this. The exact mapping can be found in <code>routes.php</code> config file.</p>"},{"location":"contributing/backend/API-development/#additional-mappings","title":"Additional Mappings","text":"<p>Additionally, a mapping is created from the following type of URL: <code>http://www.openml.org/api/&lt;any_query_string&gt;</code> to <code>http://www.openml.org/rest_api/&lt;any_query_string&gt;</code> This was done for backwards compatibility. Many plugins make calls to the not-existing <code>api</code> controller, which are automatically redirected to the <code>rest_api</code> controller.</p>"},{"location":"contributing/backend/API-development/#exceptions","title":"Exceptions","text":"<p>It is important to note that not all pages do have a specific page folder. The page folders are a good way of structuring complex GUI\u2019s that need to be presented to the user, but in cases where the internal state changes are more important than the GUI\u2019s, it might be preferable to make the controller function print the output directly. This happens for example in the functions of <code>rest_api.php</code> and <code>free_query.php</code> (although the former still has some files in the views folder that it refers to).</p>"},{"location":"contributing/backend/API-development/#xsd-schemas","title":"XSD Schemas","text":"<p>In order to ensure data integrity on the server, data that passed to upload functions is checked against XSD schema's. This ensures that the data that is uploaded is in the correct format, and does not contain any illegal characters. XSD schema's can be obtained through the API (exact links are provided in the API docs, but for example: https://www.openml.org/api/v1/xsd/openml.data.upload (where openml.data.upload can be replaced by any other schema's name). Also XML examples are provided, e.g., https://www.openml.org/api/v1/xml_example/data . The XSD schema's are exactly the same as used on the server. Whenever an upload fails and the server mentions an XML/XSD verification error, please run the uploaded xml against one of the provided XSD schema's, for example on this webtool: http://www.freeformatter.com/xml-validator-xsd.html</p> <p>In order to maintain one XSD schema for both uploading and downloading stuff, the XSD sometimes contains more fields than seem necessary from the offset. Usually, the additional fields that are indicated as such in the comments (for example, in the upload dataset xsd this are the id, upload_date, etc fields). The XSD's maintain basically three consistencies functions:</p> <ul> <li>Ensure that the correct fields are uploaded</li> <li>Ensure that the fields contain the correct data types.</li> <li>Ensure that the fields do not contain to much characters for the database to upload.</li> </ul> <p>For the latter two, it is important to note that the XSD seldom accept default string content (i.e., xs:string). Rather, we use self defined data types, that use regular expressions to ensure the right content. Examples of these are oml:system_string128, oml:casual_string128, oml:basic_latin128, where the oml prefix is used, the name indicates the level of restriction and the number indicates the maximum size of the field.</p> <p>IMPORTANT: The maximum field sizes are (often) chosen with great care. Do not extend them without consulting other team members.</p>"},{"location":"contributing/backend/API-development/#user-authentication","title":"User authentication","text":"<p>Authentication towards the server goes by means of a so-called api_key (a hexa-decimal string which uniquely identifies a user). Upon interaction with the server, the client passes this api_key to the server, and the server checks the rights of the user. Currently this goes by means of a get or post variable, but in the future we might want to use a header field (because of security). It is recommended to refresh your api_key every month.</p> <p>IMPORTANT: Most authentication operations are handled by the ION_Auth library (http://benedmunds.com/ion_auth/). DO NOT alter information directly in the user table, always use the ION_Auth API.</p> <p>A user can be part of one or many groups. The following user groups exists:</p> <ol> <li>Admin Group: With great power comes great responsibility. Admin users can overrule all security checks on the server, i.e., delete a dataset or run that is not theirs, or even delete a flow that contains runs.</li> <li>Normal Group: Level that is required for read/write interaction with the server. Almost all users are part of this group.</li> <li>Read-only Group: Level that can be used for read interaction with the server. If a user is part of this group, but not part of 'Normal Group', he is allowed to download content, but can not upload or delete content.</li> <li>Backend Group: (Work in Progress) Level that has more privileges than 'Normal Group'. Can submit Data Qualities and Evaluations.</li> </ol> <p>The ION_Auth functions in_group(), add_to_group(), remove_from_group() and get_users_groups() are key towards interaction with these tables.</p>"},{"location":"contributing/backend/Java-App/","title":"Evaluation Engine","text":"<p>The Java App is used for a number of OpenML components, such as the ARFF parser and Evaluation engine, which depend on the Weka API. It is invoked from the OpenML API by means of a CLI interface. Typically, a call looks like this:</p> <p><code>java -jar webapplication.jar -config \"api_key=S3CR3T_AP1_K3Y\" -f evaluate_run -r 500</code></p> <p>Which in this case executes the webapplication jar, invokes the function \"evaluate run\" and gives it parameter run id 500. The config parameter can be used to set some config items, in this case the api_key is mandatory. Every OpenML user has an api_key, which can be downloaded from their OpenML profile page. The response of this function is a call to the OpenML API uploading evaluation results to the OpenML database. Note that in this case the PHP website invokes the Java webapplication, which makes a call to the PHP website again, albeit another endpoint. </p> <p>The webapplication does not have direct writing rights into the database. All communication to the database goes by means of the OpenML Connector, which communicates with the OpenML API. As a consequence, the webapplication could run on any system, i.e., there is no formal need for the webapplication to be on the same server as the website code. This is important, since this created modularity, and not all servers provide a command line interface to PHP scripts.</p> <p>Another example is the following:</p> <p><code>java -jar webapplication -config \"api_key=S3CR3T_AP1_K3Y\" -f all_wrong -r 81,161 -t 59</code></p> <p>Which takes a comma separated list of run ids (no spaces) and a task id as input and outputs the test examples on the dataset on which all algorithms used in the runs produced wrong examples (in this case, weka.BayesNet_K2 and weka.SMO, respectively). An error will be displayed if there are runs not consistent with the task id in there. </p>"},{"location":"contributing/backend/Java-App/#extending-the-java-app","title":"Extending the Java App","text":"<p>The bootstrap class of the webapplication is</p> <p><code>org.openml.webapplication.Main</code></p> <p>It automatically checks authentication settings (such as api_key) and the determines which function to invoke. </p> <p>It uses a switch-like if - else contruction to facilitate the functionalities of the various functions. Additional functions can be added to this freely. From there on, it is easy to add functionality to the webapplication. </p> <p>Parameters are handled using the Apache Commons CommandLineParser class, which makes sure that the passed parameters are available to the program. </p> <p>In order to make new functionalities available to the website, there also needs to be programmed an interface to the function, somewhere in the website. The next section details on that. </p>"},{"location":"contributing/backend/Java-App/#interfacing-from-the-openml-api","title":"Interfacing from the OpenML API","text":"<p>By design, the REST API is not allowed to communicate with the Java App. All interfaces with the Java webapplication should go through other controllers of the PHP CodeIgniter framework., for example api_splits. Currently, the website features two main API's. These are represented by a Controller. Controllers can be found in the folder openml_OS/controllers. Here we see: * api_new.php, representing the REST API * api_splits.php, representing an API interfacing to the Java webapplication. </p>"},{"location":"contributing/backend/Local-Installation/","title":"Local Installation","text":""},{"location":"contributing/backend/Local-Installation/#1using-docker","title":"1.Using Docker","text":"<p>The easiest way to set up a local version of OpenML is to use Docker Compose following the instructions here (thanks to Rui Quintino!): https://github.com/openml/openml-docker-dev</p> <p>If you run into problems, please post an issue in the same github repo.</p>"},{"location":"contributing/backend/Local-Installation/#2-installation-from-scratch","title":"2. Installation from scratch","text":"<p>If you want to install a local version of OpenML from scratch please follow the steps mentioned below:</p>"},{"location":"contributing/backend/Local-Installation/#requirements","title":"Requirements","text":"<p>You'll need to have the following software running: * Apache Webserver, (with the rewrite module enabled. Is installed by default, not enabled.) * MySQL Server. * PHP 5.5 or higher (comes also with Apache) Or just a XAMP (Mac), LAMP (Linux) or WAMP (Windows) package, which conveniently contains all these applications.</p>"},{"location":"contributing/backend/Local-Installation/#databases","title":"Databases","text":"<p>Next, OpenML runs on two databases, a public database with all experiment information, and a private database, with information like user accounts etc. The latest version of both databases can be downloaded here: https://docs.openml.org/resources</p> <p>Obviously, the private database does not include any actual user account info.</p>"},{"location":"contributing/backend/Local-Installation/#backend","title":"Backend","text":"<p>The source code is available in the 'OpenML' repository: https://github.com/openml/OpenML</p> <p>OpenML is written in PHP, and can be 'installed' by copying all files in the 'www' or 'public_html' directory of Apache.</p> <p>After that, you need to provide your local paths and database accounts and passwords using the config file in: 'APACHE_WWW_DIR'/openml_OS/config/BASE_CONFIG.php.</p> <p>If everything is configured correctly, OpenML should now be running.</p>"},{"location":"contributing/backend/Local-Installation/#search-indices","title":"Search Indices","text":"<p>If you want to run your own (separate) OpenML instance, and store your own data, you'll also want to build your own search indices to show all data on the website. The OpenML website is based on the ElasticSearch stack. To install it, follow the instructions here: http://knowm.org/how-to-set-up-the-elk-stack-elasticsearch-logstash-and-kibana/</p>"},{"location":"contributing/backend/Local-Installation/#initialization","title":"Initialization","text":"<p>This script wipes all OpenML server data and rebuilds the database and search index. Replace 'openmldir' with the directory where you want OpenML to store files.</p> <pre><code># delete data from server\nsudo rm -rf /openmldir/*\nmkdir /openmldir/log\n\n# delete database\nmysqladmin -u \"root\" -p\"yourpassword\" DROP openml_expdb\nmysql -h localhost -u root -p\"yourpassword\" -e \"TRUNCATE openml.file;\"\n\n# reset ES search index\necho \"Deleting and recreating the ES index: \"\ncurl -XDELETE http://localhost:9200/openml\ncurl -XPUT 'localhost:9200/openml?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"settings\" : {\n        \"index\" : {\n            \"number_of_shards\" : 3,\n            \"number_of_replicas\" : 2\n        }\n    }\n}\n'\n\n# go to directory with the website source code\ncd /var/www/openml.org/public_html/\n\n# reinitiate the database\nmysql -u root -p\"yourpassword!\" &lt; downloads/openml_expdb.sql\n\n# fill important columns\nsudo php index.php cron install_database\n\n# rebuild search index\nsudo php index.php cron initialize_es_indices\nsudo php index.php cron build_es_indices\n\nsudo chown apache:apache /openmldir/log\nsudo chown apache:apache /openmldir/log/*\n</code></pre>"},{"location":"contributing/website/Dash/","title":"Dash visualization","text":"<p>Dash is a python framework which is suitable for building data visualization dashboards using pure python. Dash is written on top of plotly, react and flask and the graphs are defined using plotly python. The dash application is composed of two major parts :</p> <ul> <li><code>Layout</code> - Describes how the dashboard looks like</li> <li><code>Callbacks</code> - Used to update graphs, tables in the layout and makes the dashboard interactive.</li> </ul>"},{"location":"contributing/website/Dash/#files","title":"Files","text":"<p>The dash application is organized as follows:</p> <ul> <li> <p><code>dashapp.py</code></p> </li> <li> <p>Creates the dash application</p> </li> <li>The dash app is embedded in the flask app passed to <code>create_dash_app</code> function</li> <li> <p>This file need not be modified to create a new plot</p> </li> <li> <p><code>layouts.py</code></p> </li> <li> <p>contains the layout for all the pages</p> </li> <li><code>get_layout_from_data</code>- returns layout of data visualization</li> <li><code>get_layout_from_task</code>- returns layout of taskvisualization</li> <li><code>get_layout_from_flow</code>- returns layout of flow visualization</li> <li><code>get_layout_from_run</code> - returns layout of run visualization</li> <li> <p>This file needs to be modified to add a new plot (data, task, flow, run)</p> </li> <li> <p><code>callbacks.py</code></p> </li> <li>Registers all the callbacks for the dash application</li> <li>This file needs to be modified to add a new plot, especially if the plot needs to be interactive</li> </ul>"},{"location":"contributing/website/Dash/#how-the-dashboard-works","title":"How the dashboard works","text":"<p>In this dash application, we need to create the layout of the page dynamically based on the entered URL. For example, [http://127.0.0.1:5000/dashboard/data/5] needs to return the layout for dataset id #5 whereas [http://127.0.0.1:5000/dashboard/run/5] needs to return the layout for run id #5.</p> <p>Hence , the dash app is initially created with a dummy <code>app.layout</code> by dashapp.py and the callbacks are registered for the app using <code>register_callbacks</code> function.</p> <ul> <li> <p>render_layout is the callback which dynamically renders layout. Once the dash app is running, the first callback which is fired is <code>render_layout.</code>   This is the main callback invoked when a URL with a data , task, run or flow ID is entered.   Based on the information in the URL, this method returns the layout.</p> </li> <li> <p>Based on the URL, get_layout_from_data, get_layout_from_task, get_layout_from_flow, get_layout_from_run are called.   These functions define the layout of the page - tables, html Divs, tabs, graphs etc.</p> </li> <li> <p>The callbacks corresponding to each component in the layout are invoked to update the components dynamically and   make the graphs interactive. For example, update_scatter_plot in <code>data_callbacks.py</code> updates the scatter plot   component in the data visualization dashboard.</p> </li> </ul>"},{"location":"contributing/website/Flask/","title":"Flask backend","text":"<p>We use Flask as our web framework. It handles user authentication, dataset upload, task creation, and other aspects that require server-side interaction. It is designed to be independent from the OpenML API. This means that you can use it to create your own personal frontend for OpenML, using the main OpenML server to provide the data. Of course, you can also link it to your own local OpenML setup.</p>"},{"location":"contributing/website/Flask/#design","title":"Design","text":"<p>Out flask app follows Application factories design pattern. A new app instance can be created by: <pre><code>    from autoapp import create_app\n    app = create_app(config_object)\n</code></pre></p> <p>The backend is designed in a modular fashion with flask Blueprints. Currently, the flask app consists of two blueprints public and user:</p> <ul><li>Public blueprint: contains routes that do not require user authentication or authorization. like signup and forgot password.</li> <li>User blueprint: Contains routes which require user authentication like login, changes in profile and fetching API key.</li></ul> <p>New blueprints can be registered in `server/app.py` with register_blueprints function:</p> <pre><code>    def register_blueprints(app):\n        app.register_blueprint(new_blueprint)\n</code></pre>"},{"location":"contributing/website/Flask/#database-setup","title":"Database setup","text":"<p>If you want o setup a local user database similar to OpenML then follow these steps:</p> <ol> <li>Install MySQL</li> <li>Create a new database 'openml'</li> <li>Set current database to 'openml' via use method</li> <li>Download users.sql file from openml.org github repo and add it in the openml db via \"mysql -u root -p openml &lt; users.sql\"</li> <li>Edit the database path in `server/extensions.py` and `server/config.py`</li> </ol> <p>Note: Remember to add passwords and socket extension address(if any) in both in <code>server/extensions.py</code> and <code>server/config.py</code> </p>"},{"location":"contributing/website/Flask/#security","title":"Security","text":"<p>Flask backend uses JSON web tokens for all the user handling tasks. Flask JWT extended library is used to bind JWT with the flask app. Current Mechanism is :</p> <ol> <li> User logs in.</li> <li> JWT token is assigned to user and sent with every request to frontend.</li> <li> All the user information can only be accessed with a JWT token like edit profile and API-key.</li> <li> The JWT token is stored in local memory of the browser.</li> <li> The token get expired after 2 hours or get blacklisted after logout.</li> </ol> <p>JWT is registered as an extension in `server/extensions.py`. All the user password hash are saved in Argon2 format with the new backend.</p>"},{"location":"contributing/website/Flask/#registering-extensions","title":"Registering Extensions","text":"<p>To register a new extension to flask backend extension has to be added in <code>server/extensions.py</code> and initialized in server/app.py. Current extensions are : flask_argon2, flask_bcrypt, flask_jwt_extended and flask_sqlalchemy.</p>"},{"location":"contributing/website/Flask/#configuring-app","title":"Configuring App","text":"<p>Configuration variables like secret keys, Database URI and extension configurations are specified in  <code>server/config.py</code> with Config object, which is supplied to the flask app during initialization.</p>"},{"location":"contributing/website/Flask/#creating-a-new-route","title":"Creating a new route","text":"<p>To create a new route in backend you can add the route in <code>server/public/views.py</code> or <code>server/user/views.py</code> (if it requires user authorisation or JWT usage in any way).  </p>"},{"location":"contributing/website/Flask/#bindings-to-openml-server","title":"Bindings to OpenML server","text":"<p>You can specify which OpenML server to connect to. This is stored in the <code>.env</code> file in the main directory. It is set to the main OpenML server by default:</p> <pre><code>    ELASTICSEARCH_SERVER=https://www.openml.org/es\n    OPENML_SERVER=https://www.openml.org\n</code></pre> <p>The ElasticSearch server is used to download information about datasets, tasks, flows and runs, as well as to power the frontend search. The OpenML server is used for uploading datasets, tasks, and anything else that requires calls to the OpenML API.</p>"},{"location":"contributing/website/Flask/#bindings-to-frontend","title":"Bindings to frontend","text":"<p>The frontend is generated by React. See below for more information. The React app is loaded as a static website. This is done in Flask setup in file <code>server.py</code>.</p> <pre><code>    app = Flask(__name__, static_url_path='', static_folder='src/client/app/build')\n</code></pre> <p>It will find the React app there and load it.</p>"},{"location":"contributing/website/Flask/#email-server","title":"Email Server","text":"<p>OpenML uses its own mail server, You can use basically any mail server compatible with python SMTP library. Our suggestion is to use mailtrap.io for local testing. You can configure email server configurations in .env file. Currently we only use emails for confirmation email and forgotten password emails.</p>"},{"location":"contributing/website/React/","title":"React App","text":""},{"location":"contributing/website/React/#app-structure","title":"App structure","text":"<p>The structure of the source code looks as follows</p> <pre><code>App.js\nindex.js\ncomponents\n|-- Sidebar.js\n|-- Header.js\n|-- ...\nlayouts\n|-- Clear.js\n|-- Main.js\npages\n|-- auth\n|-- cover\n|-- docs\n|-- search\nroutes\n|-- index.js\n|-- Routes.js\nthemes\n</code></pre> <p>The website is designed as a single-page application. The top level files bootstrap the app. <code>index.js</code> simply renders the top component, and <code>App.js</code> adds the relevant subcomponents based on the current theme and state.</p> <p><code>Routes.js</code> links components to the possible routes (based on the URL). The list of possible routes is defined in <code>routes/index.js</code>.</p> <p><code>pages</code> contain the various pages of the website. It has subdirectories for:</p> <ul> <li><code>auth</code>: All pages that require authorization (login). These routes are protected.</li> <li><code>cover</code>: The front page of the website</li> <li><code>docs</code>: All normal information pages (e.g. 'About', 'API',...)</li> <li><code>search</code>: All pages related to searching for datasets, tasks, flows, runs, etc.</li> </ul> <p><code>layout</code> contains the possible layouts, <code>Main</code> or <code>Clear</code> (see below). You define the layout of a page by adding its route to either <code>mainRoutes</code> or <code>clearRoutes</code> in <code>routes/index.js</code>. The default is the <code>Main</code> layout.</p> <p><code>themes</code> contains the overall theme styling for the entire website. Currently, there is a dark and a light theme. They can be set using <code>setTheme</code> in the MainContext, see <code>App.js</code>.</p>"},{"location":"contributing/website/React/#component-structure","title":"Component structure","text":"<p>The component structure is shown above, for the <code>Main</code> layout. The <code>App</code> component also holds the state of the website using React's native Context API (see below). Next to the header and sidebar, the main component of the website (in yellow) shows the contents of the current <code>page</code>. In this image, this is the search page, which has several subcomponents as explained below.</p>"},{"location":"contributing/website/React/#search-page","title":"Search page","text":"<p>The search page is structured as follows:</p> <ul> <li> <p><code>SearchPanel</code>: the main search panel. Also contains callbacks for sorting and filtering, and lists what can be filtered or sorted on.</p> </li> <li> <p><code>FilterBar</code>: The top bar with the search statistics and functionality to add filters and sort results</p> </li> <li> <p><code>SearchResultsPanel</code>: The list of search results on the left. It shows a list of <code>Card</code> elements which are uniformly styled but their contents may vary. Depending on the selected type of result (selected in the left navigation bar) it is instantiated with different properties. E.g. a <code>DataListPanel</code> is a simple wrapper around <code>SearchResultsPanel</code> which defines the dataset-specific statistics to be shown in the cards.</p> <ul> <li>Search tabs: The tabs that allow you to choose between different aspects of the results (Statistics, Overview (Dash)) or the different views on the selected dataset, task, etc. (Details, Analysis (Dash),...)</li> <li><code>ItemDetail</code>: When a search result is selected, this will show the details of the selection, e.g. the dataset details. Depending on the passed <code>type</code> prop, it will render the <code>Dataset</code>, <code>Task</code>, ... component.</li> </ul> </li> </ul> <p>The <code>api.js</code> file contains the <code>search</code> function, which translates a search query, filters, and other constraints into an ElasticSearch query and returns the results.</p>"},{"location":"contributing/website/React/#style-guide","title":"Style guide","text":"<p>To keep a consistent style and minimize dependencies and complexity, we build on Material UI components and FontAwesome icons. Theming is defined in <code>themes/index.js</code> and loaded in as a context (<code>ThemeContext</code>) in <code>App.js</code>. More specific styling is always defined through styled components in the corresponding pages.</p>"},{"location":"contributing/website/React/#layouts","title":"Layouts","text":"<p>There are two top level layouts: <code>Main</code> loads the main layout with a <code>Sidebar</code>, <code>Header</code>, and a certain page with all the contents. The <code>Clear.js</code> layout has no headers or sidebars, but has a colored gradient background. It is used mainly for user login and registration or other quick forms.</p> <p>The layout of the page content should use the Material UI grid layout. This makes sure it will adapt to different device screen sizes. Test using your browsers development tools whether the layout adapts correctly to different screens, including recent smartphones.</p>"},{"location":"contributing/website/React/#styled-components","title":"Styled components","text":"<p>Any custom styling (beyond the Material UI default styling) is defined in styled components which are defined within the file for each page. Keep this as minimal as possible. Check if you can import styled components already defined for other pages, avoid duplication.</p> <p>Styled div's are defined as follows:</p> <pre><code>const OpenMLTitle = styled.div`\n  color: white;\n  font-size: 3em;\n`;\n</code></pre> <p>Material UI components can be styled the same way:</p> <pre><code>const WhiteButton = styled(Button)`\n  display: inline-block;\n  color: #fff;\n`;\n</code></pre>"},{"location":"contributing/website/React/#color-palette","title":"Color palette","text":"<p>We follow the general Material UI color palette with shade 400, except when that doesn't give sufficient contrast. The main colors used (e.g. for the icons in the sidebar are: 'green[400]', 'yellow[700]', 'blue[800]', 'red[400]', 'purple[400]', 'orange[400]', 'grey[400]'. Backgrounds are generally kept white (or dark grey for the dark theme). The global context (see below) has a <code>getColor</code> function to get the colors of the search types, e.g. <code>context.getColor(\"run\")</code> returns <code>red[400]</code>.</p>"},{"location":"contributing/website/React/#handling-state","title":"Handling state","text":"<p>There are different levels of state management:</p> <ul> <li>Global state is handled via React's native Context API (we don't use Redux). Contexts are defined in the component tree where needed (usually higher up) by a context provider component, and is accessed lower in the component tree by a context consumer. For instance, see the <code>ThemeContext.Provider</code> in <code>App.js</code> and the <code>ThemeContext.Consumer</code> in <code>Sidebar.js</code>. There is a <code>MainContext</code> which contains global state values such as the logged in user details, and the current state of the search.</li> <li>Lower level components can pass state to their child components via props.</li> <li>Local state changes should, when possible, be defined by React Hooks.</li> </ul> <p>Note that changing the global state will re-render the entire website. Hence, do this only when necessary.</p>"},{"location":"contributing/website/React/#state-and-search","title":"State and search","text":"<p>Most global state variables have to do with search. The search pages typically work by changing the <code>query</code> and <code>filters</code> variables (see <code>App.js</code>). There is a <code>setSearch</code> function in the main context that can be called to change the search parameters. It checks whether the query has changed and whether updating the global state and re-rendering the website is necessary.</p>"},{"location":"contributing/website/React/#lifecycle-methods","title":"Lifecycle Methods","text":"<p>These are the React lifecycle methods and how we use them. When a component mounts, methods 1,2,4,7 will be called. When it updates, methods 2-6 will be called.</p> <ol> <li>constructor(): Set the initial state of the components</li> <li>getDerivedStateFromProps(props, state): Static method, only for changing the local state based on props. It returns the new state.</li> <li>shouldComponentUpdate(nextProps, nextState): Decides whether a state change requires a re-rendering or not. Used to optimize performance.</li> <li>render(): Returns the JSX to be rendered. It should NOT change the state.</li> <li>getSnapshotBeforeUpdate(prevProps,prevState): Used to save 'old' DOM information right before an update. Returns a 'snapshot'.</li> <li>componentDidUpdate(prevProps,prevState,snapshot): For async requests or other operations right after component update.</li> <li>componentDidMount(): For async requests (e.g. API calls) right after the component mounted.</li> <li>componentWillUnMount(): Cleanup before the component is destroyed.</li> <li>componentDidCatch(error,info): For updating the state after an error is thrown.</li> </ol>"},{"location":"contributing/website/React/#forms-and-events","title":"Forms and Events","text":"<p>React wraps native browser events into synthetic events to handle interactions in a cross-browser compatible way. After being wrapped, they are sent to all event handlers, usually defined as callbacks. Note: for performance reasons, synthetic events are pooled and reused, so their properties are nullified after being consumed. If you want to use them asynchronously, you need to call <code>event.persist()</code>.</p> <p>HTML forms are different than other DOM elements because they keep their own state in plain HTML. To make sure that we can control the state we need to set the input field's <code>value</code> to a component state value.</p> <p>Here's an example of using an input field to change the title displayed in the component.</p> <pre><code>const titles: {mainTitle: 'OpenML'};\n\nclass App extends Component {\n  this.state = {titles};\n\n  // Receive synthetic event\n  onTitleChange = (event) =&gt; {\n    this.setState({titles.mainTitle : event.target.value});\n  }\n\n  render(){\n    return (\n      &lt;div classname=\"App\"&gt;\n        &lt;h1&gt;{this.state.titles.mainTitle}&lt;/h1&gt;\n        &lt;form&gt;\n          &lt;input type=\"text\"\n          value={this.state.titles.mainTitle} // control state\n          onChange={this.onTitleChange} // event handler callback\n          /&gt;\n        &lt;/form&gt;\n      &lt;/div&gt;\n    );\n  }\n}\n</code></pre>"},{"location":"contributing/website/Website/","title":"Getting started","text":""},{"location":"contributing/website/Website/#installation","title":"Installation","text":"<p>The OpenML website runs on Flask, React, and Dash. You need to install these first.</p> <ul> <li> <p>Download or clone the source code for the OpenML website from GitHub. Then, go into that folder (it should have the <code>requirements.txt</code> and <code>package.json</code> files). <pre><code>git clone https://github.com/openml/openml.org.git\ncd openml.org\n</code></pre></p> </li> <li> <p>Install Flask, Dash, and dependencies using PIP <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Install React and dependencies using NPM (8 or higher) <pre><code>cd server/src/client/app/\nnpm install\n</code></pre></p> </li> </ul>"},{"location":"contributing/website/Website/#building-and-running","title":"Building and running","text":"<p>Go back to the home directory. Build a production version of the website with:</p> <pre><code>npm run build --prefix server/src/client/app/\n</code></pre> <p>Start the server by running:</p> <pre><code>flask run\n</code></pre> <p>You should now see the app running in your browser at <code>localhost:5000</code></p> <p>Note: If you run the app using HTTPS, add the SSL context or use 'adhoc' to use on-the-fly certificates or you can specify your own certificates.</p> <pre><code>flask run --cert='adhoc'\n</code></pre> <p>As flask server is not suitable for production we recommend you to use some other server if you want to deploy your openml installation in production. We currently use gunicorn for production server. You can install the gunicorn server and run it: <pre><code>gunicorn --certfile cert.pem --keyfile key.pem -b localhost:5000 autoapp:app\n</code></pre></p>"},{"location":"contributing/website/Website/#development","title":"Development","text":"<p>To start the React frontend in developer mode, go to <code>server/src/client/app</code> and run:</p> <pre><code>npm run start\n</code></pre> <p>The app should automatically open at <code>localhost:3000</code> and any changes made to the code will automatically reload the website (hot loading).</p> <p>For the new Next.js frontend, install and run like this: <pre><code>cd app\nnpm install\nnpm run dev\n</code></pre></p>"},{"location":"contributing/website/Website/#structure","title":"Structure","text":"<p>The website is built on the following components:  </p> <ul> <li>A Flask backend. Written in Python, the backend takes care of all communication with the OpenML server. It builds on top of the OpenML Python API. It also takes care of user authentication and keeps the search engine (ElasticSearch) up to date with the latest information from the server. Files are located in the <code>server</code> folder.</li> <li>A React frontend. Written in JavaScript, this takes care of rendering the website. It pulls in information from the search engine, and shows plots rendered by Dash. It also contains forms (e.g. for logging in or uploading new datasets), which will be sent off to the backend for processing. Files are located in <code>server/src/client/app</code>.</li> <li>Dash dashboards. Written in Python, Dash is used for writing interactive plots. It pulls in data from the Python API, and renders the plots as React components. Files are located in <code>server/src/dashboard</code>.</li> </ul>"},{"location":"integrations/","title":"Integrations","text":"<ul> <li>Overview</li> </ul>"},{"location":"integrations/Java/","title":"Java","text":"<p>The Java API allows you connect to OpenML from Java applications.</p>"},{"location":"integrations/Java/#java-docs","title":"Java Docs","text":"<p>Read the full Java Docs.</p>"},{"location":"integrations/Java/#download","title":"Download","text":"<p>Stable releases of the Java API are available from Maven Central Or, you can check out the developer version from GitHub</p> <p>Include the jar file in your projects as usual, or install via Maven.</p>"},{"location":"integrations/Java/#quick-start","title":"Quick Start","text":"<ul> <li>Create an <code>OpenmlConnector</code> instance with your authentication details. This will create a client with all OpenML functionalities. <p>OpenmlConnector client = new OpenmlConnector(\"api_key\")</p> </li> </ul> <p>All functions are described in the Java Docs.</p>"},{"location":"integrations/Java/#downloading","title":"Downloading","text":"<p>To download data, flows, tasks, runs, etc. you need the unique id of that resource. The id is shown on each item's webpage and in the corresponding url. For instance, let's download Data set 1. The following returns a DataSetDescription object that contains all information about that data set.</p> <pre><code>DataSetDescription data = client.dataGet(1);\n</code></pre> <p>You can also search for the items you need online, and click the icon to get all id's that match a search.</p>"},{"location":"integrations/Java/#uploading","title":"Uploading","text":"<p>To upload data, flows, runs, etc. you need to provide a description of the object. We provide wrapper classes to provide this information, e.g. <code>DataSetDescription</code>, as well as to capture the server response, e.g. <code>UploadDataSet</code>, which always includes the generated id for reference:</p> <pre><code>DataSetDescription description = new DataSetDescription( \"iris\", \"The famous iris dataset\", \"arff\", \"class\");\nUploadDataSet result = client.dataUpload( description, datasetFile );\nint data_id = result.getId();\n</code></pre> <p>More details are given in the corresponding functions below. Also see the Java Docs for all possible inputs and return values.</p>"},{"location":"integrations/Java/#data-download","title":"Data download","text":""},{"location":"integrations/Java/#datagetint-data_id","title":"<code>dataGet(int data_id)</code>","text":"<p>Retrieves the description of a specified data set.</p> <pre><code>DataSetDescription data = client.dataGet(1);\nString name = data.getName();\nString version = data.getVersion();\nString description = data.getDescription();\nString url = data.getUrl();\n</code></pre>"},{"location":"integrations/Java/#datafeaturesint-data_id","title":"<code>dataFeatures(int data_id)</code>","text":"<p>Retrieves the description of the features of a specified data set.</p> <pre><code>DataFeature reponse = client.dataFeatures(1);\nDataFeature.Feature[] features = reponse.getFeatures();\nString name = features[0].getName();\nString type = features[0].getDataType();\nboolean isTarget = features[0].getIs_target();\n</code></pre>"},{"location":"integrations/Java/#dataqualityint-data_id","title":"<code>dataQuality(int data_id)</code>","text":"<p>Retrieves the description of the qualities (meta-features) of a specified data set.</p> <pre><code>    DataQuality response = client.dataQuality(1);\n    DataQuality.Quality[] qualities = reponse.getQualities();\n    String name = qualities[0].getName();\n    String value = qualities[0].getValue();\n</code></pre>"},{"location":"integrations/Java/#dataqualityint-data_id-int-start-int-end-int-interval_size","title":"<code>dataQuality(int data_id, int start, int end, int interval_size)</code>","text":"<p>For data streams. Retrieves the description of the qualities (meta-features) of a specified portion of a data stream.</p> <pre><code>    DataQuality qualities = client.dataQuality(1,0,10000,null);\n</code></pre>"},{"location":"integrations/Java/#dataqualitylist","title":"<code>dataQualityList()</code>","text":"<p>Retrieves a list of all data qualities known to OpenML.</p> <pre><code>    DataQualityList response = client.dataQualityList();\n    String[] qualities = response.getQualities();\n</code></pre>"},{"location":"integrations/Java/#data-upload","title":"Data upload","text":""},{"location":"integrations/Java/#datauploaddatasetdescription-description-file-dataset","title":"<code>dataUpload(DataSetDescription description, File dataset)</code>","text":"<p>Uploads a data set file to OpenML given a description. Throws an exception if the upload failed, see openml.data.upload for error codes.</p> <pre><code>    DataSetDescription dataset = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\");\n    UploadDataSet data = client.dataUpload( dataset, new File(\"data/path\"));\n    int data_id = result.getId();\n</code></pre>"},{"location":"integrations/Java/#datauploaddatasetdescription-description","title":"<code>dataUpload(DataSetDescription description)</code>","text":"<p>Registers an existing dataset (hosted elsewhere). The description needs to include the url of the data set. Throws an exception if the upload failed, see openml.data.upload for error codes.</p> <pre><code>    DataSetDescription description = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\");\n    description.setUrl(\"http://datarepository.org/mydataset\");\n    UploadDataSet data = client.dataUpload( description );\n    int data_id = result.getId();\n</code></pre>"},{"location":"integrations/Java/#flow-download","title":"Flow download","text":""},{"location":"integrations/Java/#flowgetint-flow_id","title":"<code>flowGet(int flow_id)</code>","text":"<p>Retrieves the description of the flow/implementation with the given id.</p> <pre><code>    Implementation flow = client.flowGet(100);\n    String name = flow.getName();\n    String version = flow.getVersion();\n    String description = flow.getDescription();\n    String binary_url = flow.getBinary_url();\n    String source_url = flow.getSource_url();\n    Parameter[] parameters = flow.getParameter();\n</code></pre>"},{"location":"integrations/Java/#flow-management","title":"Flow management","text":""},{"location":"integrations/Java/#flowowned","title":"<code>flowOwned()</code>","text":"<p>Retrieves an array of id's of all flows/implementations owned by you.</p> <pre><code>    ImplementationOwned response = client.flowOwned();\n    Integer[] ids = response.getIds();\n</code></pre>"},{"location":"integrations/Java/#flowexistsstring-name-string-version","title":"<code>flowExists(String name, String version)</code>","text":"<p>Checks whether an implementation with the given name and version is already registered on OpenML.</p> <pre><code>    ImplementationExists check = client.flowExists(\"weka.j48\", \"3.7.12\");\n    boolean exists = check.exists();\n    int flow_id = check.getId();\n</code></pre>"},{"location":"integrations/Java/#flowdeleteint-id","title":"<code>flowDelete(int id)</code>","text":"<p>Removes the flow with the given id (if you are its owner).</p> <pre><code>    ImplementationDelete response = client.openmlImplementationDelete(100);\n</code></pre>"},{"location":"integrations/Java/#flow-upload","title":"Flow upload","text":""},{"location":"integrations/Java/#flowuploadimplementation-description-file-binary-file-source","title":"<code>flowUpload(Implementation description, File binary, File source)</code>","text":"<p>Uploads implementation files (binary and/or source) to OpenML given a description.</p> <pre><code>    Implementation flow = new Implementation(\"weka.J48\", \"3.7.12\", \"description\", \"Java\", \"WEKA 3.7.12\")\n    UploadImplementation response = client.flowUpload( flow, new File(\"code.jar\"), new File(\"source.zip\"));\n    int flow_id = response.getId();\n</code></pre>"},{"location":"integrations/Java/#task-download","title":"Task download","text":""},{"location":"integrations/Java/#taskgetint-task_id","title":"<code>taskGet(int task_id)</code>","text":"<p>Retrieves the description of the task with the given id.</p> <pre><code>    Task task = client.taskGet(1);\n    String task_type = task.getTask_type();\n    Input[] inputs = task.getInputs();\n    Output[] outputs = task.getOutputs();\n</code></pre>"},{"location":"integrations/Java/#taskevaluationsint-task_id","title":"<code>taskEvaluations(int task_id)</code>","text":"<p>Retrieves all evaluations for the task with the given id.</p> <pre><code>    TaskEvaluations response = client.taskEvaluations(1);\n    Evaluation[] evaluations = response.getEvaluation();\n</code></pre>"},{"location":"integrations/Java/#taskevaluationsint-task_id-int-start-int-end-int-interval_size","title":"<code>taskEvaluations(int task_id, int start, int end, int interval_size)</code>","text":"<p>For data streams. Retrieves all evaluations for the task over the specified window of the stream.</p> <pre><code>    TaskEvaluations response = client.taskEvaluations(1);\n    Evaluation[] evaluations = response.getEvaluation();\n</code></pre>"},{"location":"integrations/Java/#run-download","title":"Run download","text":""},{"location":"integrations/Java/#rungetint-run_id","title":"<code>runGet(int run_id)</code>","text":"<p>Retrieves the description of the run with the given id.</p> <pre><code>    Run run = client.runGet(1);\n    int task_id = run.getTask_id();\n    int flow_id = run.getImplementation_id();\n    Parameter_setting[] settings = run.getParameter_settings()\n    EvaluationScore[] scores = run.getOutputEvaluation();\n</code></pre>"},{"location":"integrations/Java/#run-management","title":"Run management","text":""},{"location":"integrations/Java/#rundeleteint-run_id","title":"<code>runDelete(int run_id)</code>","text":"<p>Deletes the run with the given id (if you are its owner).</p> <pre><code>    RunDelete response = client.runDelete(1);\n</code></pre>"},{"location":"integrations/Java/#run-upload","title":"Run upload","text":""},{"location":"integrations/Java/#runuploadrun-description-mapstringfile-output_files","title":"<code>runUpload(Run description, Map&lt;String,File&gt; output_files)</code>","text":"<p>Uploads a run to OpenML, including a description and a set of output files depending on the task type.</p> <pre><code>    Run.Parameter_setting[] parameter_settings = new Run.Parameter_setting[1];\n    parameter_settings[0] = Run.Parameter_setting(null, \"M\", \"2\");\n    Run run = new Run(\"1\", null, \"100\", \"setup_string\", parameter_settings);\n    Map outputs = new HashMap&lt;String,File&gt;();\n    outputs.add(\"predictions\",new File(\"predictions.arff\"));\n    UploadRun response = client.runUpload( run, outputs);\n    int run_id = response.getRun_id();\n</code></pre>"},{"location":"integrations/Julia/","title":"OpenML.jl (Julia) Documentation","text":"<p>This is the reference documentation of <code>OpenML.jl</code>.</p> <p>The OpenML platform provides an integration platform for carrying out and comparing machine learning solutions across a broad collection of public datasets and software platforms.</p> <p>Summary of OpenML.jl functionality:</p> <ul> <li> <p><code>OpenML.list_tags</code><code>()</code>: for listing all dataset tags</p> </li> <li> <p><code>OpenML.list_datasets</code><code>(; tag=nothing, filter=nothing, output_format=...)</code>: for listing available datasets</p> </li> <li> <p><code>OpenML.describe_dataset</code><code>(id)</code>: to describe a particular dataset</p> </li> <li> <p><code>OpenML.load</code><code>(id; parser=:arff)</code>: to download a dataset</p> </li> </ul>"},{"location":"integrations/Julia/#installation","title":"Installation","text":"<pre><code>using Pkg\nPkg.add(\"OpenML\")\n</code></pre> <p>If running the demonstration below:</p> <pre><code>Pkg.add(\"DataFrames\") \nPkg.add(\"ScientificTypes\")\n</code></pre>"},{"location":"integrations/Julia/#sample-usage","title":"Sample usage","text":"<pre><code>using OpenML # or using MLJ\nusing DataFrames\n\nOpenML.list_tags()\n</code></pre> <p>Listing all datasets with the \"OpenML100\" tag which also have <code>n</code> instances and <code>p</code> features, where <code>100 &lt; n &lt; 1000</code> and <code>1 &lt; p &lt; 10</code>:</p> <pre><code>ds = OpenML.list_datasets(\n          tag = \"OpenML100\",\n          filter = \"number_instances/100..1000/number_features/1..10\",\n          output_format = DataFrame)\n</code></pre> <p>Describing and loading one of these datasets:</p> <pre><code>OpenML.describe_dataset(15)\ntable = OpenML.load(15)\n</code></pre> <p>Converting to a data frame:</p> <pre><code>df = DataFrame(table)\n</code></pre> <p>Inspecting it's schema:</p> <pre><code>using ScientificTypes\nschema(table)\n</code></pre>"},{"location":"integrations/Julia/#public-api","title":"Public API","text":"<p><code>@docs OpenML.list_tags OpenML.list_datasets OpenML.describe_dataset OpenML.load</code></p>"},{"location":"integrations/MOA/","title":"MOA","text":"<p>OpenML features extensive support for MOA. However currently this is implemented as a stand alone MOA compilation, using the latest version (as of May, 2014).</p> <p>Download MOA for OpenML</p>"},{"location":"integrations/MOA/#quick-start","title":"Quick Start","text":"<ul> <li>Download the standalone MOA environment above.</li> <li>Find your API key in your profile (log in first). Create a config file called <code>openml.conf</code> in a <code>.openml</code> directory in your home dir. It should contain the following lines: <p>api_key = YOUR_KEY</p> </li> <li>Launch the JAR file by double clicking on it, or launch from command-line using the following command: <p>java -cp openmlmoa.beta.jar moa.gui.GUI</p> </li> <li>Select the task <code>moa.tasks.openml.OpenmlDataStreamClassification</code> to evaluate a classifier on an OpenML task, and send the results to OpenML.</li> <li>Optionally, you can generate new streams using the Bayesian Network Generator: select the <code>moa.tasks.WriteStreamToArff</code> task, with <code>moa.streams.generators.BayesianNetworkGenerator</code>.</li> </ul>"},{"location":"integrations/Rest/","title":"REST API","text":"<p>OpenML offers a RESTful Web API, with predictive URLs, for uploading and downloading machine learning resources. Try the API Documentation to see examples of all calls, and test them right in your browser.</p>"},{"location":"integrations/Rest/#getting-started","title":"Getting started","text":"<ul> <li>REST services can be called using simple HTTP GET or POST actions.</li> <li>The REST Endpoint URL is <code>https://www.openml.org/api/v1/</code></li> <li>The default endpoint returns data in XML. If you prefer JSON, use the endpoint <code>https://www.openml.org/api/v1/json/</code>. Note that, to upload content, you still need to use XML (at least for now).</li> </ul>"},{"location":"integrations/Rest/#testing","title":"Testing","text":"<p>For continuous integration and testing purposes, we have a test server offering the same API, but which does not affect the production server.</p> <ul> <li>The test server REST Endpoint URL is <code>https://test.openml.org/api/v1/</code></li> </ul>"},{"location":"integrations/Rest/#error-messages","title":"Error messages","text":"<p>Error messages will look like this:</p> <pre><code>&lt;oml:error xmlns:oml=\"http://openml.org/error\"&gt;\n&lt;oml:code&gt;100&lt;/oml:code&gt;\n&lt;oml:message&gt;Please invoke legal function&lt;/oml:message&gt;\n&lt;oml:additional_information&gt;Additional information, not always available.&lt;/oml:additional_information&gt;\n&lt;/oml:error&gt;\n</code></pre> <p>All error messages are listed in the API documentation. E.g. try to get a non-existing dataset:</p> <ul> <li>in XML: https://www.openml.org/api_new/v1/data/99999</li> <li>in JSON: https://www.openml.org/api_new/v1/json/data/99999</li> </ul>"},{"location":"integrations/Rest/#examples","title":"Examples","text":"<p>You need to be logged in for these examples to work.</p>"},{"location":"integrations/Rest/#download-a-dataset","title":"Download a dataset","text":"<ul> <li>User asks for a dataset using the /data/{id} service. The <code>dataset id</code> is typically part of a task, or can be found on OpenML.org.</li> <li>OpenML returns a description of the dataset as an XML file (or JSON). Try it now</li> <li>The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.</li> <li>The dataset is returned by the server hosting the dataset. This can be OpenML, but also any other data repository. Try it now</li> </ul>"},{"location":"integrations/Rest/#download-a-flow","title":"Download a flow","text":"<ul> <li>User asks for a flow using the /flow/{id} service and a <code>flow id</code>. The <code>flow id</code> can be found on OpenML.org.</li> <li>OpenML returns a description of the flow as an XML file (or JSON). Try it now</li> <li>The flow description contains the URL where the flow can be downloaded (e.g. GitHub), either as source, binary or both, as well as additional information on history, dependencies and licence. The user calls the right URL to download it.</li> <li>The flow is returned by the server hosting it. This can be OpenML, but also any other code repository. Try it now</li> </ul>"},{"location":"integrations/Rest/#download-a-task","title":"Download a task","text":"<ul> <li>User asks for a task using the /task/{id} service and a <code>task id</code>. The <code>task id</code> is typically returned when searching for tasks.</li> <li>OpenML returns a description of the task as an XML file (or JSON). Try it now</li> <li>The task description contains the <code>dataset id</code>(s) of the datasets involved in this task. The user asks for the dataset using the /data/{id} service and the <code>dataset id</code>.</li> <li>OpenML returns a description of the dataset as an XML file (or JSON). Try it now</li> <li>The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.</li> <li>The dataset is returned by the server hosting it. This can be OpenML, but also any other data repository. Try it now</li> <li>The task description may also contain links to other resources, such as the train-test splits to be used in cross-validation. The user calls that URL to download the train-test splits.</li> <li>The train-test splits are returned by OpenML. Try it now</li> </ul>"},{"location":"integrations/Weka/","title":"Weka","text":"<p>OpenML is integrated in the Weka (Waikato Environment for Knowledge Analysis) Experimenter and the Command Line Interface.</p>"},{"location":"integrations/Weka/#installation","title":"Installation","text":"<p>OpenML is available as a weka extension in the package manager:</p> <ul> <li>Download the latest version (3.7.13 or higher).</li> <li>Launch Weka, or start from commandline: <p>java -jar weka.jar</p> </li> <li>If you need more memory (e.g. 1GB), start as follows: <p>java -Xmx1G -jar weka.jar</p> </li> <li>Open the package manager (Under 'Tools')</li> <li>Select package OpenmlWeka and click install. Afterwards, restart WEKA.</li> <li>From the Tools menu, open the 'OpenML Experimenter'.</li> </ul>"},{"location":"integrations/Weka/#graphical-interface","title":"Graphical Interface","text":"<p>You can solve OpenML Tasks in the Weka Experimenter, and automatically upload your experiments to OpenML (or store them locally).  </p> <ul> <li>From the Tools menu, open the 'OpenML Experimenter'.</li> <li>Enter your API key in the top field (log in first). You can also store this in a config file (see below).</li> <li>In the 'Tasks' panel, click the 'Add New' button to add new tasks. Insert the task id's as comma-separated values (e.g., '1,2,3,4,5'). Use the search function on OpenML to find interesting tasks and click the ID icon to list the ID's. In the future this search will also be integrated in WEKA.</li> <li>Add algorithms in the \"Algorithm\" panel.</li> <li>Go to the \"Run\" tab, and click on the \"Start\" button.</li> <li>The experiment will be executed and sent to OpenML.org.</li> <li>The runs will now appear on OpenML.org. You can follow their progress and check for errors on your profile page under 'Runs'.</li> </ul>"},{"location":"integrations/Weka/#commandline-interface","title":"CommandLine Interface","text":"<p>The Command Line interface is useful for running experiments automatically on a server, without using a GUI.</p> <ul> <li>Create a config file called <code>openml.conf</code> in a new directory called <code>.openml</code> in your home dir. It should contain the following line: <p>api_key = YOUR_KEY</p> </li> <li>Execute the following command: <p>java -cp weka.jar openml.experiment.TaskBasedExperiment -T  -C  --  <li>For example, the following command will run Weka's J48 algorithm on Task 1: <p>java -cp OpenWeka.beta.jar openml.experiment.TaskBasedExperiment -T 1 -C weka.classifiers.trees.J48</p> </li> <li>The following suffix will set some parameters of this classifier: <p>-- -C 0.25 -M 2</p> </li>"},{"location":"integrations/Weka/#api-reference","title":"API reference","text":"<p>Check the Weka integration Java Docs for more details about the possibilities.</p>"},{"location":"integrations/Weka/#issues","title":"Issues","text":"<p>Please report any bugs that you may encounter in the issue tracker: https://github.com/openml/openml-weka Or email to j.n.van.rijn@liacs.leidenuniv.nl</p>"},{"location":"integrations/apikey/","title":"Authentication","text":"<p>The OpenML server can only be accessed by users who have signed up on the OpenML platform. If you don\u2019t have an account yet, sign up now. You will receive an API key, which will authenticate you to the server and allow you to download and upload datasets, tasks, runs and flows.</p> <ul> <li>Create an OpenML account (free) on https://www.openml.org.</li> <li>After logging in, open your account page (avatar on the top right)</li> <li>Open 'Account Settings', then 'API authentication' to find your API key.</li> </ul> <p>There are two ways to permanently authenticate:</p> <ul> <li>Use the <code>openml</code> CLI tool with <code>openml configure apikey MYKEY</code>,   replacing MYKEY with your API key.</li> <li>Create a plain text file ~/.openml/config with the line   'apikey=MYKEY', replacing MYKEY with your API key. The config   file must be in the directory ~/.openml/config and exist prior to   importing the openml module.</li> </ul>"},{"location":"integrations/creating_extensions/","title":"Creating an Extension","text":"<p>OpenML-Python provides an extension interface to connect other machine learning libraries than scikit-learn to OpenML. Please check the <code>api_extensions</code> and use the scikit-learn extension in <code>openml.extensions.sklearn.SklearnExtension</code>{.interpreted-text role=\"class\"} as a starting point.</p>"},{"location":"integrations/creating_extensions/#connecting-new-machine-learning-libraries","title":"Connecting new machine learning libraries","text":""},{"location":"integrations/creating_extensions/#content-of-the-library","title":"Content of the Library","text":"<p>To leverage support from the community and to tap in the potential of OpenML, interfacing with popular machine learning libraries is essential. The OpenML-Python package is capable of downloading meta-data and results (data, flows, runs), regardless of the library that was used to upload it. However, in order to simplify the process of uploading flows and runs from a specific library, an additional interface can be built. The OpenML-Python team does not have the capacity to develop and maintain such interfaces on its own. For this reason, we have built an extension interface to allows others to contribute back. Building a suitable extension for therefore requires an understanding of the current OpenML-Python support.</p> <p>The <code>sphx_glr_examples_20_basic_simple_flows_and_runs_tutorial.py</code>{.interpreted-text role=\"ref\"} tutorial shows how scikit-learn currently works with OpenML-Python as an extension. The sklearn extension packaged with the openml-python repository can be used as a template/benchmark to build the new extension.</p>"},{"location":"integrations/creating_extensions/#api","title":"API","text":"<ul> <li>The extension scripts must import the [openml]{.title-ref} package     and be able to interface with any function from the OpenML-Python     <code>api</code>.</li> <li>The extension has to be defined as a Python class and must inherit     from <code>openml.extensions.Extension</code>.</li> <li>This class needs to have all the functions from [class     Extension]{.title-ref} overloaded as required.</li> <li>The redefined functions should have adequate and appropriate     docstrings. The [Sklearn Extension API     :class:`openml.extensions.sklearn.SklearnExtension.html]{.title-ref}     is a good example to follow.</li> </ul>"},{"location":"integrations/creating_extensions/#interfacing-with-openml-python","title":"Interfacing with OpenML-Python","text":"<p>Once the new extension class has been defined, the openml-python module to <code>openml.extensions.register_extension</code> must be called to allow OpenML-Python to interface the new extension.</p> <p>The following methods should get implemented. Although the documentation in the [Extension]{.title-ref} interface should always be leading, here we list some additional information and best practices. The [Sklearn Extension API :class:`openml.extensions.sklearn.SklearnExtension.html]{.title-ref} is a good example to follow. Note that most methods are relatively simple and can be implemented in several lines of code.</p> <ul> <li>General setup (required)<ul> <li><code>can_handle_flow</code>: Takes as     argument an OpenML flow, and checks whether this can be handled     by the current extension. The OpenML database consists of many     flows, from various workbenches (e.g., scikit-learn, Weka, mlr).     This method is called before a model is being deserialized.     Typically, the flow-dependency field is used to check whether     the specific library is present, and no unknown libraries are     present there.</li> <li><code>can_handle_model</code>: Similar as     <code>can_handle_flow</code>, except that in     this case a Python object is given. As such, in many cases, this     method can be implemented by checking whether this adheres to a     certain base class.</li> </ul> </li> <li>Serialization and De-serialization (required)<ul> <li><code>flow_to_model</code>: deserializes the     OpenML Flow into a model (if the library can indeed handle the     flow). This method has an important interplay with     <code>model_to_flow</code>. Running these     two methods in succession should result in exactly the same     model (or flow). This property can be used for unit testing     (e.g., build a model with hyperparameters, make predictions on a     task, serialize it to a flow, deserialize it back, make it     predict on the same task, and check whether the predictions are     exactly the same.) The example in the scikit-learn interface     might seem daunting, but note that here some complicated design     choices were made, that allow for all sorts of interesting     research questions. It is probably good practice to start easy.</li> <li><code>model_to_flow</code>: The inverse of     <code>flow_to_model</code>. Serializes a     model into an OpenML Flow. The flow should preserve the class,     the library version, and the tunable hyperparameters.</li> <li><code>get_version_information</code>: Return     a tuple with the version information of the important libraries.</li> <li><code>create_setup_string</code>: No longer     used, and will be deprecated soon.</li> </ul> </li> <li>Performing runs (required)<ul> <li><code>is_estimator</code>: Gets as input a     class, and checks whether it has the status of estimator in the     library (typically, whether it has a train method and a predict     method).</li> <li><code>seed_model</code>: Sets a random seed     to the model.</li> <li><code>_run_model_on_fold</code>: One of the     main requirements for a library to generate run objects for the     OpenML server. Obtains a train split (with labels) and a test     split (without labels) and the goal is to train a model on the     train split and return the predictions on the test split. On top     of the actual predictions, also the class probabilities should     be determined. For classifiers that do not return class     probabilities, this can just be the hot-encoded predicted label.     The predictions will be evaluated on the OpenML server. Also,     additional information can be returned, for example,     user-defined measures (such as runtime information, as this can     not be inferred on the server). Additionally, information about     a hyperparameter optimization trace can be provided.</li> <li><code>obtain_parameter_values</code>:     Obtains the hyperparameters of a given model and the current     values. Please note that in the case of a hyperparameter     optimization procedure (e.g., random search), you only should     return the hyperparameters of this procedure (e.g., the     hyperparameter grid, budget, etc) and that the chosen model will     be inferred from the optimization trace.</li> <li><code>check_if_model_fitted</code>: Check     whether the train method of the model has been called (and as     such, whether the predict method can be used).</li> </ul> </li> <li>Hyperparameter optimization (optional)<ul> <li><code>instantiate_model_from_hpo_class</code>{.interpreted-text     role=\"meth\"}: If a given run has recorded the hyperparameter     optimization trace, then this method can be used to     reinstantiate the model with hyperparameters of a given     hyperparameter optimization iteration. Has some similarities     with <code>flow_to_model</code> (as this     method also sets the hyperparameters of a model). Note that     although this method is required, it is not necessary to     implement any logic if hyperparameter optimization is not     implemented. Simply raise a [NotImplementedError]{.title-ref}     then.</li> </ul> </li> </ul>"},{"location":"integrations/creating_extensions/#hosting-the-library","title":"Hosting the library","text":"<p>Each extension created should be a stand-alone repository, compatible with the OpenML-Python repository. The extension repository should work off-the-shelf with OpenML-Python installed.</p> <p>Create a public Github repo with the following directory structure:</p> <pre><code>| [repo name]\n|    |-- [extension name]\n|    |    |-- __init__.py\n|    |    |-- extension.py\n|    |    |-- config.py (optionally)\n</code></pre>"},{"location":"integrations/creating_extensions/#recommended","title":"Recommended","text":"<ul> <li>Test cases to keep the extension up to date with the     [openml-python]{.title-ref} upstream changes.</li> <li>Documentation of the extension API, especially if any new     functionality added to OpenML-Python\\'s extension design.</li> <li>Examples to show how the new extension interfaces and works with     OpenML-Python.</li> <li>Create a PR to add the new extension to the OpenML-Python API     documentation.</li> </ul> <p>Happy contributing!</p>"},{"location":"integrations/getting_started/","title":"Getting Started","text":"In\u00a0[2]: Copied! <pre>from IPython.display import display, HTML, Markdown\nimport os\nimport yaml\nwith open(\"../../mkdocs.yml\", \"r\") as f:\n    load_config = yaml.safe_load(f)\nrepo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\")\nbinder_url = load_config[\"binder_url\"]\nrelative_file_path = \"integrations/getting_started.ipynb\"\ndisplay(HTML(f\"\"\"&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/{repo_url}/{relative_file_path}\"&gt;\n  &lt;img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/&gt;\n&lt;/a&gt;\"\"\"))\ndisplay(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/openml/docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\"))\n</pre> from IPython.display import display, HTML, Markdown import os import yaml with open(\"../../mkdocs.yml\", \"r\") as f:     load_config = yaml.safe_load(f) repo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\") binder_url = load_config[\"binder_url\"] relative_file_path = \"integrations/getting_started.ipynb\" display(HTML(f\"\"\" \"\"\")) display(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/openml/docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\")) In\u00a0[\u00a0]: Copied! <pre>!pip install openml\n</pre> !pip install openml In\u00a0[\u00a0]: Copied! <pre># License: BSD 3-Clause\n\nimport openml\nfrom sklearn import neighbors\n</pre> # License: BSD 3-Clause  import openml from sklearn import neighbors Warning<p>.. include:: ../../test_server_usage_warning.txt</p> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <p>When using the main server instead, make sure your apikey is configured. This can be done with the following line of code (uncomment it!). Never share your apikey with others.</p> In\u00a0[\u00a0]: Copied! <pre># openml.config.apikey = 'YOURKEY'\n</pre> # openml.config.apikey = 'YOURKEY' In\u00a0[\u00a0]: Copied! <pre># Uncomment and set your OpenML cache directory\n# import os\n# openml.config.cache_directory = os.path.expanduser('YOURDIR')\n</pre> # Uncomment and set your OpenML cache directory # import os # openml.config.cache_directory = os.path.expanduser('YOURDIR') In\u00a0[\u00a0]: Copied! <pre>task = openml.tasks.get_task(403)\ndata = openml.datasets.get_dataset(task.dataset_id)\nclf = neighbors.KNeighborsClassifier(n_neighbors=5)\nrun = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False)\n# Publish the experiment on OpenML (optional, requires an API key).\n# For this tutorial, our configuration publishes to the test server\n# as to not crowd the main server with runs created by examples.\nmyrun = run.publish()\nprint(f\"kNN on {data.name}: {myrun.openml_url}\")\n</pre> task = openml.tasks.get_task(403) data = openml.datasets.get_dataset(task.dataset_id) clf = neighbors.KNeighborsClassifier(n_neighbors=5) run = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False) # Publish the experiment on OpenML (optional, requires an API key). # For this tutorial, our configuration publishes to the test server # as to not crowd the main server with runs created by examples. myrun = run.publish() print(f\"kNN on {data.name}: {myrun.openml_url}\") In\u00a0[\u00a0]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n</pre> openml.config.stop_using_configuration_for_example()"},{"location":"integrations/getting_started/#getting-started","title":"Getting Started\u00b6","text":"<p>This page will guide you through the process of getting started with OpenML. While this page is a good starting point, for more detailed information, please refer to the integrations section and the rest of the documentation.</p>"},{"location":"integrations/getting_started/#authentication","title":"Authentication\u00b6","text":"<ul> <li>If you are using the OpenML API to download datasets, upload results, or create tasks, you will need to authenticate. You can do this by creating an account on the OpenML website and using your API key. - You can find detailed instructions on how to authenticate in the authentication section</li> </ul>"},{"location":"integrations/getting_started/#eeg-eye-state-example","title":"EEG Eye State example\u00b6","text":"<p>Download the OpenML task for the eeg-eye-state.</p>"},{"location":"integrations/getting_started/#caching","title":"Caching\u00b6","text":"<p>When downloading datasets, tasks, runs and flows, they will be cached to retrieve them without calling the server later. As with the API key, the cache directory can be either specified through the config file or through the API:</p> <ul> <li>Add the  line cachedir = 'MYDIR' to the config file, replacing 'MYDIR' with the path to the cache directory. By default, OpenML will use ~/.openml/cache as the cache directory.</li> <li>Run the code below, replacing 'YOURDIR' with the path to the cache directory.</li> </ul>"},{"location":"integrations/mlr/","title":"Machine Learning in R (mlr)","text":"<p>OpenML is readily integrated with mlr through the R API.</p> <p>Example</p> <pre><code>library(OpenML)\nlibrary(mlr)\n\ntask = getOMLTask(10)\nlrn = makeLearner(\"classif.rpart\")\nrun = runTaskMlr(task, lrn)\nrun.id = uploadOMLRun(run)\n</code></pre> <p>Key features:  </p> <ul> <li>Query and download OpenML datasets and use them however you like  </li> <li>Build any mlr learner, run it on any task and save the experiment as run objects  </li> <li>Upload your runs for collaboration or publishing  </li> <li>Query, download and reuse all shared runs  </li> </ul> <p>For many more details and examples, see the R tutorial.</p>"},{"location":"integrations/Pytorch/basic_tutorial/","title":"PyTorch sequential classification model example","text":"In\u00a0[1]: Copied! <pre>from IPython.display import display, HTML, Markdown\nimport os\nimport yaml\nwith open(\"../../../mkdocs.yml\", \"r\") as f:\n    load_config = yaml.safe_load(f)\nrepo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\")\nbinder_url = load_config[\"binder_url\"]\nrelative_file_path = \"integrations/Pytorch/basic_tutorial.ipynb\"\ndisplay(HTML(f\"\"\"&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/{repo_url}/{relative_file_path}\"&gt;\n  &lt;img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/&gt;\n&lt;/a&gt;\"\"\"))\ndisplay(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\"))\n</pre> from IPython.display import display, HTML, Markdown import os import yaml with open(\"../../../mkdocs.yml\", \"r\") as f:     load_config = yaml.safe_load(f) repo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\") binder_url = load_config[\"binder_url\"] relative_file_path = \"integrations/Pytorch/basic_tutorial.ipynb\" display(HTML(f\"\"\" \"\"\")) display(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\")) In\u00a0[1]: Copied! <pre>!pip install openml-pytorch\n</pre> !pip install openml-pytorch <pre>Requirement already satisfied: openml-pytorch in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/openml_pytorch-0.0.5-py3.9.egg (0.0.5)\nRequirement already satisfied: openml in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-pytorch) (0.13.1)\nRequirement already satisfied: torch&lt;2.2.0,&gt;=1.4.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-pytorch) (2.1.2)\nRequirement already satisfied: onnx in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-pytorch) (1.16.0)\nRequirement already satisfied: torchvision in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-pytorch) (0.16.2)\nRequirement already satisfied: typing-extensions in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from torch&lt;2.2.0,&gt;=1.4.0-&gt;openml-pytorch) (4.11.0)\nRequirement already satisfied: sympy in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from torch&lt;2.2.0,&gt;=1.4.0-&gt;openml-pytorch) (1.12)\nRequirement already satisfied: filelock in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from torch&lt;2.2.0,&gt;=1.4.0-&gt;openml-pytorch) (3.12.0)\nRequirement already satisfied: jinja2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from torch&lt;2.2.0,&gt;=1.4.0-&gt;openml-pytorch) (3.1.3)\nRequirement already satisfied: networkx in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from torch&lt;2.2.0,&gt;=1.4.0-&gt;openml-pytorch) (3.2.1)\nRequirement already satisfied: fsspec in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from torch&lt;2.2.0,&gt;=1.4.0-&gt;openml-pytorch) (2023.6.0)\nRequirement already satisfied: protobuf&gt;=3.20.2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from onnx-&gt;openml-pytorch) (5.26.1)\nRequirement already satisfied: numpy&gt;=1.20 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from onnx-&gt;openml-pytorch) (1.24.2)\nRequirement already satisfied: liac-arff&gt;=2.4.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-&gt;openml-pytorch) (2.5.0)\nRequirement already satisfied: xmltodict in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-&gt;openml-pytorch) (0.13.0)\nRequirement already satisfied: requests in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-&gt;openml-pytorch) (2.28.2)\nRequirement already satisfied: scikit-learn&gt;=0.18 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-&gt;openml-pytorch) (1.2.2)\nRequirement already satisfied: python-dateutil in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-&gt;openml-pytorch) (2.8.2)\nRequirement already satisfied: pandas&gt;=1.0.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-&gt;openml-pytorch) (1.5.3)\nRequirement already satisfied: scipy&gt;=0.13.3 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-&gt;openml-pytorch) (1.10.1)\nRequirement already satisfied: minio in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-&gt;openml-pytorch) (7.1.13)\nRequirement already satisfied: pyarrow in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml-&gt;openml-pytorch) (11.0.0)\nRequirement already satisfied: pillow!=8.3.*,&gt;=5.3.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from torchvision-&gt;openml-pytorch) (10.3.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pandas&gt;=1.0.0-&gt;openml-&gt;openml-pytorch) (2022.7.1)\nRequirement already satisfied: six&gt;=1.5 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from python-dateutil-&gt;openml-&gt;openml-pytorch) (1.16.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from scikit-learn&gt;=0.18-&gt;openml-&gt;openml-pytorch) (3.1.0)\nRequirement already satisfied: joblib&gt;=1.1.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from scikit-learn&gt;=0.18-&gt;openml-&gt;openml-pytorch) (1.2.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from jinja2-&gt;torch&lt;2.2.0,&gt;=1.4.0-&gt;openml-pytorch) (2.1.5)\nRequirement already satisfied: urllib3 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml-&gt;openml-pytorch) (1.26.15)\nRequirement already satisfied: certifi in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml-&gt;openml-pytorch) (2022.12.7)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests-&gt;openml-&gt;openml-pytorch) (3.4)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests-&gt;openml-&gt;openml-pytorch) (3.1.0)\nRequirement already satisfied: mpmath&gt;=0.19 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from sympy-&gt;torch&lt;2.2.0,&gt;=1.4.0-&gt;openml-pytorch) (1.3.0)\n\n[notice] A new release of pip is available: 23.0.1 -&gt; 24.0\n[notice] To update, run: pip install --upgrade pip\n</pre> In\u00a0[2]: Copied! <pre>import torch.optim\n\nimport openml\nimport openml_pytorch\nimport openml_pytorch.layers\nimport openml_pytorch.config\nimport logging\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom openml import OpenMLTask\nimport torchvision.models as models\n</pre> import torch.optim  import openml import openml_pytorch import openml_pytorch.layers import openml_pytorch.config import logging import torch.nn as nn import torch.nn.functional as F  from openml import OpenMLTask import torchvision.models as models In\u00a0[3]: Copied! <pre># Enable logging in order to observe the progress while running the example.\nopenml.config.logger.setLevel(logging.DEBUG)\nopenml_pytorch.config.logger.setLevel(logging.DEBUG)\n</pre> # Enable logging in order to observe the progress while running the example. openml.config.logger.setLevel(logging.DEBUG) openml_pytorch.config.logger.setLevel(logging.DEBUG) In\u00a0[4]: Copied! <pre># Load the pre-trained ResNet model\nmodel = models.resnet18(pretrained=True, progress=True)\n\n# Modify the last fully connected layer to the required number of classes\nnum_classes = 20 # For the dataset we are using\nin_features = model.fc.in_features\nmodel.fc = nn.Linear(in_features, num_classes)\n</pre> # Load the pre-trained ResNet model model = models.resnet18(pretrained=True, progress=True)  # Modify the last fully connected layer to the required number of classes num_classes = 20 # For the dataset we are using in_features = model.fc.in_features model.fc = nn.Linear(in_features, num_classes) <pre>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n</pre> In\u00a0[5]: Copied! <pre># Optional: If you're fine-tuning, you may want to freeze the pre-trained layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# If you want to train the last layer only (the newly added layer)\nfor param in model.fc.parameters():\n    param.requires_grad = True\n</pre>  # Optional: If you're fine-tuning, you may want to freeze the pre-trained layers for param in model.parameters():     param.requires_grad = False  # If you want to train the last layer only (the newly added layer) for param in model.fc.parameters():     param.requires_grad = True In\u00a0[6]: Copied! <pre># Setting an appropriate optimizer \n\ndef custom_optimizer_gen(model: torch.nn.Module, task: OpenMLTask) -&gt; torch.optim.Optimizer:\n    return torch.optim.Adam(model.fc.parameters())\n\nopenml_pytorch.config.optimizer_gen = custom_optimizer_gen\n</pre> # Setting an appropriate optimizer   def custom_optimizer_gen(model: torch.nn.Module, task: OpenMLTask) -&gt; torch.optim.Optimizer:     return torch.optim.Adam(model.fc.parameters())  openml_pytorch.config.optimizer_gen = custom_optimizer_gen In\u00a0[7]: Copied! <pre># Download the OpenML task for the Meta_Album_PNU_Micro dataset.\ntask = openml.tasks.get_task(361152)\n\n############################################################################\n# Run the model on the task (requires an API key).m\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> # Download the OpenML task for the Meta_Album_PNU_Micro dataset. task = openml.tasks.get_task(361152)  ############################################################################ # Run the model on the task (requires an API key).m run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) <pre>/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n/Users/eragon/Documents/CODE/Github/openml-pytorch/openml_pytorch/extension.py:154: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.loc[:, 'encoded_labels'] = label_encoder.transform(y)\n</pre> In\u00a0[15]: Copied! <pre>import openml\n</pre> import openml In\u00a0[26]: Copied! <pre>print(run.flow)\n</pre> print(run.flow) <pre>OpenML Flow\n===========\nFlow Name.......: torch.nn.ResNet.73f8a33b44a6743\nFlow Description: Automatically created pytorch flow.\nDependencies....: torch==2.1.2\nnumpy&gt;=1.6.1\nscipy&gt;=0.9\n</pre> In\u00a0[\u00a0]: Copied! <pre># Publish the experiment on OpenML (optional, requires an API key).\nrun.publish()\n\nprint('URL for run: %s/run/%d' % (openml.config.server, run.run_id))\n</pre>  # Publish the experiment on OpenML (optional, requires an API key). run.publish()  print('URL for run: %s/run/%d' % (openml.config.server, run.run_id))"},{"location":"integrations/Pytorch/basic_tutorial/#pytorch-sequential-classification-model-example","title":"PyTorch sequential classification model example\u00b6","text":"<p>An example of a sequential network that classifies digit images used as an OpenML flow. We use sub networks here in order to show the that network hierarchies can be achieved with ease.</p>"},{"location":"integrations/Scikit-learn/","title":"scikit-learn","text":"<p>OpenML is readily integrated with scikit-learn through the Python API. This page provides a brief overview of the key features and installation instructions. For more detailed API documentation, please refer to the official documentation.</p>"},{"location":"integrations/Scikit-learn/#key-features","title":"Key features:","text":"<ul> <li>Query and download OpenML datasets and use them however you like</li> <li>Build any sklearn estimator or pipeline and convert to OpenML flows</li> <li>Run any flow on any task and save the experiment as run objects</li> <li>Upload your runs for collaboration or publishing</li> <li>Query, download and reuse all shared runs</li> </ul>"},{"location":"integrations/Scikit-learn/#installation","title":"Installation","text":"<pre><code>pip install openml\n</code></pre>"},{"location":"integrations/Scikit-learn/#query-and-download-data","title":"Query and download data","text":"<pre><code>import openml\n\n# List all datasets and their properties\nopenml.datasets.list_datasets(output_format=\"dataframe\")\n\n# Get dataset by ID\ndataset = openml.datasets.get_dataset(61)\n\n# Get dataset by name\ndataset = openml.datasets.get_dataset('Fashion-MNIST')\n\n# Get the data itself as a dataframe (or otherwise)\nX, y, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n</code></pre>"},{"location":"integrations/Scikit-learn/#download-tasks-run-models-locally-publish-results-with-scikit-learn","title":"Download tasks, run models locally, publish results (with scikit-learn)","text":"<pre><code>from sklearn import ensemble\nfrom openml import tasks, runs\n\n# Build any model you like\nclf = ensemble.RandomForestClassifier()\n\n# Download any OpenML task\ntask = tasks.get_task(3954)\n\n# Run and evaluate your model on the task\nrun = runs.run_model_on_task(clf, task)\n\n# Share the results on OpenML. Your API key can be found in your account.\n# openml.config.apikey = 'YOUR_KEY'\nrun.publish()\n</code></pre>"},{"location":"integrations/Scikit-learn/#openml-benchmarks","title":"OpenML Benchmarks","text":"<pre><code># List all tasks in a benchmark\nbenchmark = openml.study.get_suite('OpenML-CC18')\ntasks.list_tasks(output_format=\"dataframe\", task_id=benchmark.tasks)\n\n# Return benchmark results\nopenml.evaluations.list_evaluations(\n    function=\"area_under_roc_curve\",\n    tasks=benchmark.tasks,\n    output_format=\"dataframe\"\n)\n</code></pre>"},{"location":"integrations/Scikit-learn/basic_tutorial/","title":"Basic tutorial","text":"In\u00a0[12]: Copied! <pre>from IPython.display import display, HTML, Markdown\nimport os\nimport yaml\nwith open(\"../../../mkdocs.yml\", \"r\") as f:\n    load_config = yaml.safe_load(f)\nrepo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\")\nbinder_url = load_config[\"binder_url\"]\nrelative_file_path = \"integrations/Scikit-learn/basic_tutorial.ipynb\"\ndisplay(HTML(f\"\"\"&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/{repo_url}/{relative_file_path}\"&gt;\n  &lt;img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/&gt;\n&lt;/a&gt;\"\"\"))\ndisplay(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\"))\n</pre> from IPython.display import display, HTML, Markdown import os import yaml with open(\"../../../mkdocs.yml\", \"r\") as f:     load_config = yaml.safe_load(f) repo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\") binder_url = load_config[\"binder_url\"] relative_file_path = \"integrations/Scikit-learn/basic_tutorial.ipynb\" display(HTML(f\"\"\" \"\"\")) display(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\")) In\u00a0[\u00a0]: Copied! <pre>!pip install openml\n</pre> !pip install openml In\u00a0[2]: Copied! <pre>import openml\nfrom sklearn import impute, tree, pipeline\n</pre> import openml from sklearn import impute, tree, pipeline In\u00a0[7]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <pre>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/openml/config.py:184: UserWarning: Switching to the test server https://test.openml.org/api/v1/xml to not upload results to the live server. Using the test server may result in reduced performance of the API!\n  warnings.warn(\n</pre> In\u00a0[8]: Copied! <pre># Define a scikit-learn classifier or pipeline\nclf = pipeline.Pipeline(\n    steps=[\n        ('imputer', impute.SimpleImputer()),\n        ('estimator', tree.DecisionTreeClassifier())\n    ]\n)\n</pre>  # Define a scikit-learn classifier or pipeline clf = pipeline.Pipeline(     steps=[         ('imputer', impute.SimpleImputer()),         ('estimator', tree.DecisionTreeClassifier())     ] )  In\u00a0[9]: Copied! <pre># Download the OpenML task for the pendigits dataset with 10-fold\n# cross-validation.\ntask = openml.tasks.get_task(32)\ntask\n</pre>  # Download the OpenML task for the pendigits dataset with 10-fold # cross-validation. task = openml.tasks.get_task(32) task Out[9]: <pre>OpenML Classification Task\n==========================\nTask Type Description: https://test.openml.org/tt/TaskType.SUPERVISED_CLASSIFICATION\nTask ID..............: 32\nTask URL.............: https://test.openml.org/t/32\nEstimation Procedure.: crossvalidation\nTarget Feature.......: class\n# of Classes.........: 10\nCost Matrix..........: Available</pre> In\u00a0[11]: Copied! <pre># Run the scikit-learn model on the task.\nrun = openml.runs.run_model_on_task(clf, task)\n# Publish the experiment on OpenML (optional, requires an API key.\n# You can get your own API key by signing up to OpenML.org)\n</pre> # Run the scikit-learn model on the task. run = openml.runs.run_model_on_task(clf, task) # Publish the experiment on OpenML (optional, requires an API key. # You can get your own API key by signing up to OpenML.org)  In\u00a0[\u00a0]: Copied! <pre>run.publish()\nprint(f'View the run online: {run.openml_url}')\n</pre>  run.publish() print(f'View the run online: {run.openml_url}')"},{"location":"integrations/Scikit-learn/datasets_tutorial/","title":"Datasets","text":"In\u00a0[2]: Copied! <pre>from IPython.display import display, HTML, Markdown\nimport os\nimport yaml\nwith open(\"../../../mkdocs.yml\", \"r\") as f:\n    load_config = yaml.safe_load(f)\nrepo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\")\nbinder_url = load_config[\"binder_url\"]\nrelative_file_path = \"integrations/Scikit-learn/datasets_tutorial.ipynb\"\ndisplay(HTML(f\"\"\"&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/{repo_url}/{relative_file_path}\"&gt;\n  &lt;img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/&gt;\n&lt;/a&gt;\"\"\"))\ndisplay(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\"))\n</pre> from IPython.display import display, HTML, Markdown import os import yaml with open(\"../../../mkdocs.yml\", \"r\") as f:     load_config = yaml.safe_load(f) repo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\") binder_url = load_config[\"binder_url\"] relative_file_path = \"integrations/Scikit-learn/datasets_tutorial.ipynb\" display(HTML(f\"\"\" \"\"\")) display(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\")) In\u00a0[9]: Copied! <pre>!pip install openml\n</pre> !pip install openml <pre>Requirement already satisfied: openml in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (0.14.2)\nRequirement already satisfied: scikit-learn&gt;=0.18 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (1.4.2)\nRequirement already satisfied: requests in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.31.0)\nRequirement already satisfied: liac-arff&gt;=2.4.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.5.0)\nRequirement already satisfied: numpy&gt;=1.6.2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (1.26.4)\nRequirement already satisfied: minio in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (7.2.7)\nRequirement already satisfied: pandas&gt;=1.0.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.2.2)\nRequirement already satisfied: scipy&gt;=0.13.3 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (1.13.0)\nRequirement already satisfied: pyarrow in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (16.0.0)\nRequirement already satisfied: xmltodict in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (0.13.0)\nRequirement already satisfied: python-dateutil in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.9.0.post0)\nRequirement already satisfied: tzdata&gt;=2022.7 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pandas&gt;=1.0.0-&gt;openml) (2024.1)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pandas&gt;=1.0.0-&gt;openml) (2024.1)\nRequirement already satisfied: six&gt;=1.5 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from python-dateutil-&gt;openml) (1.16.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from scikit-learn&gt;=0.18-&gt;openml) (3.5.0)\nRequirement already satisfied: joblib&gt;=1.2.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from scikit-learn&gt;=0.18-&gt;openml) (1.4.0)\nRequirement already satisfied: urllib3 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (2.2.1)\nRequirement already satisfied: typing-extensions in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (4.11.0)\nRequirement already satisfied: pycryptodome in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (3.20.0)\nRequirement already satisfied: certifi in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (2024.2.2)\nRequirement already satisfied: argon2-cffi in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (23.1.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests-&gt;openml) (3.7)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests-&gt;openml) (3.3.2)\nRequirement already satisfied: argon2-cffi-bindings in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from argon2-cffi-&gt;minio-&gt;openml) (21.2.0)\nRequirement already satisfied: cffi&gt;=1.0.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;minio-&gt;openml) (1.16.0)\nRequirement already satisfied: pycparser in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;minio-&gt;openml) (2.22)\n\n[notice] A new release of pip is available: 23.0.1 -&gt; 24.0\n[notice] To update, run: pip install --upgrade pip\n</pre> In\u00a0[2]: Copied! <pre># License: BSD 3-Clauses\n\nimport openml\nimport pandas as pd\nfrom openml.datasets import edit_dataset, fork_dataset, get_dataset\n</pre> # License: BSD 3-Clauses  import openml import pandas as pd from openml.datasets import edit_dataset, fork_dataset, get_dataset In\u00a0[3]: Copied! <pre>datalist = openml.datasets.list_datasets(output_format=\"dataframe\")\ndatalist = datalist[[\"did\", \"name\", \"NumberOfInstances\", \"NumberOfFeatures\", \"NumberOfClasses\"]]\n\nprint(f\"First 10 of {len(datalist)} datasets...\")\ndatalist.head(n=10)\n\n# The same can be done with lesser lines of code\nopenml_df = openml.datasets.list_datasets(output_format=\"dataframe\")\nopenml_df.head(n=10)\n</pre> datalist = openml.datasets.list_datasets(output_format=\"dataframe\") datalist = datalist[[\"did\", \"name\", \"NumberOfInstances\", \"NumberOfFeatures\", \"NumberOfClasses\"]]  print(f\"First 10 of {len(datalist)} datasets...\") datalist.head(n=10)  # The same can be done with lesser lines of code openml_df = openml.datasets.list_datasets(output_format=\"dataframe\") openml_df.head(n=10) <pre>First 10 of 5466 datasets...\n</pre> Out[3]: did name version uploader status format MajorityClassSize MaxNominalAttDistinctValues MinorityClassSize NumberOfClasses NumberOfFeatures NumberOfInstances NumberOfInstancesWithMissingValues NumberOfMissingValues NumberOfNumericFeatures NumberOfSymbolicFeatures 2 2 anneal 1 1 active ARFF 684.0 7.0 8.0 5.0 39.0 898.0 898.0 22175.0 6.0 33.0 3 3 kr-vs-kp 1 1 active ARFF 1669.0 3.0 1527.0 2.0 37.0 3196.0 0.0 0.0 0.0 37.0 4 4 labor 1 1 active ARFF 37.0 3.0 20.0 2.0 17.0 57.0 56.0 326.0 8.0 9.0 5 5 arrhythmia 1 1 active ARFF 245.0 13.0 2.0 13.0 280.0 452.0 384.0 408.0 206.0 74.0 6 6 letter 1 1 active ARFF 813.0 26.0 734.0 26.0 17.0 20000.0 0.0 0.0 16.0 1.0 7 7 audiology 1 1 active ARFF 57.0 24.0 1.0 24.0 70.0 226.0 222.0 317.0 0.0 70.0 8 8 liver-disorders 1 1 active ARFF NaN NaN NaN 0.0 6.0 345.0 0.0 0.0 6.0 0.0 9 9 autos 1 1 active ARFF 67.0 22.0 3.0 6.0 26.0 205.0 46.0 59.0 15.0 11.0 10 10 lymph 1 1 active ARFF 81.0 8.0 2.0 4.0 19.0 148.0 0.0 0.0 3.0 16.0 11 11 balance-scale 1 1 active ARFF 288.0 3.0 49.0 3.0 5.0 625.0 0.0 0.0 4.0 1.0 In\u00a0[4]: Copied! <pre>datalist[datalist.NumberOfInstances &gt; 10000].sort_values([\"NumberOfInstances\"]).head(n=20)\n\"\"\ndatalist.query('name == \"eeg-eye-state\"')\n\"\"\ndatalist.query(\"NumberOfClasses &gt; 50\")\n</pre> datalist[datalist.NumberOfInstances &gt; 10000].sort_values([\"NumberOfInstances\"]).head(n=20) \"\" datalist.query('name == \"eeg-eye-state\"') \"\" datalist.query(\"NumberOfClasses &gt; 50\") Out[4]: did name NumberOfInstances NumberOfFeatures NumberOfClasses 1491 1491 one-hundred-plants-margin 1600.0 65.0 100.0 1492 1492 one-hundred-plants-shape 1600.0 65.0 100.0 1493 1493 one-hundred-plants-texture 1599.0 65.0 100.0 4552 4552 BachChoralHarmony 5665.0 17.0 102.0 41167 41167 dionis 416188.0 61.0 355.0 41169 41169 helena 65196.0 28.0 100.0 41960 41960 seattlecrime6 523590.0 8.0 144.0 41983 41983 CIFAR-100 60000.0 3073.0 100.0 42078 42078 beer_reviews 1586614.0 13.0 104.0 42087 42087 beer_reviews 1586614.0 13.0 104.0 42088 42088 beer_reviews 1586614.0 13.0 104.0 42089 42089 vancouver_employee 1586614.0 13.0 104.0 42123 42123 article_influence 3615.0 7.0 3169.0 42223 42223 dataset-autoHorse_fixed 201.0 69.0 186.0 42396 42396 aloi 108000.0 129.0 1000.0 43723 43723 Toronto-Apartment-Rental-Price 1124.0 7.0 188.0 44282 44282 Meta_Album_PLK_Mini 3440.0 3.0 86.0 44283 44283 Meta_Album_FLW_Mini 4080.0 3.0 102.0 44284 44284 Meta_Album_SPT_Mini 2920.0 3.0 73.0 44285 44285 Meta_Album_BRD_Mini 12600.0 3.0 315.0 44288 44288 Meta_Album_TEX_Mini 2560.0 3.0 64.0 44289 44289 Meta_Album_CRS_Mini 7840.0 3.0 196.0 44292 44292 Meta_Album_INS_2_Mini 4080.0 3.0 102.0 44298 44298 Meta_Album_DOG_Mini 4800.0 3.0 120.0 44304 44304 Meta_Album_TEX_ALOT_Mini 10000.0 3.0 250.0 44306 44306 Meta_Album_INS_Mini 4160.0 3.0 104.0 44317 44317 Meta_Album_PLK_Extended 473273.0 3.0 102.0 44318 44318 Meta_Album_FLW_Extended 8189.0 3.0 102.0 44319 44319 Meta_Album_SPT_Extended 10416.0 3.0 73.0 44320 44320 Meta_Album_BRD_Extended 49054.0 3.0 315.0 44322 44322 Meta_Album_TEX_Extended 8675.0 3.0 64.0 44323 44323 Meta_Album_CRS_Extended 16185.0 3.0 196.0 44326 44326 Meta_Album_INS_2_Extended 75222.0 3.0 102.0 44331 44331 Meta_Album_DOG_Extended 20480.0 3.0 120.0 44337 44337 Meta_Album_TEX_ALOT_Extended 25000.0 3.0 250.0 44340 44340 Meta_Album_INS_Extended 170506.0 3.0 117.0 44533 44533 dionis_seed_0_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44534 44534 dionis_seed_1_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44535 44535 dionis_seed_2_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44536 44536 dionis_seed_3_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44537 44537 dionis_seed_4_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44728 44728 helena_seed_0_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 44729 44729 helena_seed_1_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 44730 44730 helena_seed_2_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 44731 44731 helena_seed_3_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 44732 44732 helena_seed_4_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 45049 45049 MD_MIX_Mini_Copy 28240.0 69.0 706.0 45102 45102 dailybike 731.0 13.0 606.0 45103 45103 dailybike 731.0 13.0 606.0 45104 45104 PLK_Mini_Copy 3440.0 3.0 86.0 45274 45274 PASS 1439588.0 7.0 94137.0 45569 45569 DBLP-QuAD 10000.0 10.0 9999.0 45923 45923 IndoorScenes 15620.0 3.0 67.0 45936 45936 IndoorScenes 15620.0 3.0 67.0 In\u00a0[5]: Copied! <pre># This is done based on the dataset ID.\ndataset = openml.datasets.get_dataset(1471)\n\n# Print a summary\nprint(\n    f\"This is dataset '{dataset.name}', the target feature is \"\n    f\"'{dataset.default_target_attribute}'\"\n)\nprint(f\"URL: {dataset.url}\")\nprint(dataset.description[:500])\n</pre> # This is done based on the dataset ID. dataset = openml.datasets.get_dataset(1471)  # Print a summary print(     f\"This is dataset '{dataset.name}', the target feature is \"     f\"'{dataset.default_target_attribute}'\" ) print(f\"URL: {dataset.url}\") print(dataset.description[:500]) <pre>This is dataset 'eeg-eye-state', the target feature is 'Class'\nURL: https://api.openml.org/data/v1/download/1587924/eeg-eye-state.arff\n**Author**: Oliver Roesler  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State), Baden-Wuerttemberg, Cooperative State University (DHBW), Stuttgart, Germany  \n**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html)  \n\nAll data is from one continuous EEG measurement with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and added later manually to the file after\n</pre> <p>Get the actual data.</p> <p>openml-python returns data as pandas dataframes (stored in the <code>eeg</code> variable below), and also some additional metadata that we don't care about right now.</p> In\u00a0[6]: Copied! <pre>eeg, *_ = dataset.get_data()\n</pre> eeg, *_ = dataset.get_data() <p>You can optionally choose to have openml separate out a column from the dataset. In particular, many datasets for supervised problems have a set <code>default_target_attribute</code> which may help identify the target variable.</p> In\u00a0[7]: Copied! <pre>X, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute\n)\nprint(X.head())\nprint(X.info())\n</pre> X, y, categorical_indicator, attribute_names = dataset.get_data(     target=dataset.default_target_attribute ) print(X.head()) print(X.info()) <pre>        V1       V2       V3       V4       V5       V6       V7       V8  \\\n0  4329.23  4009.23  4289.23  4148.21  4350.26  4586.15  4096.92  4641.03   \n1  4324.62  4004.62  4293.85  4148.72  4342.05  4586.67  4097.44  4638.97   \n2  4327.69  4006.67  4295.38  4156.41  4336.92  4583.59  4096.92  4630.26   \n3  4328.72  4011.79  4296.41  4155.90  4343.59  4582.56  4097.44  4630.77   \n4  4326.15  4011.79  4292.31  4151.28  4347.69  4586.67  4095.90  4627.69   \n\n        V9      V10      V11      V12      V13      V14  \n0  4222.05  4238.46  4211.28  4280.51  4635.90  4393.85  \n1  4210.77  4226.67  4207.69  4279.49  4632.82  4384.10  \n2  4207.69  4222.05  4206.67  4282.05  4628.72  4389.23  \n3  4217.44  4235.38  4210.77  4287.69  4632.31  4396.41  \n4  4210.77  4244.10  4212.82  4288.21  4632.82  4398.46  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14980 entries, 0 to 14979\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   V1      14980 non-null  float64\n 1   V2      14980 non-null  float64\n 2   V3      14980 non-null  float64\n 3   V4      14980 non-null  float64\n 4   V5      14980 non-null  float64\n 5   V6      14980 non-null  float64\n 6   V7      14980 non-null  float64\n 7   V8      14980 non-null  float64\n 8   V9      14980 non-null  float64\n 9   V10     14980 non-null  float64\n 10  V11     14980 non-null  float64\n 11  V12     14980 non-null  float64\n 12  V13     14980 non-null  float64\n 13  V14     14980 non-null  float64\ndtypes: float64(14)\nmemory usage: 1.6 MB\nNone\n</pre> <p>Sometimes you only need access to a dataset's metadata. In those cases, you can download the dataset without downloading the data file. The dataset object can be used as normal. Whenever you use any functionality that requires the data, such as <code>get_data</code>, the data will be downloaded. Starting from 0.15, not downloading data will be the default behavior instead. The data will be downloading automatically when you try to access it through openml objects, e.g., using <code>dataset.features</code>.</p> In\u00a0[8]: Copied! <pre>dataset = openml.datasets.get_dataset(1471, download_data=False)\n</pre> dataset = openml.datasets.get_dataset(1471, download_data=False) In\u00a0[9]: Copied! <pre>eegs = eeg.sample(n=1000)\n_ = pd.plotting.scatter_matrix(\n    X.iloc[:100, :4],\n    c=y[:100],\n    figsize=(10, 10),\n    marker=\"o\",\n    hist_kwds={\"bins\": 20},\n    alpha=0.8,\n    cmap=\"plasma\",\n)\n</pre> eegs = eeg.sample(n=1000) _ = pd.plotting.scatter_matrix(     X.iloc[:100, :4],     c=y[:100],     figsize=(10, 10),     marker=\"o\",     hist_kwds={\"bins\": 20},     alpha=0.8,     cmap=\"plasma\", ) <pre>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/pandas/plotting/_matplotlib/misc.py:97: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  ax.scatter(\n</pre> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <p>Edit non-critical fields, allowed for all authorized users: description, creator, contributor, collection_date, language, citation, original_data_url, paper_url</p> In\u00a0[\u00a0]: Copied! <pre>desc = (\n    \"This data sets consists of 3 different types of irises' \"\n    \"(Setosa, Versicolour, and Virginica) petal and sepal length,\"\n    \" stored in a 150x4 numpy.ndarray\"\n)\ndid = 128\ndata_id = edit_dataset(\n    did,\n    description=desc,\n    creator=\"R.A.Fisher\",\n    collection_date=\"1937\",\n    citation=\"The use of multiple measurements in taxonomic problems\",\n    language=\"English\",\n)\nedited_dataset = get_dataset(data_id)\nprint(f\"Edited dataset ID: {data_id}\")\n</pre> desc = (     \"This data sets consists of 3 different types of irises' \"     \"(Setosa, Versicolour, and Virginica) petal and sepal length,\"     \" stored in a 150x4 numpy.ndarray\" ) did = 128 data_id = edit_dataset(     did,     description=desc,     creator=\"R.A.Fisher\",     collection_date=\"1937\",     citation=\"The use of multiple measurements in taxonomic problems\",     language=\"English\", ) edited_dataset = get_dataset(data_id) print(f\"Edited dataset ID: {data_id}\") <p>Editing critical fields (default_target_attribute, row_id_attribute, ignore_attribute) is allowed only for the dataset owner. Further, critical fields cannot be edited if the dataset has any tasks associated with it. To edit critical fields of a dataset (without tasks) owned by you, configure the API key: openml.config.apikey = 'FILL_IN_OPENML_API_KEY' This example here only shows a failure when trying to work on a dataset not owned by you:</p> In\u00a0[\u00a0]: Copied! <pre>try:\n    data_id = edit_dataset(1, default_target_attribute=\"shape\")\nexcept openml.exceptions.OpenMLServerException as e:\n    print(e)\n</pre> try:     data_id = edit_dataset(1, default_target_attribute=\"shape\") except openml.exceptions.OpenMLServerException as e:     print(e) In\u00a0[\u00a0]: Copied! <pre>data_id = fork_dataset(1)\nprint(data_id)\ndata_id = edit_dataset(data_id, default_target_attribute=\"shape\")\nprint(f\"Forked dataset ID: {data_id}\")\n\nopenml.config.stop_using_configuration_for_example()\n</pre> data_id = fork_dataset(1) print(data_id) data_id = edit_dataset(data_id, default_target_attribute=\"shape\") print(f\"Forked dataset ID: {data_id}\")  openml.config.stop_using_configuration_for_example()"},{"location":"integrations/Scikit-learn/datasets_tutorial/#datasets","title":"Datasets\u00b6","text":"<p>How to list and download datasets.</p>"},{"location":"integrations/Scikit-learn/datasets_tutorial/#exercise-0","title":"Exercise 0\u00b6","text":"<ul> <li><p>List datasets</p> <ul> <li>Use the output_format parameter to select output type</li> <li>Default gives 'dict' (other option: 'dataframe', see below)</li> </ul> </li> </ul> <p>Note: list_datasets will return a pandas dataframe by default from 0.15. When using openml-python 0.14, <code>list_datasets</code> will warn you to use output_format='dataframe'.</p>"},{"location":"integrations/Scikit-learn/datasets_tutorial/#exercise-1","title":"Exercise 1\u00b6","text":"<ul> <li>Find datasets with more than 10000 examples.</li> <li>Find a dataset called 'eeg_eye_state'.</li> <li>Find all datasets with more than 50 classes.</li> </ul>"},{"location":"integrations/Scikit-learn/datasets_tutorial/#download-datasets","title":"Download datasets\u00b6","text":""},{"location":"integrations/Scikit-learn/datasets_tutorial/#exercise-2","title":"Exercise 2\u00b6","text":"<ul> <li>Explore the data visually.</li> </ul>"},{"location":"integrations/Scikit-learn/datasets_tutorial/#edit-a-created-dataset","title":"Edit a created dataset\u00b6","text":"<p>This example uses the test server, to avoid editing a dataset on the main server.</p> Warning<p>.. include:: ../../test_server_usage_warning.txt</p>"},{"location":"integrations/Scikit-learn/datasets_tutorial/#fork-dataset","title":"Fork dataset\u00b6","text":"<p>Used to create a copy of the dataset with you as the owner. Use this API only if you are unable to edit the critical fields (default_target_attribute, ignore_attribute, row_id_attribute) of a dataset through the edit_dataset API. After the dataset is forked, you can edit the new version of the dataset using edit_dataset.</p>"},{"location":"scripts/github_scraper/","title":"Github scraper","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"\nScript to scrape the github repositories of the projects in the showcase_urls.txt file and generate a markdown file with a grid of cards with the information of the repositories.\n\nDoes not rely on the GitHub API, so it is limited to the information that can be scraped from the GitHub website.\n\nInspired in part from https://brightdata.com/blog/how-tos/how-to-scrape-github-repositories-in-python\n\"\"\"\n</pre> \"\"\" Script to scrape the github repositories of the projects in the showcase_urls.txt file and generate a markdown file with a grid of cards with the information of the repositories.  Does not rely on the GitHub API, so it is limited to the information that can be scraped from the GitHub website.  Inspired in part from https://brightdata.com/blog/how-tos/how-to-scrape-github-repositories-in-python \"\"\" In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\n</pre> import requests from bs4 import BeautifulSoup from tqdm import tqdm In\u00a0[\u00a0]: Copied! <pre>with open(\"scripts/showcase_urls.txt\", \"r\") as file:\n    target_urls = file.readlines()\n    target_urls = [url.strip() for url in target_urls]\nmain_info = \"\"\"# Showcase\n\nThis page is a showcase of some projects and research done using the OpenML libary. Did you use OpenML in your work and want to share it with the community? We would love to have you!\n\nSimply create a pull request with the necessary information and we will add it to this page.\\n\"\"\"\n</pre> with open(\"scripts/showcase_urls.txt\", \"r\") as file:     target_urls = file.readlines()     target_urls = [url.strip() for url in target_urls] main_info = \"\"\"# Showcase  This page is a showcase of some projects and research done using the OpenML libary. Did you use OpenML in your work and want to share it with the community? We would love to have you!  Simply create a pull request with the necessary information and we will add it to this page.\\n\"\"\" In\u00a0[\u00a0]: Copied! <pre>def get_github_info(target_url):\n    \"\"\"\n    Get the name, description and number of stars of a GitHub repository from its URL.\n    \"\"\"\n    print(target_url)\n    page = requests.get(target_url)\n    soup = BeautifulSoup(page.text, \"html.parser\")\n    name_html_element = soup.select_one('[itemprop=\"name\"]')\n    name = name_html_element.text.strip()\n\n    bordergrid_html_element = soup.select_one(\".BorderGrid\")\n    about_html_element = bordergrid_html_element.select_one(\"h2\")\n    description_html_element = about_html_element.find_next_sibling(\"p\")\n    description = description_html_element.get_text().strip()\n\n    star_icon_html_element = bordergrid_html_element.select_one(\".octicon-star\")\n    stars_html_element = star_icon_html_element.find_next_sibling(\"strong\")\n    stars = stars_html_element.get_text().strip().replace(\",\", \"\")\n\n    return name, description, stars\n</pre> def get_github_info(target_url):     \"\"\"     Get the name, description and number of stars of a GitHub repository from its URL.     \"\"\"     print(target_url)     page = requests.get(target_url)     soup = BeautifulSoup(page.text, \"html.parser\")     name_html_element = soup.select_one('[itemprop=\"name\"]')     name = name_html_element.text.strip()      bordergrid_html_element = soup.select_one(\".BorderGrid\")     about_html_element = bordergrid_html_element.select_one(\"h2\")     description_html_element = about_html_element.find_next_sibling(\"p\")     description = description_html_element.get_text().strip()      star_icon_html_element = bordergrid_html_element.select_one(\".octicon-star\")     stars_html_element = star_icon_html_element.find_next_sibling(\"strong\")     stars = stars_html_element.get_text().strip().replace(\",\", \"\")      return name, description, stars In\u00a0[\u00a0]: Copied! <pre>def return_details(target_urls):\n    \"\"\"\n    For a list of GitHub URLs, return a dictionary with the name, description and number of stars of the repositories.\n    \"\"\"\n    target_urls = list(set(target_urls))  # remove duplicates\n    urls = {}\n    for target_url in target_urls:\n        name, description, stars = get_github_info(target_url)\n        if len(name) &gt; 0:\n            urls[target_url] = {\n                \"name\": name,\n                \"description\": description,\n                \"stars\": stars,\n            }\n    # sort by stars\n    urls = dict(\n        sorted(urls.items(), key=lambda item: int(item[1][\"stars\"]), reverse=True)\n    )\n    return urls\n</pre> def return_details(target_urls):     \"\"\"     For a list of GitHub URLs, return a dictionary with the name, description and number of stars of the repositories.     \"\"\"     target_urls = list(set(target_urls))  # remove duplicates     urls = {}     for target_url in target_urls:         name, description, stars = get_github_info(target_url)         if len(name) &gt; 0:             urls[target_url] = {                 \"name\": name,                 \"description\": description,                 \"stars\": stars,             }     # sort by stars     urls = dict(         sorted(urls.items(), key=lambda item: int(item[1][\"stars\"]), reverse=True)     )     return urls In\u00a0[\u00a0]: Copied! <pre>def return_div(url, urls):\n    \"\"\"\n        Return a div element with the information of a GitHub repository. Creates a card with the name, description and number of stars of the repository.\n\n        Example CSS\n\n    .card-container {\n        display: flex;\n        flex-wrap: wrap;\n        gap: 20px;\n        justify-content: center;\n      }\n\n      .card {\n        border: 1px solid #ccc;\n        border-radius: 5px;\n        padding: 20px;\n        width: 300px;\n        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);\n      }\n\n      .card h2 {\n        margin-top: 0;\n      }\n\n      .card p {\n        margin-bottom: 0;}\n\n        .github-logo {\n          height: 15px;\n          width: 13px;\n          margin-left: 10px;\n      }\n\n      iframe[seamless] {\n        border: none;\n    }\n    \"\"\"\n    info = urls[url]\n    return f\"\"\"\n    \\n&lt;div class=\"card\"&gt;\n    &lt;h2&gt;&lt;a href=\"{url}\"&gt;{info['name']} &lt;img class=\"github-logo\" src=\"../img/logo-github.svg\"&gt; &lt;small&gt;{info['stars']} stars&lt;/small&gt;&lt;/a&gt;&lt;/h2&gt;\n    &lt;p&gt;{info['description']}&lt;/p&gt;\n    &lt;/div&gt;\\n\n    \"\"\"\n</pre> def return_div(url, urls):     \"\"\"         Return a div element with the information of a GitHub repository. Creates a card with the name, description and number of stars of the repository.          Example CSS      .card-container {         display: flex;         flex-wrap: wrap;         gap: 20px;         justify-content: center;       }        .card {         border: 1px solid #ccc;         border-radius: 5px;         padding: 20px;         width: 300px;         box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);       }        .card h2 {         margin-top: 0;       }        .card p {         margin-bottom: 0;}          .github-logo {           height: 15px;           width: 13px;           margin-left: 10px;       }        iframe[seamless] {         border: none;     }     \"\"\"     info = urls[url]     return f\"\"\"     \\n {info['name']}  {info['stars']} stars <p>{info['description']}</p> \\n     \"\"\" In\u00a0[\u00a0]: Copied! <pre>def generate_page(info):\n    \"\"\"\n    Generate a page with a grid of cards with the information of the repositories.\n    \"\"\"\n\n    page = \"\"\"&lt;div class=\"card-container\"&gt;\\n\"\"\"\n    for target_url in tqdm(info.keys(), total=len(info)):\n        page += return_div(target_url, info)\n    page += \"&lt;/div&gt;\"\n    return page\n</pre> def generate_page(info):     \"\"\"     Generate a page with a grid of cards with the information of the repositories.     \"\"\"      page = \"\"\"\\n\"\"\"     for target_url in tqdm(info.keys(), total=len(info)):         page += return_div(target_url, info)     page += \"\"     return page In\u00a0[\u00a0]: Copied! <pre>info = return_details(target_urls)\n# print(generate_page(info))\nwith open(\"showcase.md\", \"w\") as file:\n    file.write(main_info)\n    file.write(generate_page(info))\n</pre> info = return_details(target_urls) # print(generate_page(info)) with open(\"showcase.md\", \"w\") as file:     file.write(main_info)     file.write(generate_page(info)) <p>test = [\"https://github.com/openml/openml-python\"] print(return_details(test))</p>"},{"location":"showcase/","title":"Showcase","text":"<p>This page is a showcase of some projects and research done using the OpenML libary. Did you use OpenML in your work and want to share it with the community? We would love to have you!</p> <p>Simply create a pull request with the necessary information and we will add it to this page.</p> openml-python  274 stars <p>Python module to interface with OpenML</p> openml-r  95 stars <p>R package to interface with OpenML</p> OpenML.jl  10 stars <p>Partial implementation of the OpenML API for Julia</p> openml-rust  10 stars <p>A rust interface to http://openml.org/</p> continual-automl  5 stars <p>Adaptations of AutoML libraries H2O, Autosklearn and GAMA for stream learning</p> openml-dotnet  5 stars <p>.NET API</p> openml-rapidminer  3 stars <p>RapidMiner plugin</p> openml-pytorch  3 stars <p>Pytorch extension for openml-python</p> openml-tensorflow  2 stars <p>Tensorflow extension for openml-python</p> openml-croissant  0 stars <p>Converting dataset metadata from OpenML to Croissant format</p> flow-visualization  0 stars <p>Tool to convert openml flows to ONNX and visualize them via Netron</p> openml-mxnet  0 stars <p>MXNet extension for openml</p> openml-onnx  0 stars <p>onnx extension for openml</p> openml-azure  0 stars <p>Tools for interfacing with Azure</p> OpenmlCortana  0 stars <p>Openml Cortana connector</p> openml-keras  0 stars <p>Keras extension for openml-python</p>"},{"location":"showcase/research/research/","title":"Research using OpenML","text":"<p>This page contains a list of research papers that have used OpenML. If you have used OpenML in your research and would like to have your paper listed here, please drop a PR with the relevant information.</p>"}]}