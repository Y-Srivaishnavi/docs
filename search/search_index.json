{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"An open, automated, and frictionless machine learning environment. 1000s of data sets , uniformly formatted, easy to load, organized online Models and pipelines automatically uploaded from machine learning libraries Extensive APIs to integrate OpenML into your tools and scripts Easily reproducible results (e.g. models, evaluations) for comparison and reuse Stand on the shoulders of giants, and collaborate in real time Make your work more visible and reusable Built for automation: streamline your experiments and model building Concepts \u00b6 OpenML operates on a number of core concepts which are important to understand: Datasets Datasets are pretty straight-forward. Tabular datasets are self-contained, consisting of a number of rows ( instances ) and columns (features), including their data types. Other modalities (e.g. images) are included via paths to files stored within the same folder. Datasets are uniformly formatted ( S3 buckets with Parquet tables, JSON metadata, and media files), and are auto-converted and auto-loaded in your desired format by the APIs (e.g. in Python ) in a single line of code. Example: The Iris dataset or the Plankton dataset Tasks A task consists of a dataset, together with a machine learning task to perform, such as classification or clustering and an evaluation method. For supervised tasks, this also specifies the target column in the data. Example: Classifying different iris species from other attributes and evaluate using 10-fold cross-validation. Flows A flow identifies a particular machine learning algorithm (a pipeline or untrained model) from a particular library or framework, such as scikit-learn, pyTorch, or MLR. It contains details about the structure of the model/pipeline, dependencies (e.g. the library and its version) and a list of settable hyperparameters. In short, it is a serialized description of the algorithm that in many cases can also be deserialized to reinstantiate the exact same algorithm in a particular library. Example: scikit-learn's RandomForest or a simple TensorFlow model Runs A run is an experiment - it evaluates a particular flow (pipeline/model) with particular hyperparameter settings, on a particular task. Depending on the task it will include certain results, such as model evaluations (e.g. accuracies), model predictions, and other output files (e.g. the trained model). Example: Classifying Gamma rays with scikit-learn's RandomForest Data \u00b6 Discovery \u00b6 OpenML allows fine-grained search over thousands of machine learning datasets. Via the website , you can filter by many dataset properties, such as size, type, format, and many more. Via the APIs you have access to many more filters, and you can download a complete table with statistics of all datasest. Via the APIs you can also load datasets directly into your preferred data structures such as numpy ( example in Python ). We are also working on better organization of all datasets by topic Sharing \u00b6 You can upload and download datasets through the website or though the APIs (recommended). You can share data directly from common data science libraries, e.g. from Python or R dataframes, in a few lines of code. The OpenML APIs will automatically extract lots of meta-data and store all datasets in a uniform format. import pandas as pd import openml as oml # Create an OpenML dataset from a pandas dataframe df = pd . DataFrame ( data , columns = attribute_names ) my_data = oml . datasets . functions . create_dataset ( name = \"covertype\" , description = \"Predicting forest cover ...\" , licence = \"CC0\" , data = df ) # Share the dataset on OpenML my_data . publish () Every dataset gets a dedicated page on OpenML with all known information, and can be edited further online. Data hosted elsewhere can be referenced by URL. We are also working on interconnecting OpenML with other machine learning data set repositories Automated analysis \u00b6 OpenML will automatically analyze the data and compute a range of data quality characteristics . These include simple statistics such as the number of examples and features, but also potential quality issues (e.g. missing values) and more advanced statistics (e.g. the mutual information in the features and benchmark performances of simple models). These can be useful to find, filter and compare datasets, or to automate data preprocessing. We are also working on simple metrics and automated dataset quality reports The Analysis tab (see image below, or try it live ) also shows an automated and interactive analysis of all datasets. This runs on open-source Python code via Dash and we welcome all contributions The third tab, 'Tasks', lists all tasks created on the dataset. More on that below. Dataset ID and versions \u00b6 A dataset can be uniquely identified by its dataset ID, which is shown on the website and returned by the API. It's 1596 in the covertype example above. They can also be referenced by name and ID. OpenML assigns incremental version numbers per upload with the same name. You can also add a free-form version_label with every upload. Dataset status \u00b6 When you upload a dataset, it will be marked in_preparation until it is (automatically) verified. Once approved, the dataset will become active (or verified ). If a severe issue has been found with a dataset, it can become deactivated (or deprecated ) signaling that it should not be used. By default, dataset search only returns verified datasets, but you can access and download datasets with any status. Special attributes \u00b6 Machine learning datasets often have special attributes that require special handling in order to build useful models. OpenML marks these as special attributes. A target attribute is the column that is to be predicted, also known as dependent variable. Datasets can have a default target attribute set by the author, but OpenML tasks can also overrule this. Example: The default target variable for the MNIST dataset is to predict the class from pixel values, and most supervised tasks will have the class as their target. However, one can also create a task aimed at predicting the value of pixel257 given all the other pixel values and the class column. Row id attributes indicate (externally defined) row IDs. Ignore attributes are other columns that should not be included in training data. OpenML will clearly mark these, and will (by default) drop these columns when constructing training sets. Tasks \u00b6 Tasks describe what to do with the data. OpenML covers several task types , such as classification and clustering. Tasks are containers including the data and other information such as train/test splits, and define what needs to be returned. They are machine-readable so that you can automate machine learning experiments, and easily compare algorithms evaluations (using the exact same train-test splits) against all other benchmarks shared by others on OpenML. Collaborative benchmarks \u00b6 Tasks are real-time, collaborative benchmarks (e.g. see MNIST below). In the Analysis tab, you can view timelines and leaderboards, and learn from all prior submissions to design even better algorithms. Discover the best algorithms \u00b6 All algorithms evaluated on the same task (with the same train-test splits) can be directly compared to each other, so you can easily look up which algorithms perform best overall, and download their exact configurations. Likewise, you can look up the best algorithms for similar tasks to know what to try first. Automating benchmarks \u00b6 You can search and download existing tasks, evaluate your algorithms, and automatically share the results (which are stored in a run ). Here's what this looks like in the Python API. You can do the same across hundreds of tasks at once. from sklearn import ensemble from openml import tasks , runs # Build any model you like clf = ensemble . RandomForestClassifier () # Download any OpenML task (includes the datasets) task = tasks . get_task ( 3954 ) # Automatically evaluate your model on the task run = runs . run_model_on_task ( clf , task ) # Share the results on OpenML. run . publish () You can create new tasks via the website or via the APIs as well. Flows \u00b6 Flows are machine learning pipelines, models, or scripts. They are typically uploaded directly from machine learning libraries (e.g. scikit-learn, pyTorch, TensorFlow, MLR, WEKA,...) via the corresponding APIs . Associated code (e.g., on GitHub) can be referenced by URL. Analysing algorithm performance \u00b6 Every flow gets a dedicated page with all known information. The Analysis tab shows an automated interactive analysis of all collected results. For instance, below are the results of a scikit-learn pipeline including missing value imputation, feature encoding, and a RandomForest model. It shows the results across multiple tasks, and how the AUC score is affected by certain hyperparameters. This helps to better understand specific models, as well as their strengths and weaknesses. Automated sharing \u00b6 When you evaluate algorithms and share the results, OpenML will automatically extract all the details of the algorithm (dependencies, structure, and all hyperparameters), and upload them in the background. from sklearn import ensemble from openml import tasks , runs # Build any model you like. clf = ensemble . RandomForestClassifier () # Evaluate the model on a task run = runs . run_model_on_task ( clf , task ) # Share the results, including the flow and all its details. run . publish () Reproducing algorithms and experiments \u00b6 Given an OpenML run, the exact same algorithm or model, with exactly the same hyperparameters, can be reconstructed within the same machine learning library to easily reproduce earlier results. from openml import runs # Rebuild the (scikit-learn) pipeline from run 9864498 model = openml . runs . initialize_model_from_run ( 9864498 ) Note You may need the exact same library version to reconstruct flows. The API will always state the required version. We aim to add support for VMs so that flows can be easily (re)run in any environment Runs \u00b6 Automated reproducible evaluations \u00b6 Runs are experiments (benchmarks) evaluating a specific flows on a specific task. As shown above, they are typically submitted automatically by machine learning libraries through the OpenML APIs ), including lots of automatically extracted meta-data, to create reproducible experiments. With a few for-loops you can easily run (and share) millions of experiments. Online organization \u00b6 OpenML organizes all runs online, linked to the underlying data, flows, parameter settings, people, and other details. See the many examples above, where every dot in the scatterplots is a single OpenML run. Independent (server-side) evaluation \u00b6 OpenML runs include all information needed to independently evaluate models. For most tasks, this includes all predictions, for all train-test splits, for all instances in the dataset, including all class confidences. When a run is uploaded, OpenML automatically evaluates every run using a wide array of evaluation metrics. This makes them directly comparable with all other runs shared on OpenML. For completeness, OpenML will also upload locally computed evaluation metrics and runtimes. New metrics can also be added to OpenML's evaluation engine, and computed for all runs afterwards. Or, you can download OpenML runs and analyse the results any way you like. Note Please note that while OpenML tries to maximise reproducibility, exactly reproducing all results may not always be possible because of changes in numeric libraries, operating systems, and hardware. Collections and benchmarks \u00b6 You can combine tasks and runs into collections, to run experiments across many tasks at once and collect all results. Each collection gets its own page, which can be linked to publications so that others can find all the details online. Benchmarking suites \u00b6 Collections of tasks can be published as benchmarking suites . Seamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier: - all datasets are uniformly formatted in standardized data formats - they can be easily downloaded programmatically through APIs and client libraries - they come with machine-readable meta-information, such as the occurrence of missing values, to train algorithms correctly - standardized train-test splits are provided to ensure that results can be objectively compared - results can be shared in a reproducible way through the APIs - results from other users can be easily downloaded and reused You can search for all existing benchmarking suites or create your own. For all further details, see the benchmarking guide . Benchmark studies \u00b6 Collections of runs can be published as benchmarking studies . They contain the results of all runs (possibly millions) executed on a specific benchmarking suite. OpenML allows you to easily download all such results at once via the APIs, but also visualized them online in the Analysis tab (next to the complete list of included tasks and runs). Below is an example of a benchmark study for AutoML algorithms . Tagging \u00b6 Datasets, tasks, runs and flows can be assigned tags, either via the web interface or the API. These tags can be used to search and annotate datasets, or simply to better organize your own datasets and experiments. For example, the tag OpenML-CC18 refers to all tasks included in the OpenML-CC18 benchmarkign suite. Openness and Authentication \u00b6 You can download and inspect all datasets, tasks, flows and runs through the website or the API without creating an account. However, if you want to upload datasets or experiments, you need to create an account , sign in, and find your API key on your profile page. This key can then be used with any of the OpenML APIs . Sharing (under construction) \u00b6 Currently, anything on OpenML can be shared publicly or kept private to a single user. We are working on sharing features that allow you to share your materials with other users without making them entirely public. Watch this space","title":"Get started"},{"location":"#concepts","text":"OpenML operates on a number of core concepts which are important to understand: Datasets Datasets are pretty straight-forward. Tabular datasets are self-contained, consisting of a number of rows ( instances ) and columns (features), including their data types. Other modalities (e.g. images) are included via paths to files stored within the same folder. Datasets are uniformly formatted ( S3 buckets with Parquet tables, JSON metadata, and media files), and are auto-converted and auto-loaded in your desired format by the APIs (e.g. in Python ) in a single line of code. Example: The Iris dataset or the Plankton dataset Tasks A task consists of a dataset, together with a machine learning task to perform, such as classification or clustering and an evaluation method. For supervised tasks, this also specifies the target column in the data. Example: Classifying different iris species from other attributes and evaluate using 10-fold cross-validation. Flows A flow identifies a particular machine learning algorithm (a pipeline or untrained model) from a particular library or framework, such as scikit-learn, pyTorch, or MLR. It contains details about the structure of the model/pipeline, dependencies (e.g. the library and its version) and a list of settable hyperparameters. In short, it is a serialized description of the algorithm that in many cases can also be deserialized to reinstantiate the exact same algorithm in a particular library. Example: scikit-learn's RandomForest or a simple TensorFlow model Runs A run is an experiment - it evaluates a particular flow (pipeline/model) with particular hyperparameter settings, on a particular task. Depending on the task it will include certain results, such as model evaluations (e.g. accuracies), model predictions, and other output files (e.g. the trained model). Example: Classifying Gamma rays with scikit-learn's RandomForest","title":"Concepts"},{"location":"#data","text":"","title":"Data"},{"location":"#discovery","text":"OpenML allows fine-grained search over thousands of machine learning datasets. Via the website , you can filter by many dataset properties, such as size, type, format, and many more. Via the APIs you have access to many more filters, and you can download a complete table with statistics of all datasest. Via the APIs you can also load datasets directly into your preferred data structures such as numpy ( example in Python ). We are also working on better organization of all datasets by topic","title":"Discovery"},{"location":"#sharing","text":"You can upload and download datasets through the website or though the APIs (recommended). You can share data directly from common data science libraries, e.g. from Python or R dataframes, in a few lines of code. The OpenML APIs will automatically extract lots of meta-data and store all datasets in a uniform format. import pandas as pd import openml as oml # Create an OpenML dataset from a pandas dataframe df = pd . DataFrame ( data , columns = attribute_names ) my_data = oml . datasets . functions . create_dataset ( name = \"covertype\" , description = \"Predicting forest cover ...\" , licence = \"CC0\" , data = df ) # Share the dataset on OpenML my_data . publish () Every dataset gets a dedicated page on OpenML with all known information, and can be edited further online. Data hosted elsewhere can be referenced by URL. We are also working on interconnecting OpenML with other machine learning data set repositories","title":"Sharing"},{"location":"#automated-analysis","text":"OpenML will automatically analyze the data and compute a range of data quality characteristics . These include simple statistics such as the number of examples and features, but also potential quality issues (e.g. missing values) and more advanced statistics (e.g. the mutual information in the features and benchmark performances of simple models). These can be useful to find, filter and compare datasets, or to automate data preprocessing. We are also working on simple metrics and automated dataset quality reports The Analysis tab (see image below, or try it live ) also shows an automated and interactive analysis of all datasets. This runs on open-source Python code via Dash and we welcome all contributions The third tab, 'Tasks', lists all tasks created on the dataset. More on that below.","title":"Automated analysis"},{"location":"#dataset-id-and-versions","text":"A dataset can be uniquely identified by its dataset ID, which is shown on the website and returned by the API. It's 1596 in the covertype example above. They can also be referenced by name and ID. OpenML assigns incremental version numbers per upload with the same name. You can also add a free-form version_label with every upload.","title":"Dataset ID and versions"},{"location":"#dataset-status","text":"When you upload a dataset, it will be marked in_preparation until it is (automatically) verified. Once approved, the dataset will become active (or verified ). If a severe issue has been found with a dataset, it can become deactivated (or deprecated ) signaling that it should not be used. By default, dataset search only returns verified datasets, but you can access and download datasets with any status.","title":"Dataset status"},{"location":"#special-attributes","text":"Machine learning datasets often have special attributes that require special handling in order to build useful models. OpenML marks these as special attributes. A target attribute is the column that is to be predicted, also known as dependent variable. Datasets can have a default target attribute set by the author, but OpenML tasks can also overrule this. Example: The default target variable for the MNIST dataset is to predict the class from pixel values, and most supervised tasks will have the class as their target. However, one can also create a task aimed at predicting the value of pixel257 given all the other pixel values and the class column. Row id attributes indicate (externally defined) row IDs. Ignore attributes are other columns that should not be included in training data. OpenML will clearly mark these, and will (by default) drop these columns when constructing training sets.","title":"Special attributes"},{"location":"#tasks","text":"Tasks describe what to do with the data. OpenML covers several task types , such as classification and clustering. Tasks are containers including the data and other information such as train/test splits, and define what needs to be returned. They are machine-readable so that you can automate machine learning experiments, and easily compare algorithms evaluations (using the exact same train-test splits) against all other benchmarks shared by others on OpenML.","title":"Tasks"},{"location":"#collaborative-benchmarks","text":"Tasks are real-time, collaborative benchmarks (e.g. see MNIST below). In the Analysis tab, you can view timelines and leaderboards, and learn from all prior submissions to design even better algorithms.","title":"Collaborative benchmarks"},{"location":"#discover-the-best-algorithms","text":"All algorithms evaluated on the same task (with the same train-test splits) can be directly compared to each other, so you can easily look up which algorithms perform best overall, and download their exact configurations. Likewise, you can look up the best algorithms for similar tasks to know what to try first.","title":"Discover the best algorithms"},{"location":"#automating-benchmarks","text":"You can search and download existing tasks, evaluate your algorithms, and automatically share the results (which are stored in a run ). Here's what this looks like in the Python API. You can do the same across hundreds of tasks at once. from sklearn import ensemble from openml import tasks , runs # Build any model you like clf = ensemble . RandomForestClassifier () # Download any OpenML task (includes the datasets) task = tasks . get_task ( 3954 ) # Automatically evaluate your model on the task run = runs . run_model_on_task ( clf , task ) # Share the results on OpenML. run . publish () You can create new tasks via the website or via the APIs as well.","title":"Automating benchmarks"},{"location":"#flows","text":"Flows are machine learning pipelines, models, or scripts. They are typically uploaded directly from machine learning libraries (e.g. scikit-learn, pyTorch, TensorFlow, MLR, WEKA,...) via the corresponding APIs . Associated code (e.g., on GitHub) can be referenced by URL.","title":"Flows"},{"location":"#analysing-algorithm-performance","text":"Every flow gets a dedicated page with all known information. The Analysis tab shows an automated interactive analysis of all collected results. For instance, below are the results of a scikit-learn pipeline including missing value imputation, feature encoding, and a RandomForest model. It shows the results across multiple tasks, and how the AUC score is affected by certain hyperparameters. This helps to better understand specific models, as well as their strengths and weaknesses.","title":"Analysing algorithm performance"},{"location":"#automated-sharing","text":"When you evaluate algorithms and share the results, OpenML will automatically extract all the details of the algorithm (dependencies, structure, and all hyperparameters), and upload them in the background. from sklearn import ensemble from openml import tasks , runs # Build any model you like. clf = ensemble . RandomForestClassifier () # Evaluate the model on a task run = runs . run_model_on_task ( clf , task ) # Share the results, including the flow and all its details. run . publish ()","title":"Automated sharing"},{"location":"#reproducing-algorithms-and-experiments","text":"Given an OpenML run, the exact same algorithm or model, with exactly the same hyperparameters, can be reconstructed within the same machine learning library to easily reproduce earlier results. from openml import runs # Rebuild the (scikit-learn) pipeline from run 9864498 model = openml . runs . initialize_model_from_run ( 9864498 ) Note You may need the exact same library version to reconstruct flows. The API will always state the required version. We aim to add support for VMs so that flows can be easily (re)run in any environment","title":"Reproducing algorithms and experiments"},{"location":"#runs","text":"","title":"Runs"},{"location":"#automated-reproducible-evaluations","text":"Runs are experiments (benchmarks) evaluating a specific flows on a specific task. As shown above, they are typically submitted automatically by machine learning libraries through the OpenML APIs ), including lots of automatically extracted meta-data, to create reproducible experiments. With a few for-loops you can easily run (and share) millions of experiments.","title":"Automated reproducible evaluations"},{"location":"#online-organization","text":"OpenML organizes all runs online, linked to the underlying data, flows, parameter settings, people, and other details. See the many examples above, where every dot in the scatterplots is a single OpenML run.","title":"Online organization"},{"location":"#independent-server-side-evaluation","text":"OpenML runs include all information needed to independently evaluate models. For most tasks, this includes all predictions, for all train-test splits, for all instances in the dataset, including all class confidences. When a run is uploaded, OpenML automatically evaluates every run using a wide array of evaluation metrics. This makes them directly comparable with all other runs shared on OpenML. For completeness, OpenML will also upload locally computed evaluation metrics and runtimes. New metrics can also be added to OpenML's evaluation engine, and computed for all runs afterwards. Or, you can download OpenML runs and analyse the results any way you like. Note Please note that while OpenML tries to maximise reproducibility, exactly reproducing all results may not always be possible because of changes in numeric libraries, operating systems, and hardware.","title":"Independent (server-side) evaluation"},{"location":"#collections-and-benchmarks","text":"You can combine tasks and runs into collections, to run experiments across many tasks at once and collect all results. Each collection gets its own page, which can be linked to publications so that others can find all the details online.","title":"Collections and benchmarks"},{"location":"#benchmarking-suites","text":"Collections of tasks can be published as benchmarking suites . Seamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier: - all datasets are uniformly formatted in standardized data formats - they can be easily downloaded programmatically through APIs and client libraries - they come with machine-readable meta-information, such as the occurrence of missing values, to train algorithms correctly - standardized train-test splits are provided to ensure that results can be objectively compared - results can be shared in a reproducible way through the APIs - results from other users can be easily downloaded and reused You can search for all existing benchmarking suites or create your own. For all further details, see the benchmarking guide .","title":"Benchmarking suites"},{"location":"#benchmark-studies","text":"Collections of runs can be published as benchmarking studies . They contain the results of all runs (possibly millions) executed on a specific benchmarking suite. OpenML allows you to easily download all such results at once via the APIs, but also visualized them online in the Analysis tab (next to the complete list of included tasks and runs). Below is an example of a benchmark study for AutoML algorithms .","title":"Benchmark studies"},{"location":"#tagging","text":"Datasets, tasks, runs and flows can be assigned tags, either via the web interface or the API. These tags can be used to search and annotate datasets, or simply to better organize your own datasets and experiments. For example, the tag OpenML-CC18 refers to all tasks included in the OpenML-CC18 benchmarkign suite.","title":"Tagging"},{"location":"#openness-and-authentication","text":"You can download and inspect all datasets, tasks, flows and runs through the website or the API without creating an account. However, if you want to upload datasets or experiments, you need to create an account , sign in, and find your API key on your profile page. This key can then be used with any of the OpenML APIs .","title":"Openness and Authentication"},{"location":"#sharing-under-construction","text":"Currently, anything on OpenML can be shared publicly or kept private to a single user. We are working on sharing features that allow you to share your materials with other users without making them entirely public. Watch this space","title":"Sharing (under construction)"},{"location":"API-development/","text":"Golden Rules for Development \u00b6 Code Maintainability before anything else . The code has to be understandable, and if not conflicting with that, short. Avoid code duplications as much as possible. The API controller is the only entity giving access to the API models. Therefore, the responsibility for API access can be handled by the controller Read-Only operations are of the type GET. Operations that make changes in the database are of type POST or DELETE. Important, because this is the way the controller determines to allow users with a given set of privileges to access functions. Try to avoid direct queries to the database. Instead, use the respective models functions: 'get()', 'getWhere()', 'getById()', insert(), etc (Please make yourself familiar with the basic model: read-only and write ) No external program/script execution during API calls (with one exception: data split generation). This makes the API unnecessarily slow, hard to debug and vulnerable to crashes. If necessary, make a cronjob that executes the program / script Important resources \u00b6 API docs: www.openml.org/api_docs Controller: https://github.com/openml/OpenML/blob/master/openml_OS/controllers/Api_new.php Models: https://github.com/openml/OpenML/tree/master/openml_OS/models/api/v1 Templates: https://github.com/openml/OpenML/tree/master/openml_OS/views/pages/api_new/v1 Backend code structure \u00b6 The high-level architecture of the website, including the controllers for different parts of the website (REST API, html, ...) and connections to the database. Code \u00b6 The source code is available in the 'website' repository: https://github.com/openml/website Important files and folders \u00b6 In this section we go through all important files and folder of the system. Root directory \u00b6 The root directory of OpenML contains the following files and folders. system : This folder contains all files provided by CodeIgniter 2.1.3. The contents of this folder is beyond the scope of this document, and not relevant for extending OpenML. All the files in this folder are in the same state as they were provided by Ellislabs, and none of these files should ever be changed. sparks : Sparks is a package management system for Codeigniter that allows for instant installation of libraries into the application. This folder contains two libraries provided by third party software developers, oauth1 (based on version 1 the oauth protocol) and oauth2 (similarly, based on version 2 of the oauth protocol). The exact contents of this folder is beyond the scope of this document and not relevant for extending OpenML. openml_OS : All files in this folder are written specifically for OpenML. When extending the functionality OpenML, usually one of the files in this folder needs to be adjusted. As a thorough understanding of the contents of this folder is vital for extending OpenML, we will discuss the contents of this folder in [[URL Mapping]] in more detail. index.php : This is the \u201cbootstrap\u201d file of the system. Basically, every page request on OpenML goes through this file (with the css, images and javascript files as only exception). It then determines which CodeIgniter and OpenML files need to be included. This file should not be edited. .htaccess : This file (which configures the Apache Rewrite Engine) makes sure that all URL requests will be directed to index.php . Without this file, we would need to include index.php explicitly in every URL request. This file makes sure that all other URL requests without index.php embedded in it automatically will be transformed to index.php . Eg., http://www.openml.org/frontend/page/home will be rewritten to http://www.openml.org/index.php/frontend/page/home . This will be explained in detail in [[URL Mapping]]. css : A folder containing all stylesheets. These are important for the layout of OpenML. data : A folder containing data files, e.g., datasets, implementation files, uploaded content. Please note that this folder does not necessarily needs to be present in the root directory. The OpenML Base Config file determines the exact location of this folder. downloads : Another data folder, containing files like the most recent database snapshot. img : A folder containing all static images shown on the webpage. js : A folder containing all used Javascript files and libraries, including third party libraries like jQuery and datatables. Various other files, like .gitignore, favicon.ico, etc. openml_OS \u00b6 This folder is (in CodeIgniter jargon) the \u201cApplication folder\u201d, and contains all files relevant to OpenML. Within this folder, the following folders should be present: (And also some other folders, but these are not used by OpenML) config : A folder containing all config files. Most notably, it contains the file BASE_CONFIG.php , in which all system specific variables are set; the config items within this file differs over various installations (e.g., on localhost, openml.org ). Most other config files, like database.php , will receive their values from BASE_CONFIG.php . Other important config files are autoload.php , determining which CodeIgniter / OpenML files will be loaded on any request, openML.php , containing config items specific to OpenML, and routes.php , which will be explained in [[URL Mapping]]. controllers : In the Model/View/Controller design pattern, all user interaction goes through controllers. In a webapplication setting this means that every time a URL gets requested, exactly one controller gets invoked. The exact dynamics of this will be explained in [[URL Mapping]]. core : A folder that contains CodeIgniter specific files. These are not relevant for the understanding of OpenML. helpers : This folder contains many convenience functions. Wikipedia states: \u201cA convenience function is a non-essential subroutine in a programming library or framework which is intended to ease commonly performed tasks\u201d. For example the file_upload_helper.php contains many functions that assist with uploading of files. Please note that a helper function must be explicitly loaded in either the autoload config or the files that uses its functions. libraries : Similar to sparks, this folder contains libraries specifically written for CodeIgniter. For example, the library used for all user management routines is in this folder. models : In the Model/View/Controller design pattern, models represent the state of the system. In a webapplication setting, you could say that a model is the link to the database. In OpenML, almost all tables of the database are represented by a model. Each model has general functionality applicable to all models (e.g., retrieve all records, retrieve record with constraints, insert record) and functionality specific to that model (e.g., retrieve a dataset that has certain data properties). Most models extend an (abstract) base class, located in the abstract folder. This way, all general functionality is programmed and maintained in one place. third_party : Although the name might suggests differently, this folder contains all OpenML Java libraries. views : In the Model/View/Controller design pattern, the views are the way information is presented on the screen. In a webapplication setting, a view usually is a block of (PHP generated) HTML code. The most notable view is frontend_main.php , which is the template file determining the main look and feel of OpenML. Every single page also has its own specific view (which is parsed within frontend_main.php ). These pages can be found (categorized by controller and name) in the pages folder. More about this structure is explained in [[URL Mapping]]. Frontend code structure \u00b6 Architecture and libraries involved in generating the frontend functions. Code: https://github.com/openml/website/tree/master/openml_OS/views High-level \u00b6 All pages are generated by first loading frontend_main.php . This creates the 'shell' in which the content is loaded. It loads all css and javascript libraries, and contains the html for displaying headers and footers. Create new page \u00b6 The preferred method is creating a new folder into the folder <root_directory>/openml_OS/views/pages/frontend This page can be requested by http://www.openml.org/frontend/page/<folder_name> or just http://www.openml.org/<folder_name> This method is preferred for human readable webpages, where the internal actions are simple, and the output is complex. We will describe the files that can be in this folder. pre.php : Mandatory file. Will be executed first. Do not make this file produce any output! Can be used to pre-render data, or set some variables that are used in other files. body.php : Highly recommended file. Intended for displaying the main content of this file. Will be rendered at the right location within the template file ( frontend_main.php ). javascript.php : Non-mandatory file. Intended for javascript function on which body.php relies. Will be rendered within a javascript block in the header of the page. post.php : Non mandatory file. Will only be executed when a POST request is done (e.g., when a HTML form was send using the POST protocol). Will be executed after pre.php , but before the rendering process (and thus, before body.php and javascript.php ). Should handle the posted input, e.g., file uploads. It is also recommended to add the newly created folder to the mapping in the routes.php config file. This way it can also be requested by the shortened version of the URL. (Note that we deliberately avoided to auto-load all pages into this file using a directory scan, as this makes the webplatform slow. ) URL to Page Mapping \u00b6 Most pages in OpenML are represented by a folder in /openml_OS/views/pages/frontend The contents of this folder will be parsed in the template frontend_main.php template, as described in [[backend]]. In this section we explain the way an URL is mapped to a certain OpenML page. URL Anatomy \u00b6 By default, CodeIgniter (and OpenML) accepts a URL in the following form: http://www.openml.org/index.php/<controller>/<function>/<p1>/<pN>/<free> The various parts in the URL are divided by slashes. Every URL starts with the protocol and server name (in the case of OpenML this is http://www.openml.org/ ). This is followed by the bootstrap file, which is always the same, i.e., index.php . The next part indicates the controller that needs to be invoked; typically this is frontend , rest_api or data , but it can be any file from the openml_OS folder controllers . Note that the suffix .php should not be included in the URL. The next part indicates which function of the controller should be invoked. This should be a existing, public function from the controller that is indicated in the controller part. These functions might have one or more parameters that need to be set. This is the following part of the URL (indicated by p1 and pN ). The parameters can be followed by anything in free format. Typically, this free format is used to pass on additional parameters in name - value format, or just a way of adding a human readable string to the URL for SEO purposes. For example, the following URL http://www.openml.org/index.php/frontend/page/home invokes the function page from the frontend controller and sets the only parameter of this function, $indicator , to value home . The function page loads the content of the specified folder ( $indicator ) into the main template. In this sense, the function page can be seen as some sort of specialized page loader. URL Shortening \u00b6 Since it is good practice to have URL\u2019s as short as possible, we have introduced some logic that shortens the URL\u2019s. Most importantly, the URL part that invokes index.php can be removed at no cost, since this file is always invoked. For this, we use Apache\u2019s rewrite engine. Rules for rewriting URL\u2019s can be found in the .htaccess file, but is suffices to say that any URL in the following format http://www.openml.org/index.php/<controller>/<function>/<params> can due to the rewrite engine also be requested with http://www.openml.org/<controller>/<function>/<params> Furthermore, since most of the pages are invoked by the function page of the frontend controller (hence, they come with the suffix frontend/page/page_name ) we also created a mapping that maps URL\u2019s in the following form http://www.openml.org/<page_name> to http://www.openml.org/frontend/page/<page_name> Note that Apache\u2019s rewrite engine will also add index.php to this. The exact mapping can be found in routes.php config file. Additional Mappings \u00b6 Additionally, a mapping is created from the following type of URL: http://www.openml.org/api/<any_query_string> to http://www.openml.org/rest_api/<any_query_string> This was done for backwards compatibility. Many plugins make calls to the not-existing api controller, which are automatically redirected to the rest_api controller. Exceptions \u00b6 It is important to note that not all pages do have a specific page folder. The page folders are a good way of structuring complex GUI\u2019s that need to be presented to the user, but in cases where the internal state changes are more important than the GUI\u2019s, it might be preferable to make the controller function print the output directly. This happens for example in the functions of rest_api.php and free_query.php (although the former still has some files in the views folder that it refers to). XSD Schemas \u00b6 In order to ensure data integrity on the server, data that passed to upload functions is checked against XSD schema's. This ensures that the data that is uploaded is in the correct format, and does not contain any illegal characters. XSD schema's can be obtained through the API (exact links are provided in the API docs, but for example: https://www.openml.org/api/v1/xsd/openml.data.upload (where openml.data.upload can be replaced by any other schema's name). Also XML examples are provided, e.g., https://www.openml.org/api/v1/xml_example/data . The XSD schema's are exactly the same as used on the server. Whenever an upload fails and the server mentions an XML/XSD verification error, please run the uploaded xml against one of the provided XSD schema's, for example on this webtool: http://www.freeformatter.com/xml-validator-xsd.html In order to maintain one XSD schema for both uploading and downloading stuff, the XSD sometimes contains more fields than seem necessary from the offset. Usually, the additional fields that are indicated as such in the comments (for example, in the upload dataset xsd this are the id, upload_date, etc fields). The XSD's maintain basically three consistencies functions: Ensure that the correct fields are uploaded Ensure that the fields contain the correct data types. Ensure that the fields do not contain to much characters for the database to upload. For the latter two, it is important to note that the XSD seldom accept default string content (i.e., xs:string). Rather, we use self defined data types, that use regular expressions to ensure the right content. Examples of these are oml:system_string128, oml:casual_string128, oml:basic_latin128, where the oml prefix is used, the name indicates the level of restriction and the number indicates the maximum size of the field. IMPORTANT: The maximum field sizes are (often) chosen with great care. Do not extend them without consulting other team members. User authentication \u00b6 Authentication towards the server goes by means of a so-called api_key (a hexa-decimal string which uniquely identifies a user). Upon interaction with the server, the client passes this api_key to the server, and the server checks the rights of the user. Currently this goes by means of a get or post variable, but in the future we might want to use a header field (because of security). It is recommended to refresh your api_key every month. IMPORTANT: Most authentication operations are handled by the ION_Auth library ( http://benedmunds.com/ion_auth/ ). DO NOT alter information directly in the user table, always use the ION_Auth API. A user can be part of one or many groups. The following user groups exists: Admin Group: With great power comes great responsibility. Admin users can overrule all security checks on the server, i.e., delete a dataset or run that is not theirs, or even delete a flow that contains runs. Normal Group: Level that is required for read/write interaction with the server. Almost all users are part of this group. Read-only Group: Level that can be used for read interaction with the server. If a user is part of this group, but not part of 'Normal Group', he is allowed to download content, but can not upload or delete content. Backend Group: (Work in Progress) Level that has more privileges than 'Normal Group'. Can submit Data Qualities and Evaluations. The ION_Auth functions in_group(), add_to_group(), remove_from_group() and get_users_groups() are key towards interaction with these tables.","title":"API Development"},{"location":"API-development/#golden-rules-for-development","text":"Code Maintainability before anything else . The code has to be understandable, and if not conflicting with that, short. Avoid code duplications as much as possible. The API controller is the only entity giving access to the API models. Therefore, the responsibility for API access can be handled by the controller Read-Only operations are of the type GET. Operations that make changes in the database are of type POST or DELETE. Important, because this is the way the controller determines to allow users with a given set of privileges to access functions. Try to avoid direct queries to the database. Instead, use the respective models functions: 'get()', 'getWhere()', 'getById()', insert(), etc (Please make yourself familiar with the basic model: read-only and write ) No external program/script execution during API calls (with one exception: data split generation). This makes the API unnecessarily slow, hard to debug and vulnerable to crashes. If necessary, make a cronjob that executes the program / script","title":"Golden Rules for Development"},{"location":"API-development/#important-resources","text":"API docs: www.openml.org/api_docs Controller: https://github.com/openml/OpenML/blob/master/openml_OS/controllers/Api_new.php Models: https://github.com/openml/OpenML/tree/master/openml_OS/models/api/v1 Templates: https://github.com/openml/OpenML/tree/master/openml_OS/views/pages/api_new/v1","title":"Important resources"},{"location":"API-development/#backend-code-structure","text":"The high-level architecture of the website, including the controllers for different parts of the website (REST API, html, ...) and connections to the database.","title":"Backend code structure"},{"location":"API-development/#code","text":"The source code is available in the 'website' repository: https://github.com/openml/website","title":"Code"},{"location":"API-development/#important-files-and-folders","text":"In this section we go through all important files and folder of the system.","title":"Important files and folders"},{"location":"API-development/#root-directory","text":"The root directory of OpenML contains the following files and folders. system : This folder contains all files provided by CodeIgniter 2.1.3. The contents of this folder is beyond the scope of this document, and not relevant for extending OpenML. All the files in this folder are in the same state as they were provided by Ellislabs, and none of these files should ever be changed. sparks : Sparks is a package management system for Codeigniter that allows for instant installation of libraries into the application. This folder contains two libraries provided by third party software developers, oauth1 (based on version 1 the oauth protocol) and oauth2 (similarly, based on version 2 of the oauth protocol). The exact contents of this folder is beyond the scope of this document and not relevant for extending OpenML. openml_OS : All files in this folder are written specifically for OpenML. When extending the functionality OpenML, usually one of the files in this folder needs to be adjusted. As a thorough understanding of the contents of this folder is vital for extending OpenML, we will discuss the contents of this folder in [[URL Mapping]] in more detail. index.php : This is the \u201cbootstrap\u201d file of the system. Basically, every page request on OpenML goes through this file (with the css, images and javascript files as only exception). It then determines which CodeIgniter and OpenML files need to be included. This file should not be edited. .htaccess : This file (which configures the Apache Rewrite Engine) makes sure that all URL requests will be directed to index.php . Without this file, we would need to include index.php explicitly in every URL request. This file makes sure that all other URL requests without index.php embedded in it automatically will be transformed to index.php . Eg., http://www.openml.org/frontend/page/home will be rewritten to http://www.openml.org/index.php/frontend/page/home . This will be explained in detail in [[URL Mapping]]. css : A folder containing all stylesheets. These are important for the layout of OpenML. data : A folder containing data files, e.g., datasets, implementation files, uploaded content. Please note that this folder does not necessarily needs to be present in the root directory. The OpenML Base Config file determines the exact location of this folder. downloads : Another data folder, containing files like the most recent database snapshot. img : A folder containing all static images shown on the webpage. js : A folder containing all used Javascript files and libraries, including third party libraries like jQuery and datatables. Various other files, like .gitignore, favicon.ico, etc.","title":"Root directory"},{"location":"API-development/#openml_os","text":"This folder is (in CodeIgniter jargon) the \u201cApplication folder\u201d, and contains all files relevant to OpenML. Within this folder, the following folders should be present: (And also some other folders, but these are not used by OpenML) config : A folder containing all config files. Most notably, it contains the file BASE_CONFIG.php , in which all system specific variables are set; the config items within this file differs over various installations (e.g., on localhost, openml.org ). Most other config files, like database.php , will receive their values from BASE_CONFIG.php . Other important config files are autoload.php , determining which CodeIgniter / OpenML files will be loaded on any request, openML.php , containing config items specific to OpenML, and routes.php , which will be explained in [[URL Mapping]]. controllers : In the Model/View/Controller design pattern, all user interaction goes through controllers. In a webapplication setting this means that every time a URL gets requested, exactly one controller gets invoked. The exact dynamics of this will be explained in [[URL Mapping]]. core : A folder that contains CodeIgniter specific files. These are not relevant for the understanding of OpenML. helpers : This folder contains many convenience functions. Wikipedia states: \u201cA convenience function is a non-essential subroutine in a programming library or framework which is intended to ease commonly performed tasks\u201d. For example the file_upload_helper.php contains many functions that assist with uploading of files. Please note that a helper function must be explicitly loaded in either the autoload config or the files that uses its functions. libraries : Similar to sparks, this folder contains libraries specifically written for CodeIgniter. For example, the library used for all user management routines is in this folder. models : In the Model/View/Controller design pattern, models represent the state of the system. In a webapplication setting, you could say that a model is the link to the database. In OpenML, almost all tables of the database are represented by a model. Each model has general functionality applicable to all models (e.g., retrieve all records, retrieve record with constraints, insert record) and functionality specific to that model (e.g., retrieve a dataset that has certain data properties). Most models extend an (abstract) base class, located in the abstract folder. This way, all general functionality is programmed and maintained in one place. third_party : Although the name might suggests differently, this folder contains all OpenML Java libraries. views : In the Model/View/Controller design pattern, the views are the way information is presented on the screen. In a webapplication setting, a view usually is a block of (PHP generated) HTML code. The most notable view is frontend_main.php , which is the template file determining the main look and feel of OpenML. Every single page also has its own specific view (which is parsed within frontend_main.php ). These pages can be found (categorized by controller and name) in the pages folder. More about this structure is explained in [[URL Mapping]].","title":"openml_OS"},{"location":"API-development/#frontend-code-structure","text":"Architecture and libraries involved in generating the frontend functions. Code: https://github.com/openml/website/tree/master/openml_OS/views","title":"Frontend code structure"},{"location":"API-development/#high-level","text":"All pages are generated by first loading frontend_main.php . This creates the 'shell' in which the content is loaded. It loads all css and javascript libraries, and contains the html for displaying headers and footers.","title":"High-level"},{"location":"API-development/#create-new-page","text":"The preferred method is creating a new folder into the folder <root_directory>/openml_OS/views/pages/frontend This page can be requested by http://www.openml.org/frontend/page/<folder_name> or just http://www.openml.org/<folder_name> This method is preferred for human readable webpages, where the internal actions are simple, and the output is complex. We will describe the files that can be in this folder. pre.php : Mandatory file. Will be executed first. Do not make this file produce any output! Can be used to pre-render data, or set some variables that are used in other files. body.php : Highly recommended file. Intended for displaying the main content of this file. Will be rendered at the right location within the template file ( frontend_main.php ). javascript.php : Non-mandatory file. Intended for javascript function on which body.php relies. Will be rendered within a javascript block in the header of the page. post.php : Non mandatory file. Will only be executed when a POST request is done (e.g., when a HTML form was send using the POST protocol). Will be executed after pre.php , but before the rendering process (and thus, before body.php and javascript.php ). Should handle the posted input, e.g., file uploads. It is also recommended to add the newly created folder to the mapping in the routes.php config file. This way it can also be requested by the shortened version of the URL. (Note that we deliberately avoided to auto-load all pages into this file using a directory scan, as this makes the webplatform slow. )","title":"Create new page"},{"location":"API-development/#url-to-page-mapping","text":"Most pages in OpenML are represented by a folder in /openml_OS/views/pages/frontend The contents of this folder will be parsed in the template frontend_main.php template, as described in [[backend]]. In this section we explain the way an URL is mapped to a certain OpenML page.","title":"URL to Page Mapping"},{"location":"API-development/#url-anatomy","text":"By default, CodeIgniter (and OpenML) accepts a URL in the following form: http://www.openml.org/index.php/<controller>/<function>/<p1>/<pN>/<free> The various parts in the URL are divided by slashes. Every URL starts with the protocol and server name (in the case of OpenML this is http://www.openml.org/ ). This is followed by the bootstrap file, which is always the same, i.e., index.php . The next part indicates the controller that needs to be invoked; typically this is frontend , rest_api or data , but it can be any file from the openml_OS folder controllers . Note that the suffix .php should not be included in the URL. The next part indicates which function of the controller should be invoked. This should be a existing, public function from the controller that is indicated in the controller part. These functions might have one or more parameters that need to be set. This is the following part of the URL (indicated by p1 and pN ). The parameters can be followed by anything in free format. Typically, this free format is used to pass on additional parameters in name - value format, or just a way of adding a human readable string to the URL for SEO purposes. For example, the following URL http://www.openml.org/index.php/frontend/page/home invokes the function page from the frontend controller and sets the only parameter of this function, $indicator , to value home . The function page loads the content of the specified folder ( $indicator ) into the main template. In this sense, the function page can be seen as some sort of specialized page loader.","title":"URL Anatomy"},{"location":"API-development/#url-shortening","text":"Since it is good practice to have URL\u2019s as short as possible, we have introduced some logic that shortens the URL\u2019s. Most importantly, the URL part that invokes index.php can be removed at no cost, since this file is always invoked. For this, we use Apache\u2019s rewrite engine. Rules for rewriting URL\u2019s can be found in the .htaccess file, but is suffices to say that any URL in the following format http://www.openml.org/index.php/<controller>/<function>/<params> can due to the rewrite engine also be requested with http://www.openml.org/<controller>/<function>/<params> Furthermore, since most of the pages are invoked by the function page of the frontend controller (hence, they come with the suffix frontend/page/page_name ) we also created a mapping that maps URL\u2019s in the following form http://www.openml.org/<page_name> to http://www.openml.org/frontend/page/<page_name> Note that Apache\u2019s rewrite engine will also add index.php to this. The exact mapping can be found in routes.php config file.","title":"URL Shortening"},{"location":"API-development/#additional-mappings","text":"Additionally, a mapping is created from the following type of URL: http://www.openml.org/api/<any_query_string> to http://www.openml.org/rest_api/<any_query_string> This was done for backwards compatibility. Many plugins make calls to the not-existing api controller, which are automatically redirected to the rest_api controller.","title":"Additional Mappings"},{"location":"API-development/#exceptions","text":"It is important to note that not all pages do have a specific page folder. The page folders are a good way of structuring complex GUI\u2019s that need to be presented to the user, but in cases where the internal state changes are more important than the GUI\u2019s, it might be preferable to make the controller function print the output directly. This happens for example in the functions of rest_api.php and free_query.php (although the former still has some files in the views folder that it refers to).","title":"Exceptions"},{"location":"API-development/#xsd-schemas","text":"In order to ensure data integrity on the server, data that passed to upload functions is checked against XSD schema's. This ensures that the data that is uploaded is in the correct format, and does not contain any illegal characters. XSD schema's can be obtained through the API (exact links are provided in the API docs, but for example: https://www.openml.org/api/v1/xsd/openml.data.upload (where openml.data.upload can be replaced by any other schema's name). Also XML examples are provided, e.g., https://www.openml.org/api/v1/xml_example/data . The XSD schema's are exactly the same as used on the server. Whenever an upload fails and the server mentions an XML/XSD verification error, please run the uploaded xml against one of the provided XSD schema's, for example on this webtool: http://www.freeformatter.com/xml-validator-xsd.html In order to maintain one XSD schema for both uploading and downloading stuff, the XSD sometimes contains more fields than seem necessary from the offset. Usually, the additional fields that are indicated as such in the comments (for example, in the upload dataset xsd this are the id, upload_date, etc fields). The XSD's maintain basically three consistencies functions: Ensure that the correct fields are uploaded Ensure that the fields contain the correct data types. Ensure that the fields do not contain to much characters for the database to upload. For the latter two, it is important to note that the XSD seldom accept default string content (i.e., xs:string). Rather, we use self defined data types, that use regular expressions to ensure the right content. Examples of these are oml:system_string128, oml:casual_string128, oml:basic_latin128, where the oml prefix is used, the name indicates the level of restriction and the number indicates the maximum size of the field. IMPORTANT: The maximum field sizes are (often) chosen with great care. Do not extend them without consulting other team members.","title":"XSD Schemas"},{"location":"API-development/#user-authentication","text":"Authentication towards the server goes by means of a so-called api_key (a hexa-decimal string which uniquely identifies a user). Upon interaction with the server, the client passes this api_key to the server, and the server checks the rights of the user. Currently this goes by means of a get or post variable, but in the future we might want to use a header field (because of security). It is recommended to refresh your api_key every month. IMPORTANT: Most authentication operations are handled by the ION_Auth library ( http://benedmunds.com/ion_auth/ ). DO NOT alter information directly in the user table, always use the ION_Auth API. A user can be part of one or many groups. The following user groups exists: Admin Group: With great power comes great responsibility. Admin users can overrule all security checks on the server, i.e., delete a dataset or run that is not theirs, or even delete a flow that contains runs. Normal Group: Level that is required for read/write interaction with the server. Almost all users are part of this group. Read-only Group: Level that can be used for read interaction with the server. If a user is part of this group, but not part of 'Normal Group', he is allowed to download content, but can not upload or delete content. Backend Group: (Work in Progress) Level that has more privileges than 'Normal Group'. Can submit Data Qualities and Evaluations. The ION_Auth functions in_group(), add_to_group(), remove_from_group() and get_users_groups() are key towards interaction with these tables.","title":"User authentication"},{"location":"APIs/","text":"OpenML offers a range of APIs to download and upload OpenML datasets, tasks, run algorithms on them, and share the results. REST \u00b6 The REST API allows you to talk directly to the OpenML server from any programming environment. REST Tutorial REST API Reference Python \u00b6 Download datasets into Python scripts, build models using Python machine learning libraries (e.g., scikit-learn ), and share the results online, all in a few lines of code. User Guide API Reference OpenML-Python Tutorial Cheatsheet R \u00b6 Download datasets into R scripts, build models using R machine learning packages (e.g. mlr ), and share the results online, again in a few lines of code. R Tutorial R API Reference Cheatsheet useR 2017 Tutorial Java \u00b6 If you are building machine learning systems in Java, there is also an API for that. Java Tutorial Java API Reference .NET (C#) \u00b6 The .NET library is under development, but already contains most of the functions available. .NET Tutorial GitHub repo Easy authentication \u00b6 In the interest of open science, we allow you to freely download all public resources, also through the APIs (rate limits apply when necessary). Uploading and sharing new datasets, tasks, flows and runs (or accessing any shared/private resources) is also very easy, and requires only the API key that you can find in your profile (after logging in). If you use any of the language-specific APIs, you only need to store this key in a config file and forget about it. For authenticating to the REST API, you can send your api key using Basic Auth, or by adding ?api_key='your key' to your calls. If you are logged into OpenML.org, this will be done automatically.","title":"APIs"},{"location":"APIs/#rest","text":"The REST API allows you to talk directly to the OpenML server from any programming environment. REST Tutorial REST API Reference","title":"REST"},{"location":"APIs/#python","text":"Download datasets into Python scripts, build models using Python machine learning libraries (e.g., scikit-learn ), and share the results online, all in a few lines of code. User Guide API Reference OpenML-Python Tutorial Cheatsheet","title":"Python"},{"location":"APIs/#r","text":"Download datasets into R scripts, build models using R machine learning packages (e.g. mlr ), and share the results online, again in a few lines of code. R Tutorial R API Reference Cheatsheet useR 2017 Tutorial","title":"R"},{"location":"APIs/#java","text":"If you are building machine learning systems in Java, there is also an API for that. Java Tutorial Java API Reference","title":"Java"},{"location":"APIs/#net-c","text":"The .NET library is under development, but already contains most of the functions available. .NET Tutorial GitHub repo","title":".NET (C#)"},{"location":"APIs/#easy-authentication","text":"In the interest of open science, we allow you to freely download all public resources, also through the APIs (rate limits apply when necessary). Uploading and sharing new datasets, tasks, flows and runs (or accessing any shared/private resources) is also very easy, and requires only the API key that you can find in your profile (after logging in). If you use any of the language-specific APIs, you only need to store this key in a config file and forget about it. For authenticating to the REST API, you can send your api key using Basic Auth, or by adding ?api_key='your key' to your calls. If you are logged into OpenML.org, this will be done automatically.","title":"Easy authentication"},{"location":"Basic-Concepts/","text":"Basic Concepts \u00b6 Researchers are encouraged to upload their experimental results on OpenML, so that these can be reused by anyone. Various high level papers have been published that overview the design goals, benefits and opportunities (for example, at ECML/PKDD 2013 , SIGKDD Explorations and JMLR ). However, there is no clear overview of the basic concepts upon which the platform is build. In this blog post I will review these, and discuss some best practices. This page is a slightly updated version of this blogpost Data \u00b6 One of the core components of OpenML are datasets. People can upload their datasets, and the system automatically organises these on line. An example of a dataset is the well-known Iris dataset . It shows all features, once of these is identified as the 'default target attribute', although this concept is flexible. It also shows some automatically computed data qualities (or, meta-features). Each dataset has its own unique ID. Information about the dataset, the data features and the data qualities can be obtained automatically by means of the following API functions: Get all available datasets Get dataset (required the data id) Get data features (requires the data id) Get data qualities (requires the data id) Task types and tasks \u00b6 A dataset alone does not constitute a scientific task. We must first agree on what types of results are expected to be shared. This is expressed in task types: they define what types of inputs are given, which types of output are expected to be returned, and what protocols should be used. For instance, classification tasks should include well-defined cross-validation procedures, labelled input data, and require predictions as outputs. The collection of all this information together is called a task. The Iris dataset has various tasks defined on it, for example this one . Although the web-interface does not show it, this task formally describes the target attribute that should be modelled (in this case the same as the default target attribute of the dataset, but this is flexible), the quality estimation procedure (10-fold cross-validation), the evaluation measure (predictive accuracy) and the cross-validation folds. Useful API operations include: Get all available tasks Get all available tasks of a given type (e.g. get all Classification tasks, requires the id of the task type) Get the details of a task (requires task id) Currently, there are a wide range of task types defined on OpenML, including classification, regression, on line learning, clustering and subgroup discovery. Although this set can be extended, this is currently not a supported API operation (meaning that we will add them by hand). If you interested in task types that are currently not supported, please contact us. Flows \u00b6 Tasks can be 'solved' by classifiers (or algorithms, workflows, flows). OpenML stores references to these flows. It is important to stress that flows are actually ran on the computer of the user, only meta-information about the flow is stored on OpenML. This information includes basic trivialities such as the creator, toolbox and compilation instructions, but also more formal description about hyper parameter. A flow can also contain subflows, for example, the flow Bagging can have a subflow 'Decision Tree' which would make the flow 'Bagging of Decision Trees'. A flow is distinguished by its name and 'external version', which are both provided by the uploader. When uploading a flow, it is important to think about a good naming convention for the both, for example, the git commit number could be used as external version, as this uniquely identifies a state of the code. Ideally, when two persons are using the same flow, they will use the same name and external version, so that results of the flows can be compared across tasks. (This is ensured when using the toolboxed in which OpenML is integrated, such as Weka, Scikit Learn and MLR). Useful API functions are: List all flows List all my flows Give details about a given flow (requires flow id) Runs \u00b6 Whenever a flow executes a task, this is called a run. The existence of runs is actually the main contribution of OpenML. Some experiments take weeks to complete, and having the results stored on OpenML helps other researchers reuse the experiments. The task description specifies which information should be uploaded in order to have a valid run, in most cases, for each cross-validation fold the predictions on the test set. This allows OpenML to calculate basic evaluation measures, such as predictive accuracy, ROC curves and many more. Also information about the flow and hyper parameter settings should be provided. Some useful API functions: List all runs performed on a given task (requires task id, e.g., the iris task is 59) Compare two flows on all tasks (requires a comma separated list of flow ids, e.g., 1720, 1721 for comparing k-nn with a decision tree) And many more ... Usually, the result is in some XML or JSON format (depending on the preference of the user), linking together various task ids, flow ids, etc. In order for this to become meaningful, the user needs to perform other API tasks to get information about what flows were executed, what tasks and datasets were used, etc. Details about this will be provided in another post. Setups \u00b6 Every run that is executed by a flow, contains information about the hyper parameter settings of the flow. A setup is the combination of all parameter settings of a given flow. OpenML internally links the result of a given run to a setup id. This way, experiments can be done across hyper parameter settings. For example, Compare two setups on all tasks (requires a comma separated list of setup ids, e.g., 8994, 8995, 8996 for comparing multiple MLP configurations) As setups constitute a complex concept, most of the operations concerning setups are hidden from the user. Hence, not all setup functions are properly documented yet. For example, these do not contain a page on the webinterface.","title":"Basic Concepts"},{"location":"Basic-Concepts/#basic-concepts","text":"Researchers are encouraged to upload their experimental results on OpenML, so that these can be reused by anyone. Various high level papers have been published that overview the design goals, benefits and opportunities (for example, at ECML/PKDD 2013 , SIGKDD Explorations and JMLR ). However, there is no clear overview of the basic concepts upon which the platform is build. In this blog post I will review these, and discuss some best practices. This page is a slightly updated version of this blogpost","title":"Basic Concepts"},{"location":"Basic-Concepts/#data","text":"One of the core components of OpenML are datasets. People can upload their datasets, and the system automatically organises these on line. An example of a dataset is the well-known Iris dataset . It shows all features, once of these is identified as the 'default target attribute', although this concept is flexible. It also shows some automatically computed data qualities (or, meta-features). Each dataset has its own unique ID. Information about the dataset, the data features and the data qualities can be obtained automatically by means of the following API functions: Get all available datasets Get dataset (required the data id) Get data features (requires the data id) Get data qualities (requires the data id)","title":"Data"},{"location":"Basic-Concepts/#task-types-and-tasks","text":"A dataset alone does not constitute a scientific task. We must first agree on what types of results are expected to be shared. This is expressed in task types: they define what types of inputs are given, which types of output are expected to be returned, and what protocols should be used. For instance, classification tasks should include well-defined cross-validation procedures, labelled input data, and require predictions as outputs. The collection of all this information together is called a task. The Iris dataset has various tasks defined on it, for example this one . Although the web-interface does not show it, this task formally describes the target attribute that should be modelled (in this case the same as the default target attribute of the dataset, but this is flexible), the quality estimation procedure (10-fold cross-validation), the evaluation measure (predictive accuracy) and the cross-validation folds. Useful API operations include: Get all available tasks Get all available tasks of a given type (e.g. get all Classification tasks, requires the id of the task type) Get the details of a task (requires task id) Currently, there are a wide range of task types defined on OpenML, including classification, regression, on line learning, clustering and subgroup discovery. Although this set can be extended, this is currently not a supported API operation (meaning that we will add them by hand). If you interested in task types that are currently not supported, please contact us.","title":"Task types and\u00a0tasks"},{"location":"Basic-Concepts/#flows","text":"Tasks can be 'solved' by classifiers (or algorithms, workflows, flows). OpenML stores references to these flows. It is important to stress that flows are actually ran on the computer of the user, only meta-information about the flow is stored on OpenML. This information includes basic trivialities such as the creator, toolbox and compilation instructions, but also more formal description about hyper parameter. A flow can also contain subflows, for example, the flow Bagging can have a subflow 'Decision Tree' which would make the flow 'Bagging of Decision Trees'. A flow is distinguished by its name and 'external version', which are both provided by the uploader. When uploading a flow, it is important to think about a good naming convention for the both, for example, the git commit number could be used as external version, as this uniquely identifies a state of the code. Ideally, when two persons are using the same flow, they will use the same name and external version, so that results of the flows can be compared across tasks. (This is ensured when using the toolboxed in which OpenML is integrated, such as Weka, Scikit Learn and MLR). Useful API functions are: List all flows List all my flows Give details about a given flow (requires flow id)","title":"Flows"},{"location":"Basic-Concepts/#runs","text":"Whenever a flow executes a task, this is called a run. The existence of runs is actually the main contribution of OpenML. Some experiments take weeks to complete, and having the results stored on OpenML helps other researchers reuse the experiments. The task description specifies which information should be uploaded in order to have a valid run, in most cases, for each cross-validation fold the predictions on the test set. This allows OpenML to calculate basic evaluation measures, such as predictive accuracy, ROC curves and many more. Also information about the flow and hyper parameter settings should be provided. Some useful API functions: List all runs performed on a given task (requires task id, e.g., the iris task is 59) Compare two flows on all tasks (requires a comma separated list of flow ids, e.g., 1720, 1721 for comparing k-nn with a decision tree) And many more ... Usually, the result is in some XML or JSON format (depending on the preference of the user), linking together various task ids, flow ids, etc. In order for this to become meaningful, the user needs to perform other API tasks to get information about what flows were executed, what tasks and datasets were used, etc. Details about this will be provided in another post.","title":"Runs"},{"location":"Basic-Concepts/#setups","text":"Every run that is executed by a flow, contains information about the hyper parameter settings of the flow. A setup is the combination of all parameter settings of a given flow. OpenML internally links the result of a given run to a setup id. This way, experiments can be done across hyper parameter settings. For example, Compare two setups on all tasks (requires a comma separated list of setup ids, e.g., 8994, 8995, 8996 for comparing multiple MLP configurations) As setups constitute a complex concept, most of the operations concerning setups are hidden from the user. Hence, not all setup functions are properly documented yet. For example, these do not contain a page on the webinterface.","title":"Setups"},{"location":"Client-API-Standards/","text":"This page defines a minimal standard to adhere in programming APIs. Configuration file \u00b6 The configuration file resides in a directory .openml in the home directory of the user and is called config. It consists of key = value pairs which are seperated by newlines. The following keys are defined: apikey: required to access the server server: default: http://www.openml.org verbosity: 0: normal output 1: info output 2: debug output cachedir: if not given, will default to file.path(tempdir(), \"cache\") . arff.reader: RWeka : This is the standard Java parser used in Weka. farff : The farff package lives below the mlr-org and is a newer, faster parser without Java. Caching \u00b6 Cache invalidation \u00b6 All parts of the entities which affect experiments are immutable. The entities dataset and task have a flag status which tells the user whether they can be used safely. File structure \u00b6 Caching should be implemented for datasets tasks splits predictions and further entities might follow in the future. The cache directory $cache should be specified by the user when invoking the API. The structure in the cache directory should be as following: One directory for the following entities: $cache/datasets $cache/tasks $cache/runs For every dataset there is an extra directory for which the name is the dataset ID, e.g. $cache/datasets/2 for the dataset anneal.ORIG The dataset should be called dataset.arff Every other file should be named by the API call which was used to obtain it. The XML returned by invoking openml.data.qualities should therefore be called qualities.xml. For every task there is an extra directory for which the name is the task ID, e.g. $cache/tasks/1 The task file should be called task.xml . The splits accompanying a task are stored in a file datasplits.arff . For every run there is an extra directory for which the name is the run ID, e.g. $cache/run/1 The predictions should be called predictions.arff .","title":"Client Development"},{"location":"Client-API-Standards/#configuration-file","text":"The configuration file resides in a directory .openml in the home directory of the user and is called config. It consists of key = value pairs which are seperated by newlines. The following keys are defined: apikey: required to access the server server: default: http://www.openml.org verbosity: 0: normal output 1: info output 2: debug output cachedir: if not given, will default to file.path(tempdir(), \"cache\") . arff.reader: RWeka : This is the standard Java parser used in Weka. farff : The farff package lives below the mlr-org and is a newer, faster parser without Java.","title":"Configuration file"},{"location":"Client-API-Standards/#caching","text":"","title":"Caching"},{"location":"Client-API-Standards/#cache-invalidation","text":"All parts of the entities which affect experiments are immutable. The entities dataset and task have a flag status which tells the user whether they can be used safely.","title":"Cache invalidation"},{"location":"Client-API-Standards/#file-structure","text":"Caching should be implemented for datasets tasks splits predictions and further entities might follow in the future. The cache directory $cache should be specified by the user when invoking the API. The structure in the cache directory should be as following: One directory for the following entities: $cache/datasets $cache/tasks $cache/runs For every dataset there is an extra directory for which the name is the dataset ID, e.g. $cache/datasets/2 for the dataset anneal.ORIG The dataset should be called dataset.arff Every other file should be named by the API call which was used to obtain it. The XML returned by invoking openml.data.qualities should therefore be called qualities.xml. For every task there is an extra directory for which the name is the task ID, e.g. $cache/tasks/1 The task file should be called task.xml . The splits accompanying a task are stored in a file datasplits.arff . For every run there is an extra directory for which the name is the run ID, e.g. $cache/run/1 The predictions should be called predictions.arff .","title":"File structure"},{"location":"Communication-Channels/","text":"We have several communication channels set up for different purposes: GitHub \u00b6 https://github.com/openml Issues (members and users can complain) Request new features Anyone with a GitHub account can write issues. We are happy if people get involved by writing issues, so don't be shy Slack \u00b6 https://openml.slack.com Informal communication We use slack for day to day discussions and news. If you want to join the OpenML slack chat, please message us ( openmlHQ@googlegroups.com ). Mailing List \u00b6 https://groups.google.com/forum/#!forum/openml Information on upcoming workshop Other major information Urgent or important issues If you want to receive information on major news or upcoming events, sign up for the mailing list . There is a private mailing list for OpenML core members which you can contact by sending an e-mail to openmlHQ@googlegroups.com . Twitter (@open_ml) \u00b6 https://twitter.com/open_ml News Publicly relevant information Blog \u00b6 https://medium.com/open-machine-learning/archive Tutorials News Info about papers","title":"Communication Channels"},{"location":"Communication-Channels/#github","text":"https://github.com/openml Issues (members and users can complain) Request new features Anyone with a GitHub account can write issues. We are happy if people get involved by writing issues, so don't be shy","title":"GitHub"},{"location":"Communication-Channels/#slack","text":"https://openml.slack.com Informal communication We use slack for day to day discussions and news. If you want to join the OpenML slack chat, please message us ( openmlHQ@googlegroups.com ).","title":"Slack"},{"location":"Communication-Channels/#mailing-list","text":"https://groups.google.com/forum/#!forum/openml Information on upcoming workshop Other major information Urgent or important issues If you want to receive information on major news or upcoming events, sign up for the mailing list . There is a private mailing list for OpenML core members which you can contact by sending an e-mail to openmlHQ@googlegroups.com .","title":"Mailing List"},{"location":"Communication-Channels/#twitter-open_ml","text":"https://twitter.com/open_ml News Publicly relevant information","title":"Twitter (@open_ml)"},{"location":"Communication-Channels/#blog","text":"https://medium.com/open-machine-learning/archive Tutorials News Info about papers","title":"Blog"},{"location":"Contributing/","text":"OpenML is an open source project, hosted on GitHub . We welcome everybody to help improve OpenML, and make it more useful for everyone. We want to make machine learning and data analysis simple , accessible , collaborative and open with an optimal division of labour between computers and humans. Want to get involved? \u00b6 Awesome, we're happy to have you! OpenML is dependent on the community. If you want to help, please email us ( openmlHQ@googlegroups.com ). If you feel already comfortable you can help by opening issues or make a pull request on GitHub. We also have regular workshops you can join (they are announced on openml.org). Who are we? \u00b6 We are a group of friendly people who are excited about open science and machine learning. A list of people currently involved can be found here . We need help! \u00b6 We are currently looking for help with: User feedback (best via GitHub issues, but email is also fine) Frontend / UX / Design of the website Backend / API Outreach / making OpenML better known (especially in non-ML-communities, where people have data but no analysis experise) Helping with the interfaces ( Python , WEKA , MOA , RapidMiner , Java , R ; find the links to GitHub repos here ) Helping with documenting the interfaces or the API What could we do better to get new users started? Help us to figure out what is difficult to understand about OpenML. If you are a new user, you are the perfect person for this! Beginner issues \u00b6 Check out the issues labeled Good first issue or help wanted (you need to be logged into GitHub to see these) Change the world \u00b6 If you have your own ideas on how you want to contribute, please get in touch ! We are very friendly and open to new ideas Communication channels: \u00b6 We have several communication channels set up for different purposes: GitHub \u00b6 https://github.com/openml Issues (members and users can complain) Request new features Anyone with a GitHub account can write issues. We are happy if people get involved by writing issues, so don't be shy Please post issues in the relevant issue tracker. OpenML Core - Web services and API Website - The (new) OpenML website Docs - The documentation pages Python API - The Python API R API - The OpenML R package Java API - The Java API and Java-based plugins Datasets - For issues about datasets Blog - The OpenML Blog Slack \u00b6 https://openml.slack.com Informal communication We use slack for day to day discussions and news. If you want to join the OpenML slack chat, please message us ( openmlHQ@googlegroups.com ). Mailing List \u00b6 https://groups.google.com/forum/#!forum/openml Information on upcoming workshop Other major information Urgent or important issues If you want to receive information on major news or upcoming events, sign up for the mailing list . There is a privat mailing list for OpenML core members which you can contact by sending an e-mail to openmlHQ@googlegroups.com . Twitter (@open_ml) \u00b6 https://twitter.com/open_ml News Publicly relevant information Blog \u00b6 https://blog.openml.org Tutorials News Open discussions Contributors bot \u00b6 We use all contributors bot to add contributors to the repository README. You can check how to use this here . You can contribute in a lot of ways including code, blogs, content, design and talks. You can find the emoji key here .","title":"How to Contribute"},{"location":"Contributing/#want-to-get-involved","text":"Awesome, we're happy to have you! OpenML is dependent on the community. If you want to help, please email us ( openmlHQ@googlegroups.com ). If you feel already comfortable you can help by opening issues or make a pull request on GitHub. We also have regular workshops you can join (they are announced on openml.org).","title":"Want to get involved?"},{"location":"Contributing/#who-are-we","text":"We are a group of friendly people who are excited about open science and machine learning. A list of people currently involved can be found here .","title":"Who are we?"},{"location":"Contributing/#we-need-help","text":"We are currently looking for help with: User feedback (best via GitHub issues, but email is also fine) Frontend / UX / Design of the website Backend / API Outreach / making OpenML better known (especially in non-ML-communities, where people have data but no analysis experise) Helping with the interfaces ( Python , WEKA , MOA , RapidMiner , Java , R ; find the links to GitHub repos here ) Helping with documenting the interfaces or the API What could we do better to get new users started? Help us to figure out what is difficult to understand about OpenML. If you are a new user, you are the perfect person for this!","title":"We need help!"},{"location":"Contributing/#beginner-issues","text":"Check out the issues labeled Good first issue or help wanted (you need to be logged into GitHub to see these)","title":"Beginner issues"},{"location":"Contributing/#change-the-world","text":"If you have your own ideas on how you want to contribute, please get in touch ! We are very friendly and open to new ideas","title":"Change the world"},{"location":"Contributing/#communication-channels","text":"We have several communication channels set up for different purposes:","title":"Communication channels:"},{"location":"Contributing/#github","text":"https://github.com/openml Issues (members and users can complain) Request new features Anyone with a GitHub account can write issues. We are happy if people get involved by writing issues, so don't be shy Please post issues in the relevant issue tracker. OpenML Core - Web services and API Website - The (new) OpenML website Docs - The documentation pages Python API - The Python API R API - The OpenML R package Java API - The Java API and Java-based plugins Datasets - For issues about datasets Blog - The OpenML Blog","title":"GitHub"},{"location":"Contributing/#slack","text":"https://openml.slack.com Informal communication We use slack for day to day discussions and news. If you want to join the OpenML slack chat, please message us ( openmlHQ@googlegroups.com ).","title":"Slack"},{"location":"Contributing/#mailing-list","text":"https://groups.google.com/forum/#!forum/openml Information on upcoming workshop Other major information Urgent or important issues If you want to receive information on major news or upcoming events, sign up for the mailing list . There is a privat mailing list for OpenML core members which you can contact by sending an e-mail to openmlHQ@googlegroups.com .","title":"Mailing List"},{"location":"Contributing/#twitter-open_ml","text":"https://twitter.com/open_ml News Publicly relevant information","title":"Twitter (@open_ml)"},{"location":"Contributing/#blog","text":"https://blog.openml.org Tutorials News Open discussions","title":"Blog"},{"location":"Contributing/#contributors-bot","text":"We use all contributors bot to add contributors to the repository README. You can check how to use this here . You can contribute in a lot of ways including code, blogs, content, design and talks. You can find the emoji key here .","title":"Contributors bot"},{"location":"Core-team/","text":"OpenML has many amazing contributors, which you can find on out team website . Should you be a contributor, but not on this page, let us know! Current members of the core team are: Joaquin Vanschoren Jan van Rijn Bernd Bischl Giuseppe Casaliccio Matthias Feurer Heidi Seibold You can contact us by emailing to openmlHQ@googlegroups.com . To get in touch with the broader community check out our communication channels .","title":"Core team"},{"location":"Dash/","text":"Dash visualization \u00b6 Dash is a python framework which is suitable for building data visualization dashboards using pure python. Dash is written on top of plotly, react and flask and the graphs are defined using plotly python. The dash application is composed of two major parts : Layout - Describes how the dashboard looks like Callbacks - Used to update graphs, tables in the layout and makes the dashboard interactive. Files \u00b6 The dash application is organized as follows: dashapp.py Creates the dash application The dash app is embedded in the flask app passed to create_dash_app function This file need not be modified to create a new plot layouts.py contains the layout for all the pages get_layout_from_data - returns layout of data visualization get_layout_from_task - returns layout of taskvisualization get_layout_from_flow - returns layout of flow visualization get_layout_from_run - returns layout of run visualization This file needs to be modified to add a new plot (data, task, flow, run) callbacks.py Registers all the callbacks for the dash application This file needs to be modified to add a new plot, especially if the plot needs to be interactive How the dashboard works \u00b6 In this dash application, we need to create the layout of the page dynamically based on the entered URL. For example, [ http://127.0.0.1:5000/dashboard/data/5 ] needs to return the layout for dataset id #5 whereas [ http://127.0.0.1:5000/dashboard/run/5 ] needs to return the layout for run id #5. Hence , the dash app is initially created with a dummy app.layout by dashapp.py and the callbacks are registered for the app using register_callbacks function. render_layout is the callback which dynamically renders layout. Once the dash app is running, the first callback which is fired is render_layout. This is the main callback invoked when a URL with a data , task, run or flow ID is entered. Based on the information in the URL, this method returns the layout. Based on the URL, get_layout_from_data, get_layout_from_task, get_layout_from_flow, get_layout_from_run are called. These functions define the layout of the page - tables, html Divs, tabs, graphs etc. The callbacks corresponding to each component in the layout are invoked to update the components dynamically and make the graphs interactive. For example, update_scatter_plot in data_callbacks.py updates the scatter plot component in the data visualization dashboard.","title":"Dash visualizations"},{"location":"Dash/#dash-visualization","text":"Dash is a python framework which is suitable for building data visualization dashboards using pure python. Dash is written on top of plotly, react and flask and the graphs are defined using plotly python. The dash application is composed of two major parts : Layout - Describes how the dashboard looks like Callbacks - Used to update graphs, tables in the layout and makes the dashboard interactive.","title":"Dash visualization"},{"location":"Dash/#files","text":"The dash application is organized as follows: dashapp.py Creates the dash application The dash app is embedded in the flask app passed to create_dash_app function This file need not be modified to create a new plot layouts.py contains the layout for all the pages get_layout_from_data - returns layout of data visualization get_layout_from_task - returns layout of taskvisualization get_layout_from_flow - returns layout of flow visualization get_layout_from_run - returns layout of run visualization This file needs to be modified to add a new plot (data, task, flow, run) callbacks.py Registers all the callbacks for the dash application This file needs to be modified to add a new plot, especially if the plot needs to be interactive","title":"Files"},{"location":"Dash/#how-the-dashboard-works","text":"In this dash application, we need to create the layout of the page dynamically based on the entered URL. For example, [ http://127.0.0.1:5000/dashboard/data/5 ] needs to return the layout for dataset id #5 whereas [ http://127.0.0.1:5000/dashboard/run/5 ] needs to return the layout for run id #5. Hence , the dash app is initially created with a dummy app.layout by dashapp.py and the callbacks are registered for the app using register_callbacks function. render_layout is the callback which dynamically renders layout. Once the dash app is running, the first callback which is fired is render_layout. This is the main callback invoked when a URL with a data , task, run or flow ID is entered. Based on the information in the URL, this method returns the layout. Based on the URL, get_layout_from_data, get_layout_from_task, get_layout_from_flow, get_layout_from_run are called. These functions define the layout of the page - tables, html Divs, tabs, graphs etc. The callbacks corresponding to each component in the layout are invoked to update the components dynamically and make the graphs interactive. For example, update_scatter_plot in data_callbacks.py updates the scatter plot component in the data visualization dashboard.","title":"How the dashboard works"},{"location":"Data-collections/","text":"This website is supposed to gather and explain curated lists of OpenML datasets. Efficient and Robust Automated Machine Learning - Feurer et al. - NIPS 2015 \u00b6 Contact: @mfeurer Used in: Efficient and Robust Automated Machine Learning Datasets: 1000,1002,1018,1019,1020,1021,1036,1040,1041,1049,1050,1053,1056,1067,1068,1069,1111,1112,1114,1116,1119,1120,1128,1130,1134,1138,1139,1142,1146,1161,1166,12,14,16,179,180,181,182,184,185,18,21,22,23,24,26,273,28,293,300,30,31,32,351,354,357,36,389,38,390,391,392,393,395,396,398,399,3,401,44,46,554,57,60,679,6,715,718,720,722,723,727,728,734,735,737,740,741,743,751,752,761,772,797,799,803,806,807,813,816,819,821,822,823,833,837,843,845,846,847,849,866,871,881,897,901,903,904,910,912,913,914,917,923,930,934,953,958,959,962,966,971,976,977,978,979,980,991,993,995 did name n p p.num p.syms p.bin n.class minclass maxclass n.miss 29 31 credit-g 1000 21 7 13 2 2 300 700 0 570 715 fri_c3_1000_25 1000 26 25 0 0 2 443 557 0 573 718 fri_c4_1000_100 1000 101 100 0 0 2 436 564 0 578 723 fri_c4_1000_25 1000 26 25 0 0 2 453 547 0 595 740 fri_c3_1000_10 1000 11 10 0 0 2 440 560 0 598 743 fri_c1_1000_5 1000 6 5 0 0 2 457 543 0 606 751 fri_c4_1000_10 1000 11 10 0 0 2 440 560 0 651 797 fri_c4_1000_50 1000 51 50 0 0 2 440 560 0 653 799 fri_c0_1000_5 1000 6 5 0 0 2 497 503 0 660 806 fri_c3_1000_50 1000 51 50 0 0 2 445 555 0 667 813 fri_c3_1000_5 1000 6 5 0 0 2 437 563 0 691 837 fri_c1_1000_50 1000 51 50 0 0 2 453 547 0 699 845 fri_c0_1000_10 1000 11 10 0 0 2 491 509 0 703 849 fri_c0_1000_25 1000 26 25 0 0 2 497 503 0 719 866 fri_c2_1000_50 1000 51 50 0 0 2 418 582 0 755 903 fri_c2_1000_25 1000 26 25 0 0 2 437 563 0 756 904 fri_c0_1000_50 1000 51 50 0 0 2 490 510 0 762 910 fri_c1_1000_10 1000 11 10 0 0 2 436 564 0 764 912 fri_c2_1000_5 1000 6 5 0 0 2 416 584 0 765 913 fri_c2_1000_10 1000 11 10 0 0 2 420 580 0 769 917 fri_c1_1000_25 1000 26 25 0 0 2 454 546 0 262 392 oh0.wc 1003 3183 3182 0 0 10 51 194 0 535 679 rmftsa_sleepdata 1024 3 2 1 0 4 94 404 0 596 741 rmftsa_sleepdata 1024 3 1 1 0 2 509 515 0 271 401 oh10.wc 1050 3239 3238 0 0 10 52 165 0 914 1068 pc1 1109 22 21 0 0 2 77 1032 0 786 934 socmob 1156 6 1 4 2 2 256 900 0 749 897 colleges_aaup 1161 17 13 3 0 2 348 813 87 782 930 colleges_usnews 1302 35 32 2 0 2 614 688 1144 123 185 baseball 1340 18 15 2 0 3 57 1215 20 818 966 analcatdata_halloffame 1340 18 15 2 0 2 125 1215 20 896 1049 pc4 1458 38 37 0 0 2 178 1280 0 21 23 cmc 1473 10 2 7 3 3 333 629 0 119 181 yeast 1484 9 8 0 0 10 5 463 0 261 391 re0.wc 1504 2887 2886 0 0 13 11 608 0 972 1128 OVA_Breast 1545 10937 10936 1 0 2 344 1201 0 974 1130 OVA_Lung 1545 10937 10936 0 0 2 126 1419 0 978 1134 OVA_Kidney 1545 10937 10936 0 0 2 260 1285 0 982 1138 OVA_Uterus 1545 10937 10936 1 0 2 124 1421 0 983 1139 OVA_Omentum 1545 10937 10936 0 0 2 77 1468 0 986 1142 OVA_Endometrium 1545 10937 10936 0 0 2 61 1484 0 990 1146 OVA_Prostate 1545 10937 10936 0 0 2 69 1476 0 1005 1161 OVA_Colon 1545 10937 10936 0 0 2 286 1259 0 1010 1166 OVA_Ovary 1545 10937 10936 0 0 2 198 1347 0 268 398 wap.wc 1560 8461 8460 0 0 20 5 341 0 897 1050 pc3 1563 38 37 0 0 2 160 1403 0 265 395 re1.wc 1657 3759 3758 1 0 25 10 371 0 19 21 car 1728 7 0 6 0 4 65 1210 0 843 991 car 1728 7 0 6 0 2 518 1210 0 12 12 mfeat-factors 2000 217 216 0 0 10 200 200 0 14 14 mfeat-fourier 2000 77 76 0 0 10 200 200 0 16 16 mfeat-karhunen 2000 65 64 0 0 10 200 200 0 17 18 mfeat-morphological 2000 7 6 0 0 10 200 200 0 20 22 mfeat-zernike 2000 48 47 0 0 10 200 200 0 814 962 mfeat-morphological 2000 7 6 0 0 2 200 1800 0 823 971 mfeat-fourier 2000 77 76 0 0 2 200 1800 0 830 978 mfeat-factors 2000 217 216 0 0 2 200 1800 0 847 995 mfeat-zernike 2000 48 47 0 0 2 200 1800 0 872 1020 mfeat-karhunen 2000 65 64 0 0 2 200 1800 0 766 914 balloon 2001 3 2 0 0 2 482 1519 0 913 1067 kc1 2109 22 21 0 0 2 326 1783 0 627 772 quake 2178 4 3 0 0 2 969 1209 0 33 36 segment 2310 20 19 0 0 7 330 330 0 810 958 segment 2310 20 19 0 0 2 330 1980 0 259 389 fbis.wc 2463 2001 2000 0 0 17 38 506 0 263 393 la2s.wc 3075 12433 12432 1 0 6 248 905 0 592 737 space_ga 3107 7 6 0 0 2 1541 1566 0 42 46 splice 3190 62 0 61 0 3 767 1655 0 805 953 splice 3190 62 0 61 0 2 1535 1655 0 3 3 kr-vs-kp 3196 37 0 36 34 2 1527 1669 0 266 396 la1s.wc 3204 13196 13195 1 NA 6 273 943 0 888 1041 gina_prior2 3468 785 784 0 0 10 315 383 0 35 38 sick 3772 30 7 22 20 2 231 3541 3772 52 57 hypothyroid 3772 30 7 22 20 4 2 3481 3772 852 1000 hypothyroid 3772 30 7 22 20 2 291 3481 3772 724 871 pollen 3848 6 5 0 0 2 1924 1924 0 583 728 analcatdata_supreme 4052 8 7 0 0 2 971 3081 0 575 720 abalone 4177 9 7 1 0 2 2081 2096 0 41 44 spambase 4601 58 57 0 0 2 1813 2788 0 54 60 waveform-5000 5000 41 40 0 0 3 1653 1692 0 831 979 waveform-5000 5000 41 40 0 0 2 1692 3308 0 28 30 page-blocks 5473 11 10 0 0 5 28 4913 0 873 1021 page-blocks 5473 11 10 0 0 2 560 4913 0 915 1069 pc2 5589 37 36 0 0 2 23 5566 0 26 28 optdigits 5620 65 64 0 0 10 554 572 0 832 980 optdigits 5620 65 64 0 0 2 572 5048 0 120 182 satimage 6430 37 36 0 0 6 625 1531 0 701 847 wind 6574 15 14 0 0 2 3073 3501 0 960 1116 musk 6598 170 167 2 0 2 1017 5581 0 845 993 kdd_ipums_la_97-small 7019 61 33 27 8 2 2594 4425 7019 657 803 delta_ailerons 7129 6 5 0 0 2 3346 3783 0 854 1002 kdd_ipums_la_98-small 7485 56 16 39 8 2 791 6694 7369 211 300 isolet 7797 618 617 0 0 26 298 300 0 22 24 mushroom 8124 23 0 22 4 2 3916 4208 2480 590 735 cpu_small 8192 13 12 0 0 2 2477 5715 0 607 752 puma32H 8192 33 32 0 0 2 4064 4128 0 616 761 cpu_act 8192 22 21 0 0 2 2477 5715 0 661 807 kin8nm 8192 9 8 0 0 2 4024 4168 0 670 816 puma8NH 8192 9 8 0 0 2 4078 4114 0 687 833 bank32nh 8192 33 32 0 0 2 2543 5649 0 775 923 visualizing_soil 8641 5 3 1 1 2 3888 4753 0 870 1018 kdd_ipums_la_99-small 8844 57 15 41 9 2 568 8276 8844 902 1056 mc1 9466 39 38 0 0 2 68 9398 0 673 819 delta_elevators 9517 7 6 0 0 2 4732 4785 0 260 390 new3s.wc 9558 26833 26832 1 0 44 104 696 0 828 976 kdd_JapaneseVowels 9961 15 14 0 0 2 1614 8347 0 899 1053 jm1 10885 22 21 0 0 2 2106 8779 5 30 32 pendigits 10992 17 16 0 0 10 1055 1144 0 871 1019 pendigits 10992 17 16 0 0 2 1144 9848 0 269 399 ohscal.wc 11162 11466 11465 0 0 10 709 1621 0 24 26 nursery 12960 9 0 8 1 5 2 4320 0 811 959 nursery 12960 9 0 8 1 2 4320 8640 0 589 734 ailerons 13750 41 40 0 0 2 5828 7922 0 883 1036 sylva_agnostic 14395 217 216 0 0 2 886 13509 0 887 1040 sylva_prior 14395 109 108 0 0 2 886 13509 0 577 722 pol 15000 49 48 0 0 2 5041 9959 0 700 846 elevators 16599 19 18 0 0 2 5130 11469 0 964 1120 MagicTelescope 19020 12 11 0 0 2 6688 12332 0 6 6 letter 20000 17 16 0 0 26 734 813 0 829 977 letter 20000 17 16 0 0 2 813 19187 0 676 822 cal_housing 20640 9 8 0 0 2 8385 12255 0 677 823 houses 20640 9 8 0 0 2 8914 11726 0 675 821 house_16H 22784 17 16 0 0 2 6744 16040 0 697 843 house_8L 22784 9 8 0 0 2 6744 16040 0 122 184 kropt 28056 7 0 6 0 18 27 4553 0 963 1119 adult-census 32561 16 7 8 1 2 7841 24720 2399 582 727 2dplanes 40768 11 10 0 0 2 20348 20420 0 734 881 mv 40768 11 7 3 2 2 16447 24321 0 753 901 fried 40768 11 10 0 0 2 20341 20427 0 117 179 adult 48842 15 2 12 1 2 11687 37155 3620 955 1111 KDDCup09_appetency 50000 231 192 38 4 2 890 49110 50000 956 1112 KDDCup09_churn 50000 231 192 39 NA 2 3672 46328 50000 958 1114 KDDCup09_upselling 50000 231 192 38 4 2 3682 46318 50000 414 554 mnist_784 70000 785 784 0 0 10 6313 7877 0 241 357 vehicle_sensIT 98528 101 100 1 1 2 49264 49264 0 118 180 covertype 110393 55 14 40 40 7 1339 51682 0 196 273 IMDB.drama 120919 1002 1001 1 1 2 43779 77140 0 239 351 codrna 488565 9 8 1 1 2 162855 325710 0 206 293 covertype 581012 55 54 1 1 2 283301 297711 0 240 354 poker 1025010 11 10 1 1 2 511308 513702 0","title":"Data collections"},{"location":"Data-collections/#efficient-and-robust-automated-machine-learning-feurer-et-al-nips-2015","text":"Contact: @mfeurer Used in: Efficient and Robust Automated Machine Learning Datasets: 1000,1002,1018,1019,1020,1021,1036,1040,1041,1049,1050,1053,1056,1067,1068,1069,1111,1112,1114,1116,1119,1120,1128,1130,1134,1138,1139,1142,1146,1161,1166,12,14,16,179,180,181,182,184,185,18,21,22,23,24,26,273,28,293,300,30,31,32,351,354,357,36,389,38,390,391,392,393,395,396,398,399,3,401,44,46,554,57,60,679,6,715,718,720,722,723,727,728,734,735,737,740,741,743,751,752,761,772,797,799,803,806,807,813,816,819,821,822,823,833,837,843,845,846,847,849,866,871,881,897,901,903,904,910,912,913,914,917,923,930,934,953,958,959,962,966,971,976,977,978,979,980,991,993,995 did name n p p.num p.syms p.bin n.class minclass maxclass n.miss 29 31 credit-g 1000 21 7 13 2 2 300 700 0 570 715 fri_c3_1000_25 1000 26 25 0 0 2 443 557 0 573 718 fri_c4_1000_100 1000 101 100 0 0 2 436 564 0 578 723 fri_c4_1000_25 1000 26 25 0 0 2 453 547 0 595 740 fri_c3_1000_10 1000 11 10 0 0 2 440 560 0 598 743 fri_c1_1000_5 1000 6 5 0 0 2 457 543 0 606 751 fri_c4_1000_10 1000 11 10 0 0 2 440 560 0 651 797 fri_c4_1000_50 1000 51 50 0 0 2 440 560 0 653 799 fri_c0_1000_5 1000 6 5 0 0 2 497 503 0 660 806 fri_c3_1000_50 1000 51 50 0 0 2 445 555 0 667 813 fri_c3_1000_5 1000 6 5 0 0 2 437 563 0 691 837 fri_c1_1000_50 1000 51 50 0 0 2 453 547 0 699 845 fri_c0_1000_10 1000 11 10 0 0 2 491 509 0 703 849 fri_c0_1000_25 1000 26 25 0 0 2 497 503 0 719 866 fri_c2_1000_50 1000 51 50 0 0 2 418 582 0 755 903 fri_c2_1000_25 1000 26 25 0 0 2 437 563 0 756 904 fri_c0_1000_50 1000 51 50 0 0 2 490 510 0 762 910 fri_c1_1000_10 1000 11 10 0 0 2 436 564 0 764 912 fri_c2_1000_5 1000 6 5 0 0 2 416 584 0 765 913 fri_c2_1000_10 1000 11 10 0 0 2 420 580 0 769 917 fri_c1_1000_25 1000 26 25 0 0 2 454 546 0 262 392 oh0.wc 1003 3183 3182 0 0 10 51 194 0 535 679 rmftsa_sleepdata 1024 3 2 1 0 4 94 404 0 596 741 rmftsa_sleepdata 1024 3 1 1 0 2 509 515 0 271 401 oh10.wc 1050 3239 3238 0 0 10 52 165 0 914 1068 pc1 1109 22 21 0 0 2 77 1032 0 786 934 socmob 1156 6 1 4 2 2 256 900 0 749 897 colleges_aaup 1161 17 13 3 0 2 348 813 87 782 930 colleges_usnews 1302 35 32 2 0 2 614 688 1144 123 185 baseball 1340 18 15 2 0 3 57 1215 20 818 966 analcatdata_halloffame 1340 18 15 2 0 2 125 1215 20 896 1049 pc4 1458 38 37 0 0 2 178 1280 0 21 23 cmc 1473 10 2 7 3 3 333 629 0 119 181 yeast 1484 9 8 0 0 10 5 463 0 261 391 re0.wc 1504 2887 2886 0 0 13 11 608 0 972 1128 OVA_Breast 1545 10937 10936 1 0 2 344 1201 0 974 1130 OVA_Lung 1545 10937 10936 0 0 2 126 1419 0 978 1134 OVA_Kidney 1545 10937 10936 0 0 2 260 1285 0 982 1138 OVA_Uterus 1545 10937 10936 1 0 2 124 1421 0 983 1139 OVA_Omentum 1545 10937 10936 0 0 2 77 1468 0 986 1142 OVA_Endometrium 1545 10937 10936 0 0 2 61 1484 0 990 1146 OVA_Prostate 1545 10937 10936 0 0 2 69 1476 0 1005 1161 OVA_Colon 1545 10937 10936 0 0 2 286 1259 0 1010 1166 OVA_Ovary 1545 10937 10936 0 0 2 198 1347 0 268 398 wap.wc 1560 8461 8460 0 0 20 5 341 0 897 1050 pc3 1563 38 37 0 0 2 160 1403 0 265 395 re1.wc 1657 3759 3758 1 0 25 10 371 0 19 21 car 1728 7 0 6 0 4 65 1210 0 843 991 car 1728 7 0 6 0 2 518 1210 0 12 12 mfeat-factors 2000 217 216 0 0 10 200 200 0 14 14 mfeat-fourier 2000 77 76 0 0 10 200 200 0 16 16 mfeat-karhunen 2000 65 64 0 0 10 200 200 0 17 18 mfeat-morphological 2000 7 6 0 0 10 200 200 0 20 22 mfeat-zernike 2000 48 47 0 0 10 200 200 0 814 962 mfeat-morphological 2000 7 6 0 0 2 200 1800 0 823 971 mfeat-fourier 2000 77 76 0 0 2 200 1800 0 830 978 mfeat-factors 2000 217 216 0 0 2 200 1800 0 847 995 mfeat-zernike 2000 48 47 0 0 2 200 1800 0 872 1020 mfeat-karhunen 2000 65 64 0 0 2 200 1800 0 766 914 balloon 2001 3 2 0 0 2 482 1519 0 913 1067 kc1 2109 22 21 0 0 2 326 1783 0 627 772 quake 2178 4 3 0 0 2 969 1209 0 33 36 segment 2310 20 19 0 0 7 330 330 0 810 958 segment 2310 20 19 0 0 2 330 1980 0 259 389 fbis.wc 2463 2001 2000 0 0 17 38 506 0 263 393 la2s.wc 3075 12433 12432 1 0 6 248 905 0 592 737 space_ga 3107 7 6 0 0 2 1541 1566 0 42 46 splice 3190 62 0 61 0 3 767 1655 0 805 953 splice 3190 62 0 61 0 2 1535 1655 0 3 3 kr-vs-kp 3196 37 0 36 34 2 1527 1669 0 266 396 la1s.wc 3204 13196 13195 1 NA 6 273 943 0 888 1041 gina_prior2 3468 785 784 0 0 10 315 383 0 35 38 sick 3772 30 7 22 20 2 231 3541 3772 52 57 hypothyroid 3772 30 7 22 20 4 2 3481 3772 852 1000 hypothyroid 3772 30 7 22 20 2 291 3481 3772 724 871 pollen 3848 6 5 0 0 2 1924 1924 0 583 728 analcatdata_supreme 4052 8 7 0 0 2 971 3081 0 575 720 abalone 4177 9 7 1 0 2 2081 2096 0 41 44 spambase 4601 58 57 0 0 2 1813 2788 0 54 60 waveform-5000 5000 41 40 0 0 3 1653 1692 0 831 979 waveform-5000 5000 41 40 0 0 2 1692 3308 0 28 30 page-blocks 5473 11 10 0 0 5 28 4913 0 873 1021 page-blocks 5473 11 10 0 0 2 560 4913 0 915 1069 pc2 5589 37 36 0 0 2 23 5566 0 26 28 optdigits 5620 65 64 0 0 10 554 572 0 832 980 optdigits 5620 65 64 0 0 2 572 5048 0 120 182 satimage 6430 37 36 0 0 6 625 1531 0 701 847 wind 6574 15 14 0 0 2 3073 3501 0 960 1116 musk 6598 170 167 2 0 2 1017 5581 0 845 993 kdd_ipums_la_97-small 7019 61 33 27 8 2 2594 4425 7019 657 803 delta_ailerons 7129 6 5 0 0 2 3346 3783 0 854 1002 kdd_ipums_la_98-small 7485 56 16 39 8 2 791 6694 7369 211 300 isolet 7797 618 617 0 0 26 298 300 0 22 24 mushroom 8124 23 0 22 4 2 3916 4208 2480 590 735 cpu_small 8192 13 12 0 0 2 2477 5715 0 607 752 puma32H 8192 33 32 0 0 2 4064 4128 0 616 761 cpu_act 8192 22 21 0 0 2 2477 5715 0 661 807 kin8nm 8192 9 8 0 0 2 4024 4168 0 670 816 puma8NH 8192 9 8 0 0 2 4078 4114 0 687 833 bank32nh 8192 33 32 0 0 2 2543 5649 0 775 923 visualizing_soil 8641 5 3 1 1 2 3888 4753 0 870 1018 kdd_ipums_la_99-small 8844 57 15 41 9 2 568 8276 8844 902 1056 mc1 9466 39 38 0 0 2 68 9398 0 673 819 delta_elevators 9517 7 6 0 0 2 4732 4785 0 260 390 new3s.wc 9558 26833 26832 1 0 44 104 696 0 828 976 kdd_JapaneseVowels 9961 15 14 0 0 2 1614 8347 0 899 1053 jm1 10885 22 21 0 0 2 2106 8779 5 30 32 pendigits 10992 17 16 0 0 10 1055 1144 0 871 1019 pendigits 10992 17 16 0 0 2 1144 9848 0 269 399 ohscal.wc 11162 11466 11465 0 0 10 709 1621 0 24 26 nursery 12960 9 0 8 1 5 2 4320 0 811 959 nursery 12960 9 0 8 1 2 4320 8640 0 589 734 ailerons 13750 41 40 0 0 2 5828 7922 0 883 1036 sylva_agnostic 14395 217 216 0 0 2 886 13509 0 887 1040 sylva_prior 14395 109 108 0 0 2 886 13509 0 577 722 pol 15000 49 48 0 0 2 5041 9959 0 700 846 elevators 16599 19 18 0 0 2 5130 11469 0 964 1120 MagicTelescope 19020 12 11 0 0 2 6688 12332 0 6 6 letter 20000 17 16 0 0 26 734 813 0 829 977 letter 20000 17 16 0 0 2 813 19187 0 676 822 cal_housing 20640 9 8 0 0 2 8385 12255 0 677 823 houses 20640 9 8 0 0 2 8914 11726 0 675 821 house_16H 22784 17 16 0 0 2 6744 16040 0 697 843 house_8L 22784 9 8 0 0 2 6744 16040 0 122 184 kropt 28056 7 0 6 0 18 27 4553 0 963 1119 adult-census 32561 16 7 8 1 2 7841 24720 2399 582 727 2dplanes 40768 11 10 0 0 2 20348 20420 0 734 881 mv 40768 11 7 3 2 2 16447 24321 0 753 901 fried 40768 11 10 0 0 2 20341 20427 0 117 179 adult 48842 15 2 12 1 2 11687 37155 3620 955 1111 KDDCup09_appetency 50000 231 192 38 4 2 890 49110 50000 956 1112 KDDCup09_churn 50000 231 192 39 NA 2 3672 46328 50000 958 1114 KDDCup09_upselling 50000 231 192 38 4 2 3682 46318 50000 414 554 mnist_784 70000 785 784 0 0 10 6313 7877 0 241 357 vehicle_sensIT 98528 101 100 1 1 2 49264 49264 0 118 180 covertype 110393 55 14 40 40 7 1339 51682 0 196 273 IMDB.drama 120919 1002 1001 1 1 2 43779 77140 0 239 351 codrna 488565 9 8 1 1 2 162855 325710 0 206 293 covertype 581012 55 54 1 1 2 283301 297711 0 240 354 poker 1025010 11 10 1 1 2 511308 513702 0","title":"Efficient and Robust Automated Machine Learning - Feurer et al. - NIPS 2015"},{"location":"Datasets/","text":"Data Formats \u00b6 To guarantee interoperability, we focus on a limited set of data formats. We aim to support all sorts of data, but for the moment we only fully support tabular data in the ARFF format. We are currently working on supporting a much wider range of formats. ARFF definition . Also check that attribute definitions do not mix spaces and tabs, and do not include end-of-line comments. Data repositories \u00b6 This is a list of public dataset repositories that offer additional useful machine learning datasets. These have widely varying data formats, so they require manual selection, parsing and meta-data extraction. A collection of sources made by different users https://github.com/caesar0301/awesome-public-datasets https://dreamtolearn.com/ryan/1001_datasets https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research https://pathmind.com/wiki/open-datasets https://paperswithcode.com/ https://medium.com/towards-artificial-intelligence/best-datasets-for-machine-learning-data-science-computer-vision-nlp-ai-c9541058cf4f https://lionbridge.ai/datasets/the-50-best-free-datasets-for-machine-learning/ https://www.v7labs.com/open-datasets?utm_source=v7&utm_medium=email&utm_campaign=edu_outreach Machine learning dataset repositories (mostly already in OpenML) UCI: https://archive.ics.uci.edu/ml/index.html KEEL: http://sci2s.ugr.es/keel/datasets.php LIBSVM: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ AutoWEKA datasets: http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/ skData package: https://github.com/jaberg/skdata/tree/master/skdata Rdatasets: http://vincentarelbundock.github.io/Rdatasets/datasets.html DataBrewer: https://github.com/rmax/databrewer liac-arff: https://github.com/renatopp/arff-datasets MS Open datasets: https://azure.microsoft.com/en-us/services/open-datasets/catalog/ APIs (mostly defunct): databrewer (Python): https://pypi.org/project/databrewer/ PyDataset (Python): https://github.com/iamaziz/PyDataset (wrapper for Rdatasets?) RDatasets (R): https://github.com/vincentarelbundock/Rdatasets Time series / Geo data: Data commons: https://datacommons.org/ UCR: http://timeseriesclassification.com/ Older version: http://www.cs.ucr.edu/~eamonn/time_series_data/ Deep learning datasets (mostly image data) https://www.tensorflow.org/datasets/catalog/overview http://deeplearning.net/datasets/ https://deeplearning4j.org/opendata http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html https://paperswithcode.com/datasets Extreme classification: http://manikvarma.org/downloads/XC/XMLRepository.html MLData (down) http://mldata.org/ AutoWEKA datasets: http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/ Kaggle public datasets https://www.kaggle.com/datasets RAMP Challenge datasets http://www.ramp.studio/data_domains Wolfram data repository http://datarepository.wolframcloud.com/ Data.world https://data.world/ Figshare (needs digging, lots of Excel files) https://figshare.com/search?q=dataset&quick=1 KDNuggets list of data sets (meta-list, lots of stuff here): http://www.kdnuggets.com/datasets/index.html Benchmark Data Sets for Highly Imbalanced Binary Classification http://www.cs.gsu.edu/~zding/research/imbalance-data/x19data.txt Feature Selection Challenge Datasets http://www.nipsfsc.ecs.soton.ac.uk/datasets/ http://featureselection.asu.edu/datasets.php BigML's list of 1000+ data sources http://blog.bigml.com/2013/02/28/data-data-data-thousands-of-public-data-sources/ Massive list from Data Science Central. http://www.datasciencecentral.com/profiles/blogs/data-sources-for-cool-data-science-projects R packages (also see https://github.com/openml/openml-r/issues/185 ) http://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html mlbench Stata datasets: http://www.stata-press.com/data/r13/r.html UTwente Activity recognition datasets: http://ps.ewi.utwente.nl/Datasets.php Vanderbilt: http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets Quandl https://www.quandl.com Microarray data: http://genomics-pubs.princeton.edu/oncology/ http://svitsrv25.epfl.ch/R-doc/library/multtest/html/golub.html Medical data: http://www.healthdata.gov/ http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/PPRPAGES/pprdat.htm http://hcup-us.ahrq.gov/ https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html https://nsduhweb.rti.org/respweb/homepage.cfm http://orwh.od.nih.gov/resources/policyreports/womenofcolor.asp Nature.com Scientific data repositories list https://www.nature.com/sdata/policies/repositories","title":"Datasets"},{"location":"Datasets/#data-formats","text":"To guarantee interoperability, we focus on a limited set of data formats. We aim to support all sorts of data, but for the moment we only fully support tabular data in the ARFF format. We are currently working on supporting a much wider range of formats. ARFF definition . Also check that attribute definitions do not mix spaces and tabs, and do not include end-of-line comments.","title":"Data Formats"},{"location":"Datasets/#data-repositories","text":"This is a list of public dataset repositories that offer additional useful machine learning datasets. These have widely varying data formats, so they require manual selection, parsing and meta-data extraction. A collection of sources made by different users https://github.com/caesar0301/awesome-public-datasets https://dreamtolearn.com/ryan/1001_datasets https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research https://pathmind.com/wiki/open-datasets https://paperswithcode.com/ https://medium.com/towards-artificial-intelligence/best-datasets-for-machine-learning-data-science-computer-vision-nlp-ai-c9541058cf4f https://lionbridge.ai/datasets/the-50-best-free-datasets-for-machine-learning/ https://www.v7labs.com/open-datasets?utm_source=v7&utm_medium=email&utm_campaign=edu_outreach Machine learning dataset repositories (mostly already in OpenML) UCI: https://archive.ics.uci.edu/ml/index.html KEEL: http://sci2s.ugr.es/keel/datasets.php LIBSVM: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ AutoWEKA datasets: http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/ skData package: https://github.com/jaberg/skdata/tree/master/skdata Rdatasets: http://vincentarelbundock.github.io/Rdatasets/datasets.html DataBrewer: https://github.com/rmax/databrewer liac-arff: https://github.com/renatopp/arff-datasets MS Open datasets: https://azure.microsoft.com/en-us/services/open-datasets/catalog/ APIs (mostly defunct): databrewer (Python): https://pypi.org/project/databrewer/ PyDataset (Python): https://github.com/iamaziz/PyDataset (wrapper for Rdatasets?) RDatasets (R): https://github.com/vincentarelbundock/Rdatasets Time series / Geo data: Data commons: https://datacommons.org/ UCR: http://timeseriesclassification.com/ Older version: http://www.cs.ucr.edu/~eamonn/time_series_data/ Deep learning datasets (mostly image data) https://www.tensorflow.org/datasets/catalog/overview http://deeplearning.net/datasets/ https://deeplearning4j.org/opendata http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html https://paperswithcode.com/datasets Extreme classification: http://manikvarma.org/downloads/XC/XMLRepository.html MLData (down) http://mldata.org/ AutoWEKA datasets: http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/ Kaggle public datasets https://www.kaggle.com/datasets RAMP Challenge datasets http://www.ramp.studio/data_domains Wolfram data repository http://datarepository.wolframcloud.com/ Data.world https://data.world/ Figshare (needs digging, lots of Excel files) https://figshare.com/search?q=dataset&quick=1 KDNuggets list of data sets (meta-list, lots of stuff here): http://www.kdnuggets.com/datasets/index.html Benchmark Data Sets for Highly Imbalanced Binary Classification http://www.cs.gsu.edu/~zding/research/imbalance-data/x19data.txt Feature Selection Challenge Datasets http://www.nipsfsc.ecs.soton.ac.uk/datasets/ http://featureselection.asu.edu/datasets.php BigML's list of 1000+ data sources http://blog.bigml.com/2013/02/28/data-data-data-thousands-of-public-data-sources/ Massive list from Data Science Central. http://www.datasciencecentral.com/profiles/blogs/data-sources-for-cool-data-science-projects R packages (also see https://github.com/openml/openml-r/issues/185 ) http://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html mlbench Stata datasets: http://www.stata-press.com/data/r13/r.html UTwente Activity recognition datasets: http://ps.ewi.utwente.nl/Datasets.php Vanderbilt: http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets Quandl https://www.quandl.com Microarray data: http://genomics-pubs.princeton.edu/oncology/ http://svitsrv25.epfl.ch/R-doc/library/multtest/html/golub.html Medical data: http://www.healthdata.gov/ http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/PPRPAGES/pprdat.htm http://hcup-us.ahrq.gov/ https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html https://nsduhweb.rti.org/respweb/homepage.cfm http://orwh.od.nih.gov/resources/policyreports/womenofcolor.asp Nature.com Scientific data repositories list https://www.nature.com/sdata/policies/repositories","title":"Data repositories"},{"location":"Flask/","text":"We use Flask as our web framework. It handles user authentication, dataset upload, task creation, and other aspects that require server-side interaction. It is designed to be independent from the OpenML API. This means that you can use it to create your own personal frontend for OpenML, using the main OpenML server to provide the data. Of course, you can also link it to your own local OpenML setup . Design \u00b6 Out flask app follows Application factories design pattern . A new app instance can be created by: from autoapp import create_app app = create_app ( config_object ) The backend is designed in a modular fashion with flask Blueprints . Currently, the flask app consists of two blueprints public and user: Public blueprint: contains routes that do not require user authentication or authorization. like signup and forgot password. User blueprint: Contains routes which require user authentication like login, changes in profile and fetching API key. New blueprints can be registered in `server/app.py` with register_blueprints function: def register_blueprints ( app ): app . register_blueprint ( new_blueprint ) Database setup \u00b6 If you want o setup a local user database similar to OpenML then follow these steps: Install MySQL Create a new database 'openml' Set current database to 'openml' via use method Download users.sql file from openml.org github repo and add it in the openml db via \"mysql -u root -p openml < users.sql\" Edit the database path in `server/extensions.py` and `server/config.py` Note: Remember to add passwords and socket extension address(if any) in both in server/extensions.py and server/config.py Security \u00b6 Flask backend uses JSON web tokens for all the user handling tasks. Flask JWT extended library is used to bind JWT with the flask app. Current Mechanism is : User logs in. JWT token is assigned to user and sent with every request to frontend. All the user information can only be accessed with a JWT token like edit profile and API-key. The JWT token is stored in local memory of the browser. The token get expired after 2 hours or get blacklisted after logout. JWT is registered as an extension in `server/extensions.py`. All the user password hash are saved in Argon2 format with the new backend. Registering Extensions \u00b6 To register a new extension to flask backend extension has to be added in server/extensions.py and initialized in server/app.py. Current extensions are : flask_argon2, flask_bcrypt, flask_jwt_extended and flask_sqlalchemy. Configuring App \u00b6 Configuration variables like secret keys, Database URI and extension configurations are specified in server/config.py with Config object, which is supplied to the flask app during initialization. Creating a new route \u00b6 To create a new route in backend you can add the route in server/public/views.py or server/user/views.py (if it requires user authorisation or JWT usage in any way). Bindings to OpenML server \u00b6 You can specify which OpenML server to connect to. This is stored in the .env file in the main directory. It is set to the main OpenML server by default: ELASTICSEARCH_SERVER = https : // www . openml . org / es OPENML_SERVER = https : // www . openml . org The ElasticSearch server is used to download information about datasets, tasks, flows and runs, as well as to power the frontend search. The OpenML server is used for uploading datasets, tasks, and anything else that requires calls to the OpenML API. Bindings to frontend \u00b6 The frontend is generated by React . See below for more information. The React app is loaded as a static website. This is done in Flask setup in file server.py . app = Flask ( __name__ , static_url_path = '' , static_folder = 'src/client/app/build' ) It will find the React app there and load it. Email Server \u00b6 OpenML uses its own mail server, You can use basically any mail server compatible with python SMTP library. Our suggestion is to use mailtrap.io for local testing. You can configure email server configurations in .env file. Currently we only use emails for confirmation email and forgotten password emails.","title":"Flask backend"},{"location":"Flask/#design","text":"Out flask app follows Application factories design pattern . A new app instance can be created by: from autoapp import create_app app = create_app ( config_object ) The backend is designed in a modular fashion with flask Blueprints . Currently, the flask app consists of two blueprints public and user: Public blueprint: contains routes that do not require user authentication or authorization. like signup and forgot password. User blueprint: Contains routes which require user authentication like login, changes in profile and fetching API key. New blueprints can be registered in `server/app.py` with register_blueprints function: def register_blueprints ( app ): app . register_blueprint ( new_blueprint )","title":"Design"},{"location":"Flask/#database-setup","text":"If you want o setup a local user database similar to OpenML then follow these steps: Install MySQL Create a new database 'openml' Set current database to 'openml' via use method Download users.sql file from openml.org github repo and add it in the openml db via \"mysql -u root -p openml < users.sql\" Edit the database path in `server/extensions.py` and `server/config.py` Note: Remember to add passwords and socket extension address(if any) in both in server/extensions.py and server/config.py","title":"Database setup"},{"location":"Flask/#security","text":"Flask backend uses JSON web tokens for all the user handling tasks. Flask JWT extended library is used to bind JWT with the flask app. Current Mechanism is : User logs in. JWT token is assigned to user and sent with every request to frontend. All the user information can only be accessed with a JWT token like edit profile and API-key. The JWT token is stored in local memory of the browser. The token get expired after 2 hours or get blacklisted after logout. JWT is registered as an extension in `server/extensions.py`. All the user password hash are saved in Argon2 format with the new backend.","title":"Security"},{"location":"Flask/#registering-extensions","text":"To register a new extension to flask backend extension has to be added in server/extensions.py and initialized in server/app.py. Current extensions are : flask_argon2, flask_bcrypt, flask_jwt_extended and flask_sqlalchemy.","title":"Registering Extensions"},{"location":"Flask/#configuring-app","text":"Configuration variables like secret keys, Database URI and extension configurations are specified in server/config.py with Config object, which is supplied to the flask app during initialization.","title":"Configuring App"},{"location":"Flask/#creating-a-new-route","text":"To create a new route in backend you can add the route in server/public/views.py or server/user/views.py (if it requires user authorisation or JWT usage in any way).","title":"Creating a new route"},{"location":"Flask/#bindings-to-openml-server","text":"You can specify which OpenML server to connect to. This is stored in the .env file in the main directory. It is set to the main OpenML server by default: ELASTICSEARCH_SERVER = https : // www . openml . org / es OPENML_SERVER = https : // www . openml . org The ElasticSearch server is used to download information about datasets, tasks, flows and runs, as well as to power the frontend search. The OpenML server is used for uploading datasets, tasks, and anything else that requires calls to the OpenML API.","title":"Bindings to OpenML server"},{"location":"Flask/#bindings-to-frontend","text":"The frontend is generated by React . See below for more information. The React app is loaded as a static website. This is done in Flask setup in file server.py . app = Flask ( __name__ , static_url_path = '' , static_folder = 'src/client/app/build' ) It will find the React app there and load it.","title":"Bindings to frontend"},{"location":"Flask/#email-server","text":"OpenML uses its own mail server, You can use basically any mail server compatible with python SMTP library. Our suggestion is to use mailtrap.io for local testing. You can configure email server configurations in .env file. Currently we only use emails for confirmation email and forgotten password emails.","title":"Email Server"},{"location":"Gamification/","text":"Gamification is the use of game thinking and game mechanics in non-game contexts to engage users in solving problems and increase users' self contributions (definition from Wikipedia ). Examples: * Foursquare badge list * Class badges In order to increase user participation and loyalty, we can include some badges to user profile. Here is a list of possible badges: Datasets \u00b6 1 dataset: 10 datasets: 100 datasets: Submitting a dataset bigger than 1GB: Tasks \u00b6 1 task: 10 tasks: 100 tasks: 100 tasks of the same type: Supervised Classification: Supervised Data Stream Classification: Supervised Regression: Clustering: Learning Curve: Machine Learning Challenge: Survival Analysis: Flows \u00b6 1 flow: 10 flows: 100 flows: Runs \u00b6 1 run: 10 runs: 100 runs: 100 Weka flows: 100 R flows: 1000 runs: 10,000 runs: Submitting runs during 4 consecutive days:","title":"Gamification"},{"location":"Gamification/#datasets","text":"1 dataset: 10 datasets: 100 datasets: Submitting a dataset bigger than 1GB:","title":"Datasets"},{"location":"Gamification/#tasks","text":"1 task: 10 tasks: 100 tasks: 100 tasks of the same type: Supervised Classification: Supervised Data Stream Classification: Supervised Regression: Clustering: Learning Curve: Machine Learning Challenge: Survival Analysis:","title":"Tasks"},{"location":"Gamification/#flows","text":"1 flow: 10 flows: 100 flows:","title":"Flows"},{"location":"Gamification/#runs","text":"1 run: 10 runs: 100 runs: 100 Weka flows: 100 R flows: 1000 runs: 10,000 runs: Submitting runs during 4 consecutive days:","title":"Runs"},{"location":"Governance/","text":"The purpose of this document is to formalize the governance process used by the OpenML project (the OpenML GitHub organization which contains all code and projects related to OpenML.org), to clarify how decisions are made and how the various elements of our community interact. This document establishes a decision-making structure that takes into account feedback from all members of the community and strives to find consensus, while avoiding any deadlocks. The OpenML project is an independent open source project that is legally represented by the Open Machine Learning Foundation . The Open Machine Learning Foundation is a not-for-profit organization supporting, but not controlling, the OpenML project. The Foundation is open to engage with universities, companies, or anyone sharing the same goals. The OpenML project has a separate governance model described in this document. This is a meritocratic, consensus-based community project. Anyone with an interest in the project can join the community, contribute to the project design, and participate in the decision making process. This document describes how that participation takes place and how to set about earning merit within the project community. Roles And Responsibilities \u00b6 Contributors \u00b6 Contributors are community members who contribute in concrete ways to the project. Anyone can become a contributor, and contributions can take many forms \u2013 not only code \u2013 as detailed in the contributors guide. Contributors need to create pull requests to contribute to the code or documentation. Core contributors \u00b6 Core contributors are community members who have shown that they are dedicated to the continued development of the project through ongoing engagement with the community. They have shown they can be trusted to maintain OpenML with care. Being a core contributor allows contributors to more easily carry on with their project related activities by giving them write access to the project\u2019s repository (abiding by the decision making process described below, e.g. merging pull requests that obey the decision making procedure described below) and is represented as being an organization member on the OpenML GitHub organization. Core contributors are expected to review code contributions, can merge approved pull requests, can cast votes for and against merging a pull-request, and can be involved in deciding major changes to the API. New core contributors can be nominated by any existing core contributors. Once they have been nominated, there will be a vote in the private OpenML core email list by the current core contributors. While it is expected that most votes will be unanimous, a two-thirds majority of the cast votes is enough. The vote needs to be open for at least 1 week. Core contributors that have not contributed to the project (commits or GitHub comments) in the past 12 months will become emeritus core contributors and recant their commit and voting rights until they become active again. The list of core contributors, active and emeritus (with dates at which they became active) is public on the OpenML website. Steering Committee \u00b6 The Steering Committee (SC) members are core contributors who have additional responsibilities to ensure the smooth running of the project. SC members are expected to participate in strategic planning, join monthly meetings, and approve changes to the governance model. The purpose of the SC is to ensure a smooth progress from the big-picture perspective. Indeed, changes that impact the full project require a synthetic analysis and a consensus that is both explicit and informed. In cases that the core contributor community (which includes the SC members) fails to reach such a consensus in the required time frame, the SC is the entity to resolve the issue. The SC consists of community representatives and partner representatives. Community representatives of the SC are nominated by a core contributor. A nomination will result in a discussion that cannot take more than a month and then a vote by the core contributors which will stay open for a week. SC membership votes are subject to a two-third majority of all cast votes as well as a simple majority approval of all the current SC members. Partner institutions who enter a collaboration agreement or sponsorship agreement with the OpenML Foundation can nominate a representative on the Steering Committee, if so agreed in the agreement. Such a collaboration should in principle include one full-time developer to work on OpenML (in cash or in kind) for the duration of the agreement. New partner representatives have to be confirmed by the SC following the same voting rules above. The OpenML community must have at least equal footing in the steering committee. Additional SC members may be nominated to ensure this, following the membership voting rules described above. When decisions are escalated to the steering committee (see the decision making process below), and no consensus can be found within a month, the SC can meet and decide by consensus or with a simple majority of all cast votes. SC members who do not actively engage with the SC duties are expected to resign. The initial Steering Committee of OpenML consists of Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Heidi Seibold, Jan van Rijn, and Joaquin Vanschoren. They all represent the OpenML community. Decision Making Process \u00b6 Decisions about the future of the project are made through discussion with all members of the community. All non-sensitive project management discussion takes place on the project contributors\u2019 mailing list and the issue trackers of the sub-projects. Occasionally, sensitive discussion occurs on the private core developer email list (see below). This includes voting on core/SC membership or discussion of internal disputes. All discussions must follow the OpenML honor code . OpenML uses a \u201cconsensus seeking\u201d process for making decisions. The group tries to find a resolution that has no open objections among core contributors. At any point during the discussion, any core contributors can call for a vote, which will conclude one month from the call for the vote, or when two thirds of all votes are in favor. If no option can gather two thirds of the votes cast (ignoring abstentions), the decision is escalated to the SC, which in turn will use consensus seeking with the fallback option of a simple majority vote if no consensus can be found within a month. This is what we hereafter may refer to as \u201cthe decision making process\u201d. It applies to all core OpenML repositories. Decisions (in addition to adding core contributors and SC membership as above) are made according to the following rules: Normal changes: Minor Documentation changes, such as typo fixes, or addition / correction of a sentence: requires one approved review by a core contributor, and no objections in the comments (lazy consensus). Core contributors are expected to give \u201creasonable time\u201d to others to give their opinion on the pull request if they\u2019re not confident others would agree. Non-server packages that only have one core contributor are not subject to the ruling in the bullet point above (i.e. a sole core developer can make decisions on their own). Major changes: - Major changes to the API principles and metadata schema require a concrete proposal outlined in an OpenML Request for Comments (RfC), which has to be opened for public consultation for at least 1 month. The final version has to be approved using the decision-making process outlined above (two-third of the cast vote by core contributors or simple majority if escalated to the SC). Voting is typically done as a comment in the pull request (+1, -1, or 0 to abstain). - RfCs must be announced and shared via the public mailing list and may link additional content (such as blog posts or google docs etc. detailing the changes). - Changes to the governance model use the same decision process outlined above. If a veto -1 vote is cast on a lazy consensus, the proposer can appeal to the community and core contributors and the change can be approved or rejected using the decision making procedure outlined above. Communication channels \u00b6 OpenML uses the following communication channels: An open contributor mailing list and the GitHub issue trackers. A chat application for daily interaction with the community (currently Slack). Private email lists (without archive) for the core developers ( core@openml.org ) and steering committee ( steering@openml.org ), for membership voting and sensitive discussions. Biyearly Steering Committee meeting at predefined times, listed on the website, and asynchronous discussions on a discussion board. They are open to all steering committee members and core contributors, and they can all request discussion on a topic. Closed meetings for SC members only can be called in if there are sensitive discussions or other valid reasons. A monthly Engineering meeting at predefined times, listed on the website. The meeting is open to all. Discussion points are put on the [project roadmap] ( https://github.com/orgs/openml/projects/2 ).","title":"Governance"},{"location":"Governance/#roles-and-responsibilities","text":"","title":"Roles And Responsibilities"},{"location":"Governance/#contributors","text":"Contributors are community members who contribute in concrete ways to the project. Anyone can become a contributor, and contributions can take many forms \u2013 not only code \u2013 as detailed in the contributors guide. Contributors need to create pull requests to contribute to the code or documentation.","title":"Contributors"},{"location":"Governance/#core-contributors","text":"Core contributors are community members who have shown that they are dedicated to the continued development of the project through ongoing engagement with the community. They have shown they can be trusted to maintain OpenML with care. Being a core contributor allows contributors to more easily carry on with their project related activities by giving them write access to the project\u2019s repository (abiding by the decision making process described below, e.g. merging pull requests that obey the decision making procedure described below) and is represented as being an organization member on the OpenML GitHub organization. Core contributors are expected to review code contributions, can merge approved pull requests, can cast votes for and against merging a pull-request, and can be involved in deciding major changes to the API. New core contributors can be nominated by any existing core contributors. Once they have been nominated, there will be a vote in the private OpenML core email list by the current core contributors. While it is expected that most votes will be unanimous, a two-thirds majority of the cast votes is enough. The vote needs to be open for at least 1 week. Core contributors that have not contributed to the project (commits or GitHub comments) in the past 12 months will become emeritus core contributors and recant their commit and voting rights until they become active again. The list of core contributors, active and emeritus (with dates at which they became active) is public on the OpenML website.","title":"Core contributors"},{"location":"Governance/#steering-committee","text":"The Steering Committee (SC) members are core contributors who have additional responsibilities to ensure the smooth running of the project. SC members are expected to participate in strategic planning, join monthly meetings, and approve changes to the governance model. The purpose of the SC is to ensure a smooth progress from the big-picture perspective. Indeed, changes that impact the full project require a synthetic analysis and a consensus that is both explicit and informed. In cases that the core contributor community (which includes the SC members) fails to reach such a consensus in the required time frame, the SC is the entity to resolve the issue. The SC consists of community representatives and partner representatives. Community representatives of the SC are nominated by a core contributor. A nomination will result in a discussion that cannot take more than a month and then a vote by the core contributors which will stay open for a week. SC membership votes are subject to a two-third majority of all cast votes as well as a simple majority approval of all the current SC members. Partner institutions who enter a collaboration agreement or sponsorship agreement with the OpenML Foundation can nominate a representative on the Steering Committee, if so agreed in the agreement. Such a collaboration should in principle include one full-time developer to work on OpenML (in cash or in kind) for the duration of the agreement. New partner representatives have to be confirmed by the SC following the same voting rules above. The OpenML community must have at least equal footing in the steering committee. Additional SC members may be nominated to ensure this, following the membership voting rules described above. When decisions are escalated to the steering committee (see the decision making process below), and no consensus can be found within a month, the SC can meet and decide by consensus or with a simple majority of all cast votes. SC members who do not actively engage with the SC duties are expected to resign. The initial Steering Committee of OpenML consists of Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Heidi Seibold, Jan van Rijn, and Joaquin Vanschoren. They all represent the OpenML community.","title":"Steering Committee"},{"location":"Governance/#decision-making-process","text":"Decisions about the future of the project are made through discussion with all members of the community. All non-sensitive project management discussion takes place on the project contributors\u2019 mailing list and the issue trackers of the sub-projects. Occasionally, sensitive discussion occurs on the private core developer email list (see below). This includes voting on core/SC membership or discussion of internal disputes. All discussions must follow the OpenML honor code . OpenML uses a \u201cconsensus seeking\u201d process for making decisions. The group tries to find a resolution that has no open objections among core contributors. At any point during the discussion, any core contributors can call for a vote, which will conclude one month from the call for the vote, or when two thirds of all votes are in favor. If no option can gather two thirds of the votes cast (ignoring abstentions), the decision is escalated to the SC, which in turn will use consensus seeking with the fallback option of a simple majority vote if no consensus can be found within a month. This is what we hereafter may refer to as \u201cthe decision making process\u201d. It applies to all core OpenML repositories. Decisions (in addition to adding core contributors and SC membership as above) are made according to the following rules: Normal changes: Minor Documentation changes, such as typo fixes, or addition / correction of a sentence: requires one approved review by a core contributor, and no objections in the comments (lazy consensus). Core contributors are expected to give \u201creasonable time\u201d to others to give their opinion on the pull request if they\u2019re not confident others would agree. Non-server packages that only have one core contributor are not subject to the ruling in the bullet point above (i.e. a sole core developer can make decisions on their own). Major changes: - Major changes to the API principles and metadata schema require a concrete proposal outlined in an OpenML Request for Comments (RfC), which has to be opened for public consultation for at least 1 month. The final version has to be approved using the decision-making process outlined above (two-third of the cast vote by core contributors or simple majority if escalated to the SC). Voting is typically done as a comment in the pull request (+1, -1, or 0 to abstain). - RfCs must be announced and shared via the public mailing list and may link additional content (such as blog posts or google docs etc. detailing the changes). - Changes to the governance model use the same decision process outlined above. If a veto -1 vote is cast on a lazy consensus, the proposer can appeal to the community and core contributors and the change can be approved or rejected using the decision making procedure outlined above.","title":"Decision Making Process"},{"location":"Governance/#communication-channels","text":"OpenML uses the following communication channels: An open contributor mailing list and the GitHub issue trackers. A chat application for daily interaction with the community (currently Slack). Private email lists (without archive) for the core developers ( core@openml.org ) and steering committee ( steering@openml.org ), for membership voting and sensitive discussions. Biyearly Steering Committee meeting at predefined times, listed on the website, and asynchronous discussions on a discussion board. They are open to all steering committee members and core contributors, and they can all request discussion on a topic. Closed meetings for SC members only can be called in if there are sensitive discussions or other valid reasons. A monthly Engineering meeting at predefined times, listed on the website. The meeting is open to all. Discussion points are put on the [project roadmap] ( https://github.com/orgs/openml/projects/2 ).","title":"Communication channels"},{"location":"Helper-functions/","text":"Mostly written in Java, these functions build search indexes, compute dataset characteristics, generate tasks and evaluate the results of certain tasks. Code \u00b6 The Java code is available in the 'OpenML' repository: https://github.com/openml/OpenML/tree/master/Java Components \u00b6 General: - OpenML : Building Lucene search index and smaller tools, e.g. extracting documentation from WEKA source files and ARFF files - generateApiDocs : Generates API HTML Documentation - http_post_file : Example how to post files to the api using Java. Support for tasks: - foldgeneration : Java code for generating cross-validation folds. Can be used from command line. - splitgeneration : Split generator for cross validation and holdout. Unsure what's the difference with the previous? - generate_predictions : Helper class to build prediction files based on WEKA output. Move to WEKA repository? - evaluate_predictions : The evaluation engine computing evaluation scores based on submitted predictions","title":"Helper functions"},{"location":"Helper-functions/#code","text":"The Java code is available in the 'OpenML' repository: https://github.com/openml/OpenML/tree/master/Java","title":"Code"},{"location":"Helper-functions/#components","text":"General: - OpenML : Building Lucene search index and smaller tools, e.g. extracting documentation from WEKA source files and ARFF files - generateApiDocs : Generates API HTML Documentation - http_post_file : Example how to post files to the api using Java. Support for tasks: - foldgeneration : Java code for generating cross-validation folds. Can be used from command line. - splitgeneration : Split generator for cross validation and holdout. Unsure what's the difference with the previous? - generate_predictions : Helper class to build prediction files based on WEKA output. Move to WEKA repository? - evaluate_predictions : The evaluation engine computing evaluation scores based on submitted predictions","title":"Components"},{"location":"Home/","text":"OpenML Components \u00b6 To make development easier, OpenML has been subdivided into several subprojects with their own repositories, wikis, and issue trackers: * Website itself and API services: https://github.com/openml/website * Java library for interfacing with the OpenML API: https://github.com/openml/java * R package for interfacing with the OpenML API: https://github.com/openml/r * Python module for interfacing with the OpenML API (stub): https://github.com/openml/python * WEKA plugin: https://github.com/openml/weka * RapidMiner plugin: https://github.com/openml/rapidminer * KNIME plugin: https://github.com/openml/knime Suggestions for further integrations \u00b6 We need more data. Other people made efforts for hosting and selecting ML data already. [[Data-Repositories]] lists them. List must be extended and we need to check how much we already have integrated. Local installation of OpenML \u00b6 Developers who are working on new features may need a [[Local Installation]] for testing purposes. Backend development \u00b6 The website is built using a PHP/Java backend and a PHP/javascript frontend. An overview: * [[Web APP|WebApp-(PHP)]]: The high-level architecture of the website, including the controllers for different parts of the website (REST API, html, ...) and connections to the database. * [[Helper functions]]: Mostly written in Java, these functions build search indexes, compute dataset characteristics, generate tasks and evaluate the results of certain tasks. * [[URL Mapping]] A guide to the basics how a URL maps to internal files.","title":"Home"},{"location":"Home/#openml-components","text":"To make development easier, OpenML has been subdivided into several subprojects with their own repositories, wikis, and issue trackers: * Website itself and API services: https://github.com/openml/website * Java library for interfacing with the OpenML API: https://github.com/openml/java * R package for interfacing with the OpenML API: https://github.com/openml/r * Python module for interfacing with the OpenML API (stub): https://github.com/openml/python * WEKA plugin: https://github.com/openml/weka * RapidMiner plugin: https://github.com/openml/rapidminer * KNIME plugin: https://github.com/openml/knime","title":"OpenML Components"},{"location":"Home/#suggestions-for-further-integrations","text":"We need more data. Other people made efforts for hosting and selecting ML data already. [[Data-Repositories]] lists them. List must be extended and we need to check how much we already have integrated.","title":"Suggestions for further integrations"},{"location":"Home/#local-installation-of-openml","text":"Developers who are working on new features may need a [[Local Installation]] for testing purposes.","title":"Local installation of OpenML"},{"location":"Home/#backend-development","text":"The website is built using a PHP/Java backend and a PHP/javascript frontend. An overview: * [[Web APP|WebApp-(PHP)]]: The high-level architecture of the website, including the controllers for different parts of the website (REST API, html, ...) and connections to the database. * [[Helper functions]]: Mostly written in Java, these functions build search indexes, compute dataset characteristics, generate tasks and evaluate the results of certain tasks. * [[URL Mapping]] A guide to the basics how a URL maps to internal files.","title":"Backend development"},{"location":"IM-accounts/","text":"Skype \u00b6 Joaquin: joaquin.vanschoren Jan: Zwaaikom Venkatesh: lilacsunbird Luis Torgo: ltorgo Bernd Bischl: bernd.bischl Google \u00b6 Joaquin: joaquin.vanschoren@gmail.com Jan: janvanrijn@gmail.com Venkatesh: lilacsunbird Luis Torgo: ltorgo@gmail.com Bernd Bischl: bernd.bischl@gmail.com","title":"IM accounts"},{"location":"IM-accounts/#skype","text":"Joaquin: joaquin.vanschoren Jan: Zwaaikom Venkatesh: lilacsunbird Luis Torgo: ltorgo Bernd Bischl: bernd.bischl","title":"Skype"},{"location":"IM-accounts/#google","text":"Joaquin: joaquin.vanschoren@gmail.com Jan: janvanrijn@gmail.com Venkatesh: lilacsunbird Luis Torgo: ltorgo@gmail.com Bernd Bischl: bernd.bischl@gmail.com","title":"Google"},{"location":"Java-App/","text":"The Java App is used for a number of OpenML components, such as the ARFF parser and Evaluation engine, which depend on the Weka API. It is invoked from the OpenML API by means of a CLI interface. Typically, a call looks like this: java -jar webapplication.jar -config \"api_key=S3CR3T_AP1_K3Y\" -f evaluate_run -r 500 Which in this case executes the webapplication jar, invokes the function \"evaluate run\" and gives it parameter run id 500. The config parameter can be used to set some config items, in this case the api_key is mandatory. Every OpenML user has an api_key, which can be downloaded from their OpenML profile page . The response of this function is a call to the OpenML API uploading evaluation results to the OpenML database. Note that in this case the PHP website invokes the Java webapplication, which makes a call to the PHP website again, albeit another endpoint. The webapplication does not have direct writing rights into the database. All communication to the database goes by means of the OpenML Connector , which communicates with the OpenML API. As a consequence, the webapplication could run on any system, i.e., there is no formal need for the webapplication to be on the same server as the website code. This is important, since this created modularity, and not all servers provide a command line interface to PHP scripts. Another example is the following: java -jar webapplication -config \"api_key=S3CR3T_AP1_K3Y\" -f all_wrong -r 81,161 -t 59 Which takes a comma separated list of run ids (no spaces) and a task id as input and outputs the test examples on the dataset on which all algorithms used in the runs produced wrong examples (in this case, weka.BayesNet_K2 and weka.SMO, respectively). An error will be displayed if there are runs not consistent with the task id in there. Extending the Java App \u00b6 The bootstrap class of the webapplication is org.openml.webapplication.Main It automatically checks authentication settings (such as api_key) and the determines which function to invoke. It uses a switch-like if - else contruction to facilitate the functionalities of the various functions. Additional functions can be added to this freely. From there on, it is easy to add functionality to the webapplication. Parameters are handled using the Apache Commons CommandLineParser class, which makes sure that the passed parameters are available to the program. In order to make new functionalities available to the website, there also needs to be programmed an interface to the function, somewhere in the website. The next section details on that. Interfacing from the OpenML API \u00b6 By design, the REST API is not allowed to communicate with the Java App. All interfaces with the Java webapplication should go through other controllers of the PHP CodeIgniter framework., for example api_splits. Currently, the website features two main API's. These are represented by a Controller. Controllers can be found in the folder openml_OS/controllers. Here we see: * api_new.php, representing the REST API * api_splits.php, representing an API interfacing to the Java webapplication.","title":"Evaluation Engine"},{"location":"Java-App/#extending-the-java-app","text":"The bootstrap class of the webapplication is org.openml.webapplication.Main It automatically checks authentication settings (such as api_key) and the determines which function to invoke. It uses a switch-like if - else contruction to facilitate the functionalities of the various functions. Additional functions can be added to this freely. From there on, it is easy to add functionality to the webapplication. Parameters are handled using the Apache Commons CommandLineParser class, which makes sure that the passed parameters are available to the program. In order to make new functionalities available to the website, there also needs to be programmed an interface to the function, somewhere in the website. The next section details on that.","title":"Extending the Java App"},{"location":"Java-App/#interfacing-from-the-openml-api","text":"By design, the REST API is not allowed to communicate with the Java App. All interfaces with the Java webapplication should go through other controllers of the PHP CodeIgniter framework., for example api_splits. Currently, the website features two main API's. These are represented by a Controller. Controllers can be found in the folder openml_OS/controllers. Here we see: * api_new.php, representing the REST API * api_splits.php, representing an API interfacing to the Java webapplication.","title":"Interfacing from the OpenML API"},{"location":"Java-guide/","text":"The Java API allows you connect to OpenML from Java applications. Java Docs \u00b6 Read the full Java Docs . Download \u00b6 Stable releases of the Java API are available from Maven Central Or, you can check out the developer version from GitHub Include the jar file in your projects as usual, or install via Maven . Quick Start \u00b6 Create an OpenmlConnector instance with your authentication details. This will create a client with all OpenML functionalities. OpenmlConnector client = new OpenmlConnector(\"api_key\") All functions are described in the Java Docs . Downloading \u00b6 To download data, flows, tasks, runs, etc. you need the unique id of that resource. The id is shown on each item's webpage and in the corresponding url. For instance, let's download Data set 1 . The following returns a DataSetDescription object that contains all information about that data set. DataSetDescription data = client.dataGet(1); You can also search for the items you need online, and click the icon to get all id's that match a search. Uploading \u00b6 To upload data, flows, runs, etc. you need to provide a description of the object. We provide wrapper classes to provide this information, e.g. DataSetDescription , as well as to capture the server response, e.g. UploadDataSet , which always includes the generated id for reference: DataSetDescription description = new DataSetDescription( \"iris\", \"The famous iris dataset\", \"arff\", \"class\"); UploadDataSet result = client.dataUpload( description, datasetFile ); int data_id = result.getId(); More details are given in the corresponding functions below. Also see the Java Docs for all possible inputs and return values. Data download \u00b6 dataGet(int data_id) \u00b6 Retrieves the description of a specified data set. DataSetDescription data = client.dataGet(1); String name = data.getName(); String version = data.getVersion(); String description = data.getDescription(); String url = data.getUrl(); dataFeatures(int data_id) \u00b6 Retrieves the description of the features of a specified data set. DataFeature reponse = client.dataFeatures(1); DataFeature.Feature[] features = reponse.getFeatures(); String name = features[0].getName(); String type = features[0].getDataType(); boolean isTarget = features[0].getIs_target(); dataQuality(int data_id) \u00b6 Retrieves the description of the qualities (meta-features) of a specified data set. DataQuality response = client.dataQuality(1); DataQuality.Quality[] qualities = reponse.getQualities(); String name = qualities[0].getName(); String value = qualities[0].getValue(); dataQuality(int data_id, int start, int end, int interval_size) \u00b6 For data streams. Retrieves the description of the qualities (meta-features) of a specified portion of a data stream. DataQuality qualities = client.dataQuality(1,0,10000,null); dataQualityList() \u00b6 Retrieves a list of all data qualities known to OpenML. DataQualityList response = client.dataQualityList(); String[] qualities = response.getQualities(); Data upload \u00b6 dataUpload(DataSetDescription description, File dataset) \u00b6 Uploads a data set file to OpenML given a description. Throws an exception if the upload failed, see openml.data.upload for error codes. DataSetDescription dataset = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\"); UploadDataSet data = client.dataUpload( dataset, new File(\"data/path\")); int data_id = result.getId(); dataUpload(DataSetDescription description) \u00b6 Registers an existing dataset (hosted elsewhere). The description needs to include the url of the data set. Throws an exception if the upload failed, see openml.data.upload for error codes. DataSetDescription description = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\"); description.setUrl(\"http://datarepository.org/mydataset\"); UploadDataSet data = client.dataUpload( description ); int data_id = result.getId(); Flow download \u00b6 flowGet(int flow_id) \u00b6 Retrieves the description of the flow/implementation with the given id. Implementation flow = client.flowGet(100); String name = flow.getName(); String version = flow.getVersion(); String description = flow.getDescription(); String binary_url = flow.getBinary_url(); String source_url = flow.getSource_url(); Parameter[] parameters = flow.getParameter(); Flow management \u00b6 flowOwned() \u00b6 Retrieves an array of id's of all flows/implementations owned by you. ImplementationOwned response = client.flowOwned(); Integer[] ids = response.getIds(); flowExists(String name, String version) \u00b6 Checks whether an implementation with the given name and version is already registered on OpenML. ImplementationExists check = client.flowExists(\"weka.j48\", \"3.7.12\"); boolean exists = check.exists(); int flow_id = check.getId(); flowDelete(int id) \u00b6 Removes the flow with the given id (if you are its owner). ImplementationDelete response = client.openmlImplementationDelete(100); Flow upload \u00b6 flowUpload(Implementation description, File binary, File source) \u00b6 Uploads implementation files (binary and/or source) to OpenML given a description. Implementation flow = new Implementation(\"weka.J48\", \"3.7.12\", \"description\", \"Java\", \"WEKA 3.7.12\") UploadImplementation response = client.flowUpload( flow, new File(\"code.jar\"), new File(\"source.zip\")); int flow_id = response.getId(); Task download \u00b6 taskGet(int task_id) \u00b6 Retrieves the description of the task with the given id. Task task = client.taskGet(1); String task_type = task.getTask_type(); Input[] inputs = task.getInputs(); Output[] outputs = task.getOutputs(); taskEvaluations(int task_id) \u00b6 Retrieves all evaluations for the task with the given id. TaskEvaluations response = client.taskEvaluations(1); Evaluation[] evaluations = response.getEvaluation(); taskEvaluations(int task_id, int start, int end, int interval_size) \u00b6 For data streams. Retrieves all evaluations for the task over the specified window of the stream. TaskEvaluations response = client.taskEvaluations(1); Evaluation[] evaluations = response.getEvaluation(); Run download \u00b6 runGet(int run_id) \u00b6 Retrieves the description of the run with the given id. Run run = client.runGet(1); int task_id = run.getTask_id(); int flow_id = run.getImplementation_id(); Parameter_setting[] settings = run.getParameter_settings() EvaluationScore[] scores = run.getOutputEvaluation(); Run management \u00b6 runDelete(int run_id) \u00b6 Deletes the run with the given id (if you are its owner). RunDelete response = client.runDelete(1); Run upload \u00b6 runUpload(Run description, Map<String,File> output_files) \u00b6 Uploads a run to OpenML, including a description and a set of output files depending on the task type. Run.Parameter_setting[] parameter_settings = new Run.Parameter_setting[1]; parameter_settings[0] = Run.Parameter_setting(null, \"M\", \"2\"); Run run = new Run(\"1\", null, \"100\", \"setup_string\", parameter_settings); Map outputs = new HashMap<String,File>(); outputs.add(\"predictions\",new File(\"predictions.arff\")); UploadRun response = client.runUpload( run, outputs); int run_id = response.getRun_id();","title":"Java guide"},{"location":"Java-guide/#java-docs","text":"Read the full Java Docs .","title":"Java Docs"},{"location":"Java-guide/#download","text":"Stable releases of the Java API are available from Maven Central Or, you can check out the developer version from GitHub Include the jar file in your projects as usual, or install via Maven .","title":"Download"},{"location":"Java-guide/#quick-start","text":"Create an OpenmlConnector instance with your authentication details. This will create a client with all OpenML functionalities. OpenmlConnector client = new OpenmlConnector(\"api_key\") All functions are described in the Java Docs .","title":"Quick Start"},{"location":"Java-guide/#downloading","text":"To download data, flows, tasks, runs, etc. you need the unique id of that resource. The id is shown on each item's webpage and in the corresponding url. For instance, let's download Data set 1 . The following returns a DataSetDescription object that contains all information about that data set. DataSetDescription data = client.dataGet(1); You can also search for the items you need online, and click the icon to get all id's that match a search.","title":"Downloading"},{"location":"Java-guide/#uploading","text":"To upload data, flows, runs, etc. you need to provide a description of the object. We provide wrapper classes to provide this information, e.g. DataSetDescription , as well as to capture the server response, e.g. UploadDataSet , which always includes the generated id for reference: DataSetDescription description = new DataSetDescription( \"iris\", \"The famous iris dataset\", \"arff\", \"class\"); UploadDataSet result = client.dataUpload( description, datasetFile ); int data_id = result.getId(); More details are given in the corresponding functions below. Also see the Java Docs for all possible inputs and return values.","title":"Uploading"},{"location":"Java-guide/#data-download","text":"","title":"Data download"},{"location":"Java-guide/#datagetint-data_id","text":"Retrieves the description of a specified data set. DataSetDescription data = client.dataGet(1); String name = data.getName(); String version = data.getVersion(); String description = data.getDescription(); String url = data.getUrl();","title":"dataGet(int data_id)"},{"location":"Java-guide/#datafeaturesint-data_id","text":"Retrieves the description of the features of a specified data set. DataFeature reponse = client.dataFeatures(1); DataFeature.Feature[] features = reponse.getFeatures(); String name = features[0].getName(); String type = features[0].getDataType(); boolean isTarget = features[0].getIs_target();","title":"dataFeatures(int data_id)"},{"location":"Java-guide/#dataqualityint-data_id","text":"Retrieves the description of the qualities (meta-features) of a specified data set. DataQuality response = client.dataQuality(1); DataQuality.Quality[] qualities = reponse.getQualities(); String name = qualities[0].getName(); String value = qualities[0].getValue();","title":"dataQuality(int data_id)"},{"location":"Java-guide/#dataqualityint-data_id-int-start-int-end-int-interval_size","text":"For data streams. Retrieves the description of the qualities (meta-features) of a specified portion of a data stream. DataQuality qualities = client.dataQuality(1,0,10000,null);","title":"dataQuality(int data_id, int start, int end, int interval_size)"},{"location":"Java-guide/#dataqualitylist","text":"Retrieves a list of all data qualities known to OpenML. DataQualityList response = client.dataQualityList(); String[] qualities = response.getQualities();","title":"dataQualityList()"},{"location":"Java-guide/#data-upload","text":"","title":"Data upload"},{"location":"Java-guide/#datauploaddatasetdescription-description-file-dataset","text":"Uploads a data set file to OpenML given a description. Throws an exception if the upload failed, see openml.data.upload for error codes. DataSetDescription dataset = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\"); UploadDataSet data = client.dataUpload( dataset, new File(\"data/path\")); int data_id = result.getId();","title":"dataUpload(DataSetDescription description, File dataset)"},{"location":"Java-guide/#datauploaddatasetdescription-description","text":"Registers an existing dataset (hosted elsewhere). The description needs to include the url of the data set. Throws an exception if the upload failed, see openml.data.upload for error codes. DataSetDescription description = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\"); description.setUrl(\"http://datarepository.org/mydataset\"); UploadDataSet data = client.dataUpload( description ); int data_id = result.getId();","title":"dataUpload(DataSetDescription description)"},{"location":"Java-guide/#flow-download","text":"","title":"Flow download"},{"location":"Java-guide/#flowgetint-flow_id","text":"Retrieves the description of the flow/implementation with the given id. Implementation flow = client.flowGet(100); String name = flow.getName(); String version = flow.getVersion(); String description = flow.getDescription(); String binary_url = flow.getBinary_url(); String source_url = flow.getSource_url(); Parameter[] parameters = flow.getParameter();","title":"flowGet(int flow_id)"},{"location":"Java-guide/#flow-management","text":"","title":"Flow management"},{"location":"Java-guide/#flowowned","text":"Retrieves an array of id's of all flows/implementations owned by you. ImplementationOwned response = client.flowOwned(); Integer[] ids = response.getIds();","title":"flowOwned()"},{"location":"Java-guide/#flowexistsstring-name-string-version","text":"Checks whether an implementation with the given name and version is already registered on OpenML. ImplementationExists check = client.flowExists(\"weka.j48\", \"3.7.12\"); boolean exists = check.exists(); int flow_id = check.getId();","title":"flowExists(String name, String version)"},{"location":"Java-guide/#flowdeleteint-id","text":"Removes the flow with the given id (if you are its owner). ImplementationDelete response = client.openmlImplementationDelete(100);","title":"flowDelete(int id)"},{"location":"Java-guide/#flow-upload","text":"","title":"Flow upload"},{"location":"Java-guide/#flowuploadimplementation-description-file-binary-file-source","text":"Uploads implementation files (binary and/or source) to OpenML given a description. Implementation flow = new Implementation(\"weka.J48\", \"3.7.12\", \"description\", \"Java\", \"WEKA 3.7.12\") UploadImplementation response = client.flowUpload( flow, new File(\"code.jar\"), new File(\"source.zip\")); int flow_id = response.getId();","title":"flowUpload(Implementation description, File binary, File source)"},{"location":"Java-guide/#task-download","text":"","title":"Task download"},{"location":"Java-guide/#taskgetint-task_id","text":"Retrieves the description of the task with the given id. Task task = client.taskGet(1); String task_type = task.getTask_type(); Input[] inputs = task.getInputs(); Output[] outputs = task.getOutputs();","title":"taskGet(int task_id)"},{"location":"Java-guide/#taskevaluationsint-task_id","text":"Retrieves all evaluations for the task with the given id. TaskEvaluations response = client.taskEvaluations(1); Evaluation[] evaluations = response.getEvaluation();","title":"taskEvaluations(int task_id)"},{"location":"Java-guide/#taskevaluationsint-task_id-int-start-int-end-int-interval_size","text":"For data streams. Retrieves all evaluations for the task over the specified window of the stream. TaskEvaluations response = client.taskEvaluations(1); Evaluation[] evaluations = response.getEvaluation();","title":"taskEvaluations(int task_id, int start, int end, int interval_size)"},{"location":"Java-guide/#run-download","text":"","title":"Run download"},{"location":"Java-guide/#rungetint-run_id","text":"Retrieves the description of the run with the given id. Run run = client.runGet(1); int task_id = run.getTask_id(); int flow_id = run.getImplementation_id(); Parameter_setting[] settings = run.getParameter_settings() EvaluationScore[] scores = run.getOutputEvaluation();","title":"runGet(int run_id)"},{"location":"Java-guide/#run-management","text":"","title":"Run management"},{"location":"Java-guide/#rundeleteint-run_id","text":"Deletes the run with the given id (if you are its owner). RunDelete response = client.runDelete(1);","title":"runDelete(int run_id)"},{"location":"Java-guide/#run-upload","text":"","title":"Run upload"},{"location":"Java-guide/#runuploadrun-description-mapstringfile-output_files","text":"Uploads a run to OpenML, including a description and a set of output files depending on the task type. Run.Parameter_setting[] parameter_settings = new Run.Parameter_setting[1]; parameter_settings[0] = Run.Parameter_setting(null, \"M\", \"2\"); Run run = new Run(\"1\", null, \"100\", \"setup_string\", parameter_settings); Map outputs = new HashMap<String,File>(); outputs.add(\"predictions\",new File(\"predictions.arff\")); UploadRun response = client.runUpload( run, outputs); int run_id = response.getRun_id();","title":"runUpload(Run description, Map&lt;String,File&gt; output_files)"},{"location":"Job-Scheduling/","text":"In many cases, a scientist could be interested in running a wide range of flows over a range of datasets (or tasks). For this purpose, a job scheduler has been implemented. OpenML maintains a list of (setup,task) tuples, that users requested to run. A setup is a flow with information about all parameter settings. Flows can be linked to a specific workbench. For example, the flows weka.J48(1) (which has id 60) and weka.SMO_PolyKernel(1) (which has id 70) are linked to Weka_3.7.10. When the same algorithm is uploaded from out another Weka version, a new version of the implementation is registered at OpenML. For example, if we would upload weka.J48 from the old version weka_3.7.5, OpenML would register the implementation as weka.J48(2). (In fact, this is what actually happend. See implementation id 100.) In order to schedule jobs, go to the job scheduler ( Alpha version , no guarantees). Select a task type, and give the experiment a name. It is important to filter the tasks and setups using the menu on the left, since the overview can be huge. Note that there are a some tasks on \"big datasets\", which can slow down the experimentation proces. Be considerate including those. Also make sure to filter on setups with flows of your own workbench version. If these are not in the system yet, register these on the share page . [OpenML Weka] The OpenML weka package can be used to automatically execute the scheduled jobs. Run it with the following command: java -cp openmlweka.jar org.openml.weka.experiment.RunJob -T 3 -N 1000 With T being the task type to execute, and N the number of runs you want to perform. (You can set this number as high as you like, the experimenter will stop as soon as there are no jobs left.) Note that this experimenter will only execute jobs that are of this Weka version. The OpenML Weka package can automatically register implementations on OpenML. Using the GUI, execute an OpenML task with all the implementations that you want to be registered. Before running those, these will automatically be registered on OpenML.","title":"Job Scheduling"},{"location":"Local-Installation/","text":"1.Using Docker \u00b6 The easiest way to set up a local version of OpenML is to use Docker Compose following the instructions here (thanks to Rui Quintino!): https://github.com/openml/openml-docker-dev If you run into problems, please post an issue in the same github repo. 2. Installation from scratch \u00b6 If you want to install a local version of OpenML from scratch please follow the steps mentioned below: Requirements \u00b6 You'll need to have the following software running: * Apache Webserver, (with the rewrite module enabled. Is installed by default, not enabled.) * MySQL Server. * PHP 5.5 or higher (comes also with Apache) Or just a XAMP (Mac), LAMP (Linux) or WAMP (Windows) package, which conveniently contains all these applications. Databases \u00b6 Next, OpenML runs on two databases, a public database with all experiment information, and a private database, with information like user accounts etc. The latest version of both databases can be downloaded here: https://docs.openml.org/resources Obviously, the private database does not include any actual user account info. Backend \u00b6 The source code is available in the 'OpenML' repository: https://github.com/openml/OpenML OpenML is written in PHP, and can be 'installed' by copying all files in the 'www' or 'public_html' directory of Apache. After that, you need to provide your local paths and database accounts and passwords using the config file in: 'APACHE_WWW_DIR'/openml_OS/config/BASE_CONFIG.php. If everything is configured correctly, OpenML should now be running. Search Indices \u00b6 If you want to run your own (separate) OpenML instance, and store your own data, you'll also want to build your own search indices to show all data on the website. The OpenML website is based on the ElasticSearch stack. To install it, follow the instructions here: http://knowm.org/how-to-set-up-the-elk-stack-elasticsearch-logstash-and-kibana/ Initialization \u00b6 This script wipes all OpenML server data and rebuilds the database and search index. Replace 'openmldir' with the directory where you want OpenML to store files. # delete data from server sudo rm -rf /openmldir/* mkdir /openmldir/log # delete database mysqladmin -u \"root\" -p\"yourpassword\" DROP openml_expdb mysql -h localhost -u root -p\"yourpassword\" -e \"TRUNCATE openml.file;\" # reset ES search index echo \"Deleting and recreating the ES index: \" curl -XDELETE http://localhost:9200/openml curl -XPUT 'localhost:9200/openml?pretty' -H 'Content-Type: application/json' -d' { \"settings\" : { \"index\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 2 } } } ' # go to directory with the website source code cd /var/www/openml.org/public_html/ # reinitiate the database mysql -u root -p\"yourpassword!\" < downloads/openml_expdb.sql # fill important columns sudo php index.php cron install_database # rebuild search index sudo php index.php cron initialize_es_indices sudo php index.php cron build_es_indices sudo chown apache:apache /openmldir/log sudo chown apache:apache /openmldir/log/*","title":"Local Installation"},{"location":"Local-Installation/#1using-docker","text":"The easiest way to set up a local version of OpenML is to use Docker Compose following the instructions here (thanks to Rui Quintino!): https://github.com/openml/openml-docker-dev If you run into problems, please post an issue in the same github repo.","title":"1.Using Docker"},{"location":"Local-Installation/#2-installation-from-scratch","text":"If you want to install a local version of OpenML from scratch please follow the steps mentioned below:","title":"2. Installation from scratch"},{"location":"Local-Installation/#requirements","text":"You'll need to have the following software running: * Apache Webserver, (with the rewrite module enabled. Is installed by default, not enabled.) * MySQL Server. * PHP 5.5 or higher (comes also with Apache) Or just a XAMP (Mac), LAMP (Linux) or WAMP (Windows) package, which conveniently contains all these applications.","title":"Requirements"},{"location":"Local-Installation/#databases","text":"Next, OpenML runs on two databases, a public database with all experiment information, and a private database, with information like user accounts etc. The latest version of both databases can be downloaded here: https://docs.openml.org/resources Obviously, the private database does not include any actual user account info.","title":"Databases"},{"location":"Local-Installation/#backend","text":"The source code is available in the 'OpenML' repository: https://github.com/openml/OpenML OpenML is written in PHP, and can be 'installed' by copying all files in the 'www' or 'public_html' directory of Apache. After that, you need to provide your local paths and database accounts and passwords using the config file in: 'APACHE_WWW_DIR'/openml_OS/config/BASE_CONFIG.php. If everything is configured correctly, OpenML should now be running.","title":"Backend"},{"location":"Local-Installation/#search-indices","text":"If you want to run your own (separate) OpenML instance, and store your own data, you'll also want to build your own search indices to show all data on the website. The OpenML website is based on the ElasticSearch stack. To install it, follow the instructions here: http://knowm.org/how-to-set-up-the-elk-stack-elasticsearch-logstash-and-kibana/","title":"Search Indices"},{"location":"Local-Installation/#initialization","text":"This script wipes all OpenML server data and rebuilds the database and search index. Replace 'openmldir' with the directory where you want OpenML to store files. # delete data from server sudo rm -rf /openmldir/* mkdir /openmldir/log # delete database mysqladmin -u \"root\" -p\"yourpassword\" DROP openml_expdb mysql -h localhost -u root -p\"yourpassword\" -e \"TRUNCATE openml.file;\" # reset ES search index echo \"Deleting and recreating the ES index: \" curl -XDELETE http://localhost:9200/openml curl -XPUT 'localhost:9200/openml?pretty' -H 'Content-Type: application/json' -d' { \"settings\" : { \"index\" : { \"number_of_shards\" : 3, \"number_of_replicas\" : 2 } } } ' # go to directory with the website source code cd /var/www/openml.org/public_html/ # reinitiate the database mysql -u root -p\"yourpassword!\" < downloads/openml_expdb.sql # fill important columns sudo php index.php cron install_database # rebuild search index sudo php index.php cron initialize_es_indices sudo php index.php cron build_es_indices sudo chown apache:apache /openmldir/log sudo chown apache:apache /openmldir/log/*","title":"Initialization"},{"location":"MOA/","text":"OpenML features extensive support for MOA. However currently this is implemented as a stand alone MOA compilation, using the latest version (as of May, 2014). Download MOA for OpenML Quick Start \u00b6 Download the standalone MOA environment above. Find your API key in your profile (log in first). Create a config file called openml.conf in a .openml directory in your home dir. It should contain the following lines: api_key = YOUR_KEY Launch the JAR file by double clicking on it, or launch from command-line using the following command: java -cp openmlmoa.beta.jar moa.gui.GUI Select the task moa.tasks.openml.OpenmlDataStreamClassification to evaluate a classifier on an OpenML task, and send the results to OpenML. Optionally, you can generate new streams using the Bayesian Network Generator: select the moa.tasks.WriteStreamToArff task, with moa.streams.generators.BayesianNetworkGenerator .","title":"MOA"},{"location":"MOA/#quick-start","text":"Download the standalone MOA environment above. Find your API key in your profile (log in first). Create a config file called openml.conf in a .openml directory in your home dir. It should contain the following lines: api_key = YOUR_KEY Launch the JAR file by double clicking on it, or launch from command-line using the following command: java -cp openmlmoa.beta.jar moa.gui.GUI Select the task moa.tasks.openml.OpenmlDataStreamClassification to evaluate a classifier on an OpenML task, and send the results to OpenML. Optionally, you can generate new streams using the Bayesian Network Generator: select the moa.tasks.WriteStreamToArff task, with moa.streams.generators.BayesianNetworkGenerator .","title":"Quick Start"},{"location":"NET-API/","text":"The .Net API allows you connect to OpenML from .Net applications. Download \u00b6 Stable releases of the .Net API are available via NuGet . Use the NuGet package explorer in the Visual Studia, write \u201cInstall-Package openMl\u201d to the NuGet package manager console or download the whole package from the NuGet website and add it into your project. Or, you can check out the developer version from GitHub . Quick Start \u00b6 Create an OpenmlConnector instance with your api key. You can find this key in your account settings. This will create a client with OpenML functionalities, The functionalities mirror the OpenMlApi and not all of them are (yet) implemented. If you need some feature, don\u2019t hesitate contact us via our Git page. `var connector = new OpenMlConnector(\"YOURAPIKEY\");` All OpenMlConnector methods are documented via the usual .Net comments. Get dataset description \u00b6 `var datasetDescription = connector.GetDatasetDescription(1);` List datasets \u00b6 `var data = connector.ListDatasets();` Get run \u00b6 `var run = connector.GetRun(1);` List task types \u00b6 `var taskTypes = connector.ListTaskTypes();` Get task type \u00b6 `var taskType = connector.GetTaskType(1);` List evaluation measures \u00b6 `var measures = connector.ListEvaluationMeasures();` List estimation procedures \u00b6 `var estimationProcs = connector.ListEstimationProcedures();` Get estimation procedure \u00b6 `var estimationProc = connector.GetEstimationProcedure(1);` List data qualities \u00b6 `var dataQualities = connector.ListDataQualities();`","title":"NET API"},{"location":"NET-API/#download","text":"Stable releases of the .Net API are available via NuGet . Use the NuGet package explorer in the Visual Studia, write \u201cInstall-Package openMl\u201d to the NuGet package manager console or download the whole package from the NuGet website and add it into your project. Or, you can check out the developer version from GitHub .","title":"Download"},{"location":"NET-API/#quick-start","text":"Create an OpenmlConnector instance with your api key. You can find this key in your account settings. This will create a client with OpenML functionalities, The functionalities mirror the OpenMlApi and not all of them are (yet) implemented. If you need some feature, don\u2019t hesitate contact us via our Git page. `var connector = new OpenMlConnector(\"YOURAPIKEY\");` All OpenMlConnector methods are documented via the usual .Net comments.","title":"Quick Start"},{"location":"NET-API/#get-dataset-description","text":"`var datasetDescription = connector.GetDatasetDescription(1);`","title":"Get dataset description"},{"location":"NET-API/#list-datasets","text":"`var data = connector.ListDatasets();`","title":"List datasets"},{"location":"NET-API/#get-run","text":"`var run = connector.GetRun(1);`","title":"Get run"},{"location":"NET-API/#list-task-types","text":"`var taskTypes = connector.ListTaskTypes();`","title":"List task types"},{"location":"NET-API/#get-task-type","text":"`var taskType = connector.GetTaskType(1);`","title":"Get task type"},{"location":"NET-API/#list-evaluation-measures","text":"`var measures = connector.ListEvaluationMeasures();`","title":"List evaluation measures"},{"location":"NET-API/#list-estimation-procedures","text":"`var estimationProcs = connector.ListEstimationProcedures();`","title":"List estimation procedures"},{"location":"NET-API/#get-estimation-procedure","text":"`var estimationProc = connector.GetEstimationProcedure(1);`","title":"Get estimation procedure"},{"location":"NET-API/#list-data-qualities","text":"`var dataQualities = connector.ListDataQualities();`","title":"List data qualities"},{"location":"OpenML-Docs/","text":"General Documentation \u00b6 The general documentation (the one you are reading now) in written in MarkDown, can be easily edited by clicking the edit button (the pencil icon) on the top of every page. It will open up an editing page on GitHub (you do need to be logged in on GitHub). When you are done, add a small message explaining the change and click 'commit changes'. On the next page, just launch the pull request. We will then review it and approve the changes, or discuss them if necessary. The sources are generated by MkDocs , using the Material theme . Check these docs to see what is possible in terms of styling. Deployment To deploy the documentation manually, you need to have MkDocs and MkDocs-Material installed: pip install mkdocs pip install mkdocs-material pip install fontawesome_markdown To deploy the documentation locally, run mkdocs serve in the top directory (with the mkdocs.yml file). Any changes made after that will be hot-loaded. The documentation will be auto-deployed with every push or merge with the master branch of https://www.github.com/openml/docs/ . In the background, a CI job will run mkdocs gh-deploy , which will build the HTML files and push them to the gh-pages branch of openml/docs. https://docs.openml.org is just a reverse proxy for https://openml.github.io/docs/ . REST API \u00b6 The REST API is documented using Swagger.io, in YAML. This generates a nice web interface that also allows trying out the API calls using your own API key (when you are logged in). You can edit the sources on SwaggerHub . When you are done, export to json and replace the downloads/swagger.json file in the OpenML main GitHub repository. You need to do a pull request that is then reviewed by us. When we merge the new file the changes are immediately available. The data API can be edited in the same way. Python API \u00b6 To edit the tutorial, you have to edit the reStructuredText files on openml-python/doc . When done, you can do a pull request. To edit the documentation of the python functions, edit the docstrings in the Python code . When done, you can do a pull request. Note Developers: A CircleCI job will automatically render the documentation on every GitHub commit, using Sphinx . R API \u00b6 To edit the tutorial, you have to edit the Rmarkdown files on openml-r/vignettes . To edit the documentation of the R functions, edit the Roxygen documention next to the functions in the R code . Note Developers: A Travis job will automatically render the documentation on every GitHub commit, using knitr . The Roxygen documentation is updated every time a new version is released on CRAN. Java API \u00b6 The Java Tutorial is written in markdown and can be edited the usual way (see above). To edit the documentation of the Java functions, edit the documentation next to the functions in the Java code . Javadocs: https://www.openml.org/docs/ Note Developers: A Travis job will automatically render the documentation on every GitHub commit, using Javadoc .","title":"Documenting"},{"location":"OpenML-Docs/#general-documentation","text":"The general documentation (the one you are reading now) in written in MarkDown, can be easily edited by clicking the edit button (the pencil icon) on the top of every page. It will open up an editing page on GitHub (you do need to be logged in on GitHub). When you are done, add a small message explaining the change and click 'commit changes'. On the next page, just launch the pull request. We will then review it and approve the changes, or discuss them if necessary. The sources are generated by MkDocs , using the Material theme . Check these docs to see what is possible in terms of styling. Deployment To deploy the documentation manually, you need to have MkDocs and MkDocs-Material installed: pip install mkdocs pip install mkdocs-material pip install fontawesome_markdown To deploy the documentation locally, run mkdocs serve in the top directory (with the mkdocs.yml file). Any changes made after that will be hot-loaded. The documentation will be auto-deployed with every push or merge with the master branch of https://www.github.com/openml/docs/ . In the background, a CI job will run mkdocs gh-deploy , which will build the HTML files and push them to the gh-pages branch of openml/docs. https://docs.openml.org is just a reverse proxy for https://openml.github.io/docs/ .","title":"General Documentation"},{"location":"OpenML-Docs/#rest-api","text":"The REST API is documented using Swagger.io, in YAML. This generates a nice web interface that also allows trying out the API calls using your own API key (when you are logged in). You can edit the sources on SwaggerHub . When you are done, export to json and replace the downloads/swagger.json file in the OpenML main GitHub repository. You need to do a pull request that is then reviewed by us. When we merge the new file the changes are immediately available. The data API can be edited in the same way.","title":"REST API"},{"location":"OpenML-Docs/#python-api","text":"To edit the tutorial, you have to edit the reStructuredText files on openml-python/doc . When done, you can do a pull request. To edit the documentation of the python functions, edit the docstrings in the Python code . When done, you can do a pull request. Note Developers: A CircleCI job will automatically render the documentation on every GitHub commit, using Sphinx .","title":"Python API"},{"location":"OpenML-Docs/#r-api","text":"To edit the tutorial, you have to edit the Rmarkdown files on openml-r/vignettes . To edit the documentation of the R functions, edit the Roxygen documention next to the functions in the R code . Note Developers: A Travis job will automatically render the documentation on every GitHub commit, using knitr . The Roxygen documentation is updated every time a new version is released on CRAN.","title":"R API"},{"location":"OpenML-Docs/#java-api","text":"The Java Tutorial is written in markdown and can be edited the usual way (see above). To edit the documentation of the Java functions, edit the documentation next to the functions in the Java code . Javadocs: https://www.openml.org/docs/ Note Developers: A Travis job will automatically render the documentation on every GitHub commit, using Javadoc .","title":"Java API"},{"location":"OpenML_definition/","text":"OpenML is at its core a database, from which entities can be downloaded and to which entities can be uploaded. Although there are various interfaces for these, at the core all communication with the database goes through the API. In this document, we describe the standard how to upload entities to OpenML and what the resulting database state will be. Data \u00b6 Data is uploaded through the function post data . The following files are needed: description : An XML adhiring to the XSD schema . dataset : An ARFF file containing the data (optional, if not set, there should be an URL in the description, pointing to this file). Uploading any other files will result in an error. Tasks \u00b6 Tasks are uploaded through the function post task . The following files are needed: description : An XML adhering to the XSD schema . Uploading any other files will result in an error. The task file should contain several input fields. These are a name and value combination of fields that are marked to be relevant by the task type definition. There are several task type definitions, e.g.: Supervised Classification Supervised Regression Learning Curve Data Stream Classification Note that the task types themselves are flexible content (ideally users can contribute task types) and therefore the documents are not part of the OpenML definition. The task types define which input fields should be set, when creating a task. Duplicate tasks (i.e., same value for task_type_id and all input fields equal) will be rejected. When creating a task, the API checks for all of the input fields whether the input is legitimate. (Todo: describe the checks and what they depend on). FLow \u00b6 Flows are uploaded through the function post flow . The following file is needed: description : An XML adhering to the XSD schema . Uploading any other files will result in an error. Duplicate flows (i.e., same values for name and external_version ) will be rejected. Runs \u00b6 Runs are uploaded through the function post run . The following files are needed: description : An XML adhering to the XSD schema . predictions : An ARFF file containing the predictions (optional, depending on the task). trace : An ARFF file containing the run trace (optional, depending on the flow). Uploading any other files will result in an error. Predictions \u00b6 The contents of the prediction file depends on the task type. Task type: Supervised classification \u00b6 Example predictions file repeat NUMERIC fold NUMERIC row_id NUMERIC confidence.{$classname}: optional. various columns, describing the confidence per class. The values of these columns should add to 1 (precision 1e-6). (proposal) decision_function.{$classname}: optional. various columns, describing decision function per class. prediction {$classname} Runs that have a different set of columns will be rejected. Trace \u00b6 Example trace file repeat: cross-validation repeat fold: cross-validation fold iteration: the index order within this repeat/fold combination evaluation (float): the evaluation score that was attached based on the validation set selected {True, False}: Whether in this repeat/run combination this was the selected hyperparameter configuration (exactly one should be tagged with True) Per optimized parameter a column that has the name of the parameter and the prefix \"parameter_\" setup_string: Due to legacy reasons accepted, but will be ignored by the default evaluation engine (open question) what is in the same fold/repeat combination the same config is ran multiple times with same evaluation? Traces that have a different set of columns will be rejected. Data Features \u00b6 Data features are uploaded by the Java Evaluation Engine and will be documented later. Data Qualities \u00b6 Data qualities are uploaded by the Java Evaluation Engine and will be documented later. Evaluations \u00b6 Evaluations are uploaded by Java Evaluation Engine and will be documented later. Trace Iterations \u00b6 Trace Iterations are uploaded by Java Evaluation Engine and will be documented later.","title":"OpenML Definition"},{"location":"OpenML_definition/#data","text":"Data is uploaded through the function post data . The following files are needed: description : An XML adhiring to the XSD schema . dataset : An ARFF file containing the data (optional, if not set, there should be an URL in the description, pointing to this file). Uploading any other files will result in an error.","title":"Data"},{"location":"OpenML_definition/#tasks","text":"Tasks are uploaded through the function post task . The following files are needed: description : An XML adhering to the XSD schema . Uploading any other files will result in an error. The task file should contain several input fields. These are a name and value combination of fields that are marked to be relevant by the task type definition. There are several task type definitions, e.g.: Supervised Classification Supervised Regression Learning Curve Data Stream Classification Note that the task types themselves are flexible content (ideally users can contribute task types) and therefore the documents are not part of the OpenML definition. The task types define which input fields should be set, when creating a task. Duplicate tasks (i.e., same value for task_type_id and all input fields equal) will be rejected. When creating a task, the API checks for all of the input fields whether the input is legitimate. (Todo: describe the checks and what they depend on).","title":"Tasks"},{"location":"OpenML_definition/#flow","text":"Flows are uploaded through the function post flow . The following file is needed: description : An XML adhering to the XSD schema . Uploading any other files will result in an error. Duplicate flows (i.e., same values for name and external_version ) will be rejected.","title":"FLow"},{"location":"OpenML_definition/#runs","text":"Runs are uploaded through the function post run . The following files are needed: description : An XML adhering to the XSD schema . predictions : An ARFF file containing the predictions (optional, depending on the task). trace : An ARFF file containing the run trace (optional, depending on the flow). Uploading any other files will result in an error.","title":"Runs"},{"location":"OpenML_definition/#predictions","text":"The contents of the prediction file depends on the task type.","title":"Predictions"},{"location":"OpenML_definition/#task-type-supervised-classification","text":"Example predictions file repeat NUMERIC fold NUMERIC row_id NUMERIC confidence.{$classname}: optional. various columns, describing the confidence per class. The values of these columns should add to 1 (precision 1e-6). (proposal) decision_function.{$classname}: optional. various columns, describing decision function per class. prediction {$classname} Runs that have a different set of columns will be rejected.","title":"Task type: Supervised classification"},{"location":"OpenML_definition/#trace","text":"Example trace file repeat: cross-validation repeat fold: cross-validation fold iteration: the index order within this repeat/fold combination evaluation (float): the evaluation score that was attached based on the validation set selected {True, False}: Whether in this repeat/run combination this was the selected hyperparameter configuration (exactly one should be tagged with True) Per optimized parameter a column that has the name of the parameter and the prefix \"parameter_\" setup_string: Due to legacy reasons accepted, but will be ignored by the default evaluation engine (open question) what is in the same fold/repeat combination the same config is ran multiple times with same evaluation? Traces that have a different set of columns will be rejected.","title":"Trace"},{"location":"OpenML_definition/#data-features","text":"Data features are uploaded by the Java Evaluation Engine and will be documented later.","title":"Data Features"},{"location":"OpenML_definition/#data-qualities","text":"Data qualities are uploaded by the Java Evaluation Engine and will be documented later.","title":"Data Qualities"},{"location":"OpenML_definition/#evaluations","text":"Evaluations are uploaded by Java Evaluation Engine and will be documented later.","title":"Evaluations"},{"location":"OpenML_definition/#trace-iterations","text":"Trace Iterations are uploaded by Java Evaluation Engine and will be documented later.","title":"Trace Iterations"},{"location":"Python-API/","text":"Fallback link for browsers that don't support iframes","title":"Python API"},{"location":"Python-changelog/","text":"Fallback link for browsers that don't support iframes","title":"Python changelog"},{"location":"Python-contributing/","text":"Fallback link for browsers that don't support iframes","title":"Python contributing"},{"location":"Python-examples/","text":"Fallback link for browsers that don't support iframes","title":"Python examples"},{"location":"Python-guide/","text":"Fallback link for browsers that don't support iframes","title":"Python guide"},{"location":"Python-start/","text":"Fallback link for browsers that don't support iframes","title":"Python start"},{"location":"R-API/","text":"Fallback link for browsers that don't support iframes","title":"R API"},{"location":"R-guide/","text":"Fallback link for browsers that don't support iframes","title":"R guide"},{"location":"REST-API/","text":"REST APIs \u00b6 The REST API allows you to talk directly to the OpenML server from any programming environment. The REST API has two parts (with different endpoints): The Main REST API \u00b6 Has all main functions to download OpenML data or share your own. API Documentation The File API \u00b6 Serves datasets and other files stored on OpenML servers. File API Documentation","title":"REST APIs"},{"location":"REST-API/#rest-apis","text":"The REST API allows you to talk directly to the OpenML server from any programming environment. The REST API has two parts (with different endpoints):","title":"REST APIs"},{"location":"REST-API/#the-main-rest-api","text":"Has all main functions to download OpenML data or share your own. API Documentation","title":"The Main REST API"},{"location":"REST-API/#the-file-api","text":"Serves datasets and other files stored on OpenML servers. File API Documentation","title":"The File API"},{"location":"REST-tutorial/","text":"REST tutorial \u00b6 OpenML offers a RESTful Web API, with predictive URLs, for uploading and downloading machine learning resources. Try the API Documentation to see examples of all calls, and test them right in your browser. Getting started \u00b6 REST services can be called using simple HTTP GET or POST actions. The REST Endpoint URL is https://www.openml.org/api/v1/ The default endpoint returns data in XML. If you prefer JSON, use the endpoint https://www.openml.org/api/v1/json/ . Note that, to upload content, you still need to use XML (at least for now). Testing \u00b6 For continuous integration and testing purposes, we have a test server offering the same API, but which does not affect the production server. The test server REST Endpoint URL is https://test.openml.org/api/v1/ Error messages \u00b6 Error messages will look like this: <oml:error xmlns:oml= \"http://openml.org/error\" > <oml:code> 100 </oml:code> <oml:message> Please invoke legal function </oml:message> <oml:additional_information> Additional information, not always available. </oml:additional_information> </oml:error> All error messages are listed in the API documentation. E.g. try to get a non-existing dataset: in XML: https://www.openml.org/api_new/v1/data/99999 in JSON: https://www.openml.org/api_new/v1/json/data/99999 Examples \u00b6 You need to be logged in for these examples to work. Download a dataset \u00b6 User asks for a dataset using the /data/{id} service. The dataset id is typically part of a task, or can be found on OpenML.org. OpenML returns a description of the dataset as an XML file (or JSON). Try it now The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset. The dataset is returned by the server hosting the dataset. This can be OpenML, but also any other data repository. Try it now Download a flow \u00b6 User asks for a flow using the /flow/{id} service and a flow id . The flow id can be found on OpenML.org. OpenML returns a description of the flow as an XML file (or JSON). Try it now The flow description contains the URL where the flow can be downloaded (e.g. GitHub), either as source, binary or both, as well as additional information on history, dependencies and licence. The user calls the right URL to download it. The flow is returned by the server hosting it. This can be OpenML, but also any other code repository. Try it now Download a task \u00b6 User asks for a task using the /task/{id} service and a task id . The task id is typically returned when searching for tasks. OpenML returns a description of the task as an XML file (or JSON). Try it now The task description contains the dataset id (s) of the datasets involved in this task. The user asks for the dataset using the /data/{id} service and the dataset id . OpenML returns a description of the dataset as an XML file (or JSON). Try it now The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset. The dataset is returned by the server hosting it. This can be OpenML, but also any other data repository. Try it now The task description may also contain links to other resources, such as the train-test splits to be used in cross-validation. The user calls that URL to download the train-test splits. The train-test splits are returned by OpenML. Try it now","title":"REST tutorial"},{"location":"REST-tutorial/#rest-tutorial","text":"OpenML offers a RESTful Web API, with predictive URLs, for uploading and downloading machine learning resources. Try the API Documentation to see examples of all calls, and test them right in your browser.","title":"REST tutorial"},{"location":"REST-tutorial/#getting-started","text":"REST services can be called using simple HTTP GET or POST actions. The REST Endpoint URL is https://www.openml.org/api/v1/ The default endpoint returns data in XML. If you prefer JSON, use the endpoint https://www.openml.org/api/v1/json/ . Note that, to upload content, you still need to use XML (at least for now).","title":"Getting started"},{"location":"REST-tutorial/#testing","text":"For continuous integration and testing purposes, we have a test server offering the same API, but which does not affect the production server. The test server REST Endpoint URL is https://test.openml.org/api/v1/","title":"Testing"},{"location":"REST-tutorial/#error-messages","text":"Error messages will look like this: <oml:error xmlns:oml= \"http://openml.org/error\" > <oml:code> 100 </oml:code> <oml:message> Please invoke legal function </oml:message> <oml:additional_information> Additional information, not always available. </oml:additional_information> </oml:error> All error messages are listed in the API documentation. E.g. try to get a non-existing dataset: in XML: https://www.openml.org/api_new/v1/data/99999 in JSON: https://www.openml.org/api_new/v1/json/data/99999","title":"Error messages"},{"location":"REST-tutorial/#examples","text":"You need to be logged in for these examples to work.","title":"Examples"},{"location":"REST-tutorial/#download-a-dataset","text":"User asks for a dataset using the /data/{id} service. The dataset id is typically part of a task, or can be found on OpenML.org. OpenML returns a description of the dataset as an XML file (or JSON). Try it now The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset. The dataset is returned by the server hosting the dataset. This can be OpenML, but also any other data repository. Try it now","title":"Download a dataset"},{"location":"REST-tutorial/#download-a-flow","text":"User asks for a flow using the /flow/{id} service and a flow id . The flow id can be found on OpenML.org. OpenML returns a description of the flow as an XML file (or JSON). Try it now The flow description contains the URL where the flow can be downloaded (e.g. GitHub), either as source, binary or both, as well as additional information on history, dependencies and licence. The user calls the right URL to download it. The flow is returned by the server hosting it. This can be OpenML, but also any other code repository. Try it now","title":"Download a flow"},{"location":"REST-tutorial/#download-a-task","text":"User asks for a task using the /task/{id} service and a task id . The task id is typically returned when searching for tasks. OpenML returns a description of the task as an XML file (or JSON). Try it now The task description contains the dataset id (s) of the datasets involved in this task. The user asks for the dataset using the /data/{id} service and the dataset id . OpenML returns a description of the dataset as an XML file (or JSON). Try it now The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset. The dataset is returned by the server hosting it. This can be OpenML, but also any other data repository. Try it now The task description may also contain links to other resources, such as the train-test splits to be used in cross-validation. The user calls that URL to download the train-test splits. The train-test splits are returned by OpenML. Try it now","title":"Download a task"},{"location":"React/","text":"React App \u00b6 App structure \u00b6 The structure of the source code looks as follows App.js index.js components |-- Sidebar.js |-- Header.js |-- ... layouts |-- Clear.js |-- Main.js pages |-- auth |-- cover |-- docs |-- search routes |-- index.js |-- Routes.js themes The website is designed as a single-page application. The top level files bootstrap the app. index.js simply renders the top component, and App.js adds the relevant subcomponents based on the current theme and state. Routes.js links components to the possible routes (based on the URL). The list of possible routes is defined in routes/index.js . pages contain the various pages of the website. It has subdirectories for: auth : All pages that require authorization (login). These routes are protected. cover : The front page of the website docs : All normal information pages (e.g. 'About', 'API',...) search : All pages related to searching for datasets, tasks, flows, runs, etc. layout contains the possible layouts, Main or Clear (see below). You define the layout of a page by adding its route to either mainRoutes or clearRoutes in routes/index.js . The default is the Main layout. themes contains the overall theme styling for the entire website. Currently, there is a dark and a light theme. They can be set using setTheme in the MainContext, see App.js . Component structure \u00b6 The component structure is shown above, for the Main layout. The App component also holds the state of the website using React's native Context API (see below). Next to the header and sidebar, the main component of the website (in yellow) shows the contents of the current page . In this image, this is the search page, which has several subcomponents as explained below. Search page \u00b6 The search page is structured as follows: SearchPanel : the main search panel. Also contains callbacks for sorting and filtering, and lists what can be filtered or sorted on. FilterBar : The top bar with the search statistics and functionality to add filters and sort results SearchResultsPanel : The list of search results on the left. It shows a list of Card elements which are uniformly styled but their contents may vary. Depending on the selected type of result (selected in the left navigation bar) it is instantiated with different properties. E.g. a DataListPanel is a simple wrapper around SearchResultsPanel which defines the dataset-specific statistics to be shown in the cards. Search tabs: The tabs that allow you to choose between different aspects of the results (Statistics, Overview (Dash)) or the different views on the selected dataset, task, etc. (Details, Analysis (Dash),...) ItemDetail : When a search result is selected, this will show the details of the selection, e.g. the dataset details. Depending on the passed type prop, it will render the Dataset , Task , ... component. The api.js file contains the search function, which translates a search query, filters, and other constraints into an ElasticSearch query and returns the results. Style guide \u00b6 To keep a consistent style and minimize dependencies and complexity, we build on Material UI components and FontAwesome icons. Theming is defined in themes/index.js and loaded in as a context ( ThemeContext ) in App.js . More specific styling is always defined through styled components in the corresponding pages. Layouts \u00b6 There are two top level layouts: Main loads the main layout with a Sidebar , Header , and a certain page with all the contents. The Clear.js layout has no headers or sidebars, but has a colored gradient background. It is used mainly for user login and registration or other quick forms. The layout of the page content should use the Material UI grid layout . This makes sure it will adapt to different device screen sizes. Test using your browsers development tools whether the layout adapts correctly to different screens, including recent smartphones. Styled components \u00b6 Any custom styling (beyond the Material UI default styling) is defined in styled components which are defined within the file for each page. Keep this as minimal as possible. Check if you can import styled components already defined for other pages, avoid duplication. Styled div's are defined as follows: const OpenMLTitle = styled . div ` color: white; font-size: 3em; ` ; Material UI components can be styled the same way: const WhiteButton = styled ( Button ) ` display: inline-block; color: #fff; ` ; Color palette \u00b6 We follow the general Material UI color palette with shade 400, except when that doesn't give sufficient contrast. The main colors used (e.g. for the icons in the sidebar are: 'green[400]', 'yellow[700]', 'blue[800]', 'red[400]', 'purple[400]', 'orange[400]', 'grey[400]'. Backgrounds are generally kept white (or dark grey for the dark theme). The global context (see below) has a getColor function to get the colors of the search types, e.g. context.getColor(\"run\") returns red[400] . Handling state \u00b6 There are different levels of state management: Global state is handled via React's native Context API (we don't use Redux). Contexts are defined in the component tree where needed (usually higher up) by a context provider component, and is accessed lower in the component tree by a context consumer. For instance, see the ThemeContext.Provider in App.js and the ThemeContext.Consumer in Sidebar.js . There is a MainContext which contains global state values such as the logged in user details, and the current state of the search. Lower level components can pass state to their child components via props. Local state changes should, when possible, be defined by React Hooks. Note that changing the global state will re-render the entire website. Hence, do this only when necessary. State and search \u00b6 Most global state variables have to do with search. The search pages typically work by changing the query and filters variables (see App.js ). There is a setSearch function in the main context that can be called to change the search parameters. It checks whether the query has changed and whether updating the global state and re-rendering the website is necessary. Lifecycle Methods \u00b6 These are the React lifecycle methods and how we use them. When a component mounts, methods 1,2,4,7 will be called. When it updates, methods 2-6 will be called. constructor(): Set the initial state of the components getDerivedStateFromProps(props, state): Static method, only for changing the local state based on props. It returns the new state. shouldComponentUpdate(nextProps, nextState): Decides whether a state change requires a re-rendering or not. Used to optimize performance. render(): Returns the JSX to be rendered. It should NOT change the state. getSnapshotBeforeUpdate(prevProps,prevState): Used to save 'old' DOM information right before an update. Returns a 'snapshot'. componentDidUpdate(prevProps,prevState,snapshot): For async requests or other operations right after component update. componentDidMount(): For async requests (e.g. API calls) right after the component mounted. componentWillUnMount(): Cleanup before the component is destroyed. componentDidCatch(error,info): For updating the state after an error is thrown. Forms and Events \u00b6 React wraps native browser events into synthetic events to handle interactions in a cross-browser compatible way. After being wrapped, they are sent to all event handlers, usually defined as callbacks. Note: for performance reasons, synthetic events are pooled and reused, so their properties are nullified after being consumed. If you want to use them asynchronously, you need to call event.persist() . HTML forms are different than other DOM elements because they keep their own state in plain HTML. To make sure that we can control the state we need to set the input field's value to a component state value. Here's an example of using an input field to change the title displayed in the component. const titles : { mainTitle : 'OpenML' }; class App extends Component { this . state = { titles }; // Receive synthetic event onTitleChange = ( event ) => { this . setState ({ titles . mainTitle : event . target . value }); } render (){ return ( < div classname = \"App\" > < h1 > { this . state . titles . mainTitle } < /h1> < form > < input type = \"text\" value = { this . state . titles . mainTitle } // control state onChange = { this . onTitleChange } // event handler callback /> < /form> < /div> ); } }","title":"React frontend"},{"location":"React/#react-app","text":"","title":"React App"},{"location":"React/#app-structure","text":"The structure of the source code looks as follows App.js index.js components |-- Sidebar.js |-- Header.js |-- ... layouts |-- Clear.js |-- Main.js pages |-- auth |-- cover |-- docs |-- search routes |-- index.js |-- Routes.js themes The website is designed as a single-page application. The top level files bootstrap the app. index.js simply renders the top component, and App.js adds the relevant subcomponents based on the current theme and state. Routes.js links components to the possible routes (based on the URL). The list of possible routes is defined in routes/index.js . pages contain the various pages of the website. It has subdirectories for: auth : All pages that require authorization (login). These routes are protected. cover : The front page of the website docs : All normal information pages (e.g. 'About', 'API',...) search : All pages related to searching for datasets, tasks, flows, runs, etc. layout contains the possible layouts, Main or Clear (see below). You define the layout of a page by adding its route to either mainRoutes or clearRoutes in routes/index.js . The default is the Main layout. themes contains the overall theme styling for the entire website. Currently, there is a dark and a light theme. They can be set using setTheme in the MainContext, see App.js .","title":"App structure"},{"location":"React/#component-structure","text":"The component structure is shown above, for the Main layout. The App component also holds the state of the website using React's native Context API (see below). Next to the header and sidebar, the main component of the website (in yellow) shows the contents of the current page . In this image, this is the search page, which has several subcomponents as explained below.","title":"Component structure"},{"location":"React/#search-page","text":"The search page is structured as follows: SearchPanel : the main search panel. Also contains callbacks for sorting and filtering, and lists what can be filtered or sorted on. FilterBar : The top bar with the search statistics and functionality to add filters and sort results SearchResultsPanel : The list of search results on the left. It shows a list of Card elements which are uniformly styled but their contents may vary. Depending on the selected type of result (selected in the left navigation bar) it is instantiated with different properties. E.g. a DataListPanel is a simple wrapper around SearchResultsPanel which defines the dataset-specific statistics to be shown in the cards. Search tabs: The tabs that allow you to choose between different aspects of the results (Statistics, Overview (Dash)) or the different views on the selected dataset, task, etc. (Details, Analysis (Dash),...) ItemDetail : When a search result is selected, this will show the details of the selection, e.g. the dataset details. Depending on the passed type prop, it will render the Dataset , Task , ... component. The api.js file contains the search function, which translates a search query, filters, and other constraints into an ElasticSearch query and returns the results.","title":"Search page"},{"location":"React/#style-guide","text":"To keep a consistent style and minimize dependencies and complexity, we build on Material UI components and FontAwesome icons. Theming is defined in themes/index.js and loaded in as a context ( ThemeContext ) in App.js . More specific styling is always defined through styled components in the corresponding pages.","title":"Style guide"},{"location":"React/#layouts","text":"There are two top level layouts: Main loads the main layout with a Sidebar , Header , and a certain page with all the contents. The Clear.js layout has no headers or sidebars, but has a colored gradient background. It is used mainly for user login and registration or other quick forms. The layout of the page content should use the Material UI grid layout . This makes sure it will adapt to different device screen sizes. Test using your browsers development tools whether the layout adapts correctly to different screens, including recent smartphones.","title":"Layouts"},{"location":"React/#styled-components","text":"Any custom styling (beyond the Material UI default styling) is defined in styled components which are defined within the file for each page. Keep this as minimal as possible. Check if you can import styled components already defined for other pages, avoid duplication. Styled div's are defined as follows: const OpenMLTitle = styled . div ` color: white; font-size: 3em; ` ; Material UI components can be styled the same way: const WhiteButton = styled ( Button ) ` display: inline-block; color: #fff; ` ;","title":"Styled components"},{"location":"React/#color-palette","text":"We follow the general Material UI color palette with shade 400, except when that doesn't give sufficient contrast. The main colors used (e.g. for the icons in the sidebar are: 'green[400]', 'yellow[700]', 'blue[800]', 'red[400]', 'purple[400]', 'orange[400]', 'grey[400]'. Backgrounds are generally kept white (or dark grey for the dark theme). The global context (see below) has a getColor function to get the colors of the search types, e.g. context.getColor(\"run\") returns red[400] .","title":"Color palette"},{"location":"React/#handling-state","text":"There are different levels of state management: Global state is handled via React's native Context API (we don't use Redux). Contexts are defined in the component tree where needed (usually higher up) by a context provider component, and is accessed lower in the component tree by a context consumer. For instance, see the ThemeContext.Provider in App.js and the ThemeContext.Consumer in Sidebar.js . There is a MainContext which contains global state values such as the logged in user details, and the current state of the search. Lower level components can pass state to their child components via props. Local state changes should, when possible, be defined by React Hooks. Note that changing the global state will re-render the entire website. Hence, do this only when necessary.","title":"Handling state"},{"location":"React/#state-and-search","text":"Most global state variables have to do with search. The search pages typically work by changing the query and filters variables (see App.js ). There is a setSearch function in the main context that can be called to change the search parameters. It checks whether the query has changed and whether updating the global state and re-rendering the website is necessary.","title":"State and search"},{"location":"React/#lifecycle-methods","text":"These are the React lifecycle methods and how we use them. When a component mounts, methods 1,2,4,7 will be called. When it updates, methods 2-6 will be called. constructor(): Set the initial state of the components getDerivedStateFromProps(props, state): Static method, only for changing the local state based on props. It returns the new state. shouldComponentUpdate(nextProps, nextState): Decides whether a state change requires a re-rendering or not. Used to optimize performance. render(): Returns the JSX to be rendered. It should NOT change the state. getSnapshotBeforeUpdate(prevProps,prevState): Used to save 'old' DOM information right before an update. Returns a 'snapshot'. componentDidUpdate(prevProps,prevState,snapshot): For async requests or other operations right after component update. componentDidMount(): For async requests (e.g. API calls) right after the component mounted. componentWillUnMount(): Cleanup before the component is destroyed. componentDidCatch(error,info): For updating the state after an error is thrown.","title":"Lifecycle Methods"},{"location":"React/#forms-and-events","text":"React wraps native browser events into synthetic events to handle interactions in a cross-browser compatible way. After being wrapped, they are sent to all event handlers, usually defined as callbacks. Note: for performance reasons, synthetic events are pooled and reused, so their properties are nullified after being consumed. If you want to use them asynchronously, you need to call event.persist() . HTML forms are different than other DOM elements because they keep their own state in plain HTML. To make sure that we can control the state we need to set the input field's value to a component state value. Here's an example of using an input field to change the title displayed in the component. const titles : { mainTitle : 'OpenML' }; class App extends Component { this . state = { titles }; // Receive synthetic event onTitleChange = ( event ) => { this . setState ({ titles . mainTitle : event . target . value }); } render (){ return ( < div classname = \"App\" > < h1 > { this . state . titles . mainTitle } < /h1> < form > < input type = \"text\" value = { this . state . titles . mainTitle } // control state onChange = { this . onTitleChange } // event handler callback /> < /form> < /div> ); } }","title":"Forms and Events"},{"location":"Website/","text":"Installation \u00b6 The OpenML website runs on Flask , React , and Dash . You need to install these first. Download or clone the source code for the OpenML website from GitHub . Then, go into that folder (it should have the requirements.txt and package.json files). git clone https : // github . com / openml / openml . org . git cd openml . org Install Flask, Dash, and dependencies using PIP pip install - r requirements . txt Install React and dependencies using NPM (8 or higher) cd server / src / client / app / npm install Building and running \u00b6 Go back to the home directory. Build a production version of the website with: npm run build -- prefix server / src / client / app / Start the server by running: flask run You should now see the app running in your browser at localhost:5000 Note: If you run the app using HTTPS, add the SSL context or use 'adhoc' to use on-the-fly certificates or you can specify your own certificates. flask run -- cert = 'adhoc' As flask server is not suitable for production we recommend you to use some other server if you want to deploy your openml installation in production. We currently use gunicorn for production server. You can install the gunicorn server and run it: gunicorn --certfile cert.pem --keyfile key.pem -b localhost:5000 autoapp:app Development \u00b6 To start the React frontend in developer mode, go to server/src/client/app and run: npm run start The app should automatically open at localhost:3000 and any changes made to the code will automatically reload the website (hot loading). Structure \u00b6 The website is built on the following components: A Flask backend . Written in Python, the backend takes care of all communication with the OpenML server. It builds on top of the OpenML Python API. It also takes care of user authentication and keeps the search engine (ElasticSearch) up to date with the latest information from the server. Files are located in the server folder. A React frontend . Written in JavaScript, this takes care of rendering the website. It pulls in information from the search engine, and shows plots rendered by Dash. It also contains forms (e.g. for logging in or uploading new datasets), which will be sent off to the backend for processing. Files are located in server/src/client/app . Dash dashboards . Written in Python, Dash is used for writing interactive plots. It pulls in data from the Python API, and renders the plots as React components. Files are located in server/src/dashboard .","title":"Getting started"},{"location":"Website/#installation","text":"The OpenML website runs on Flask , React , and Dash . You need to install these first. Download or clone the source code for the OpenML website from GitHub . Then, go into that folder (it should have the requirements.txt and package.json files). git clone https : // github . com / openml / openml . org . git cd openml . org Install Flask, Dash, and dependencies using PIP pip install - r requirements . txt Install React and dependencies using NPM (8 or higher) cd server / src / client / app / npm install","title":"Installation"},{"location":"Website/#building-and-running","text":"Go back to the home directory. Build a production version of the website with: npm run build -- prefix server / src / client / app / Start the server by running: flask run You should now see the app running in your browser at localhost:5000 Note: If you run the app using HTTPS, add the SSL context or use 'adhoc' to use on-the-fly certificates or you can specify your own certificates. flask run -- cert = 'adhoc' As flask server is not suitable for production we recommend you to use some other server if you want to deploy your openml installation in production. We currently use gunicorn for production server. You can install the gunicorn server and run it: gunicorn --certfile cert.pem --keyfile key.pem -b localhost:5000 autoapp:app","title":"Building and running"},{"location":"Website/#development","text":"To start the React frontend in developer mode, go to server/src/client/app and run: npm run start The app should automatically open at localhost:3000 and any changes made to the code will automatically reload the website (hot loading).","title":"Development"},{"location":"Website/#structure","text":"The website is built on the following components: A Flask backend . Written in Python, the backend takes care of all communication with the OpenML server. It builds on top of the OpenML Python API. It also takes care of user authentication and keeps the search engine (ElasticSearch) up to date with the latest information from the server. Files are located in the server folder. A React frontend . Written in JavaScript, this takes care of rendering the website. It pulls in information from the search engine, and shows plots rendered by Dash. It also contains forms (e.g. for logging in or uploading new datasets), which will be sent off to the backend for processing. Files are located in server/src/client/app . Dash dashboards . Written in Python, Dash is used for writing interactive plots. It pulls in data from the Python API, and renders the plots as React components. Files are located in server/src/dashboard .","title":"Structure"},{"location":"Weka/","text":"OpenML is integrated in the Weka (Waikato Environment for Knowledge Analysis) Experimenter and the Command Line Interface. Installation \u00b6 OpenML is available as a weka extension in the package manager: Download the latest version (3.7.13 or higher). Launch Weka, or start from commandline: java -jar weka.jar If you need more memory (e.g. 1GB), start as follows: java -Xmx1G -jar weka.jar Open the package manager (Under 'Tools') Select package OpenmlWeka and click install. Afterwards, restart WEKA. From the Tools menu, open the 'OpenML Experimenter'. Graphical Interface \u00b6 You can solve OpenML Tasks in the Weka Experimenter, and automatically upload your experiments to OpenML (or store them locally). From the Tools menu, open the 'OpenML Experimenter'. Enter your API key in the top field (log in first). You can also store this in a config file (see below). In the 'Tasks' panel, click the 'Add New' button to add new tasks. Insert the task id's as comma-separated values (e.g., '1,2,3,4,5'). Use the search function on OpenML to find interesting tasks and click the ID icon to list the ID's. In the future this search will also be integrated in WEKA. Add algorithms in the \"Algorithm\" panel. Go to the \"Run\" tab, and click on the \"Start\" button. The experiment will be executed and sent to OpenML.org. The runs will now appear on OpenML.org. You can follow their progress and check for errors on your profile page under 'Runs'. CommandLine Interface \u00b6 The Command Line interface is useful for running experiments automatically on a server, without using a GUI. Create a config file called openml.conf in a new directory called .openml in your home dir. It should contain the following line: api_key = YOUR_KEY Execute the following command: java -cp weka.jar openml.experiment.TaskBasedExperiment -T -C -- For example, the following command will run Weka's J48 algorithm on Task 1: java -cp OpenWeka.beta.jar openml.experiment.TaskBasedExperiment -T 1 -C weka.classifiers.trees.J48 The following suffix will set some parameters of this classifier: -- -C 0.25 -M 2 API reference \u00b6 Check the Weka integration Java Docs for more details about the possibilities. Issues \u00b6 Please report any bugs that you may encounter in the issue tracker: https://github.com/openml/openml-weka Or email to j.n.van.rijn@liacs.leidenuniv.nl","title":"WEKA"},{"location":"Weka/#installation","text":"OpenML is available as a weka extension in the package manager: Download the latest version (3.7.13 or higher). Launch Weka, or start from commandline: java -jar weka.jar If you need more memory (e.g. 1GB), start as follows: java -Xmx1G -jar weka.jar Open the package manager (Under 'Tools') Select package OpenmlWeka and click install. Afterwards, restart WEKA. From the Tools menu, open the 'OpenML Experimenter'.","title":"Installation"},{"location":"Weka/#graphical-interface","text":"You can solve OpenML Tasks in the Weka Experimenter, and automatically upload your experiments to OpenML (or store them locally). From the Tools menu, open the 'OpenML Experimenter'. Enter your API key in the top field (log in first). You can also store this in a config file (see below). In the 'Tasks' panel, click the 'Add New' button to add new tasks. Insert the task id's as comma-separated values (e.g., '1,2,3,4,5'). Use the search function on OpenML to find interesting tasks and click the ID icon to list the ID's. In the future this search will also be integrated in WEKA. Add algorithms in the \"Algorithm\" panel. Go to the \"Run\" tab, and click on the \"Start\" button. The experiment will be executed and sent to OpenML.org. The runs will now appear on OpenML.org. You can follow their progress and check for errors on your profile page under 'Runs'.","title":"Graphical Interface"},{"location":"Weka/#commandline-interface","text":"The Command Line interface is useful for running experiments automatically on a server, without using a GUI. Create a config file called openml.conf in a new directory called .openml in your home dir. It should contain the following line: api_key = YOUR_KEY Execute the following command: java -cp weka.jar openml.experiment.TaskBasedExperiment -T -C -- For example, the following command will run Weka's J48 algorithm on Task 1: java -cp OpenWeka.beta.jar openml.experiment.TaskBasedExperiment -T 1 -C weka.classifiers.trees.J48 The following suffix will set some parameters of this classifier: -- -C 0.25 -M 2","title":"CommandLine Interface"},{"location":"Weka/#api-reference","text":"Check the Weka integration Java Docs for more details about the possibilities.","title":"API reference"},{"location":"Weka/#issues","text":"Please report any bugs that you may encounter in the issue tracker: https://github.com/openml/openml-weka Or email to j.n.van.rijn@liacs.leidenuniv.nl","title":"Issues"},{"location":"altmetrics/","text":"To encourage open science, OpenML now includes a score system to track and reward scientific activity, reach and impact, and in the future will include further gamification features such as badges. Because the system is still experimental and very much in development, the details are subject to change. Below, the score system is described in more detailed followed by our rationale for this system for those interested. If anything is unclear or you have any feedback of the system do not hesitate to let us know. The scores \u00b6 All scores are awarded to users and involve datasets, flows, tasks and runs, or knowledge pieces in short. Activity Activity score is awarded to users for contributing to the knowledge base of OpenML. This includes uploading knowledge pieces, leaving likes and downloading new knowledge pieces. Uploads are rewarded strongest, with 3 activity, followed by likes, with 2 activity, and downloads are rewarded the least, with 1 activity. Reach Reach score is awarded to knowledge pieces and by extension their uploaders for the expressed interest of other users. It is increased by 2 for every user that leaves a like on a knowledge piece and increased by 1 for every user that downloads it for the first time. Impact Impact score is awarded to knowledge pieces and by extension their uploaders for the reuse of these knowledge pieces. A dataset is reused if when it is used as input in a task while flows and tasks are reused in runs. 1 Impact is awarded for every reuse by a user that is not the uploader. Impact of a reused knowledge piece is further increased by half of the acquired reach and half of the acquired impact of a reuse, usually rounded down. So the impact of a dataset that is used in a single task with reach 10 and impact 5, is 8 (\u230a1+0.5*10+0.5*5 \u230b). The rationale \u00b6 One of OpenML's core ideas is to create an open science environment for sharing and exploration of knowledge while getting credit for your work. The activity score serves the encouragement of sharing and exploration. Reach makes exploration easier (by finding well liked, and/or often downloaded knowledge pieces), while also providing a form of credit to the user. Impact is another form of credit that is closer in concept to citation scores. Where to find it \u00b6 The number of likes and downloads as well as the reach and impact of knowledge pieces can be found on the top of their respective pages, for example the Iris data set . In the top right you will also find the new Like button next to the already familiar download button. When searching for knowledge pieces on the search page , you will now be able to see the statistics mentioned above as well. In addition you can sort the search results on their downloads, likes, reach or impact. On user profiles you will find all statistics relevant to that user, as well as graphs of their progress on the three scores. Badges \u00b6 Badges are intended to provide discrete goals for users to aim for. They are only in a conceptual phase, depending on the community's reaction they will be further developed. The badges a user has acquired can be found on their user profile below the score graphs. The currently implemented badges are: Clockwork Scientist For being active every day for a period of time. Team Player For collaborating with other users; reusing a knowledge piece of someone who has reused a knowledge piece of yours. Good News Everyone For achieving a high reach on singular knowledge piece you uploaded. Downvotes \u00b6 Although not part of the scores, downvotes have also been introduced. They are intended to indicate a flaw of a data set, flow, task or run that can be fixed, for example a missing description. If you want to indicate something is wrong with a knowledge piece, click the number of issues statistic at the top the page. A panel will open where you either agree with an already raised issue anonymously or submit your own issue (not anonymously). You can also sort search results by the number of downvotes, or issues on the search page . Opting out \u00b6 If you really do not like the altmetrics you can opt-out by changing the setting on your profile. This hides your scores and badges from other users and hides their scores and badges from you. You will still be able to see the number of likes, downloads and downvotes on knowledge pieces, and your likes, downloads and downvotes will still be counted.","title":"Altmetrics"},{"location":"altmetrics/#the-scores","text":"All scores are awarded to users and involve datasets, flows, tasks and runs, or knowledge pieces in short.","title":"The scores"},{"location":"altmetrics/#the-rationale","text":"One of OpenML's core ideas is to create an open science environment for sharing and exploration of knowledge while getting credit for your work. The activity score serves the encouragement of sharing and exploration. Reach makes exploration easier (by finding well liked, and/or often downloaded knowledge pieces), while also providing a form of credit to the user. Impact is another form of credit that is closer in concept to citation scores.","title":"The rationale"},{"location":"altmetrics/#where-to-find-it","text":"The number of likes and downloads as well as the reach and impact of knowledge pieces can be found on the top of their respective pages, for example the Iris data set . In the top right you will also find the new Like button next to the already familiar download button. When searching for knowledge pieces on the search page , you will now be able to see the statistics mentioned above as well. In addition you can sort the search results on their downloads, likes, reach or impact. On user profiles you will find all statistics relevant to that user, as well as graphs of their progress on the three scores.","title":"Where to find it"},{"location":"altmetrics/#badges","text":"Badges are intended to provide discrete goals for users to aim for. They are only in a conceptual phase, depending on the community's reaction they will be further developed. The badges a user has acquired can be found on their user profile below the score graphs. The currently implemented badges are: Clockwork Scientist For being active every day for a period of time. Team Player For collaborating with other users; reusing a knowledge piece of someone who has reused a knowledge piece of yours. Good News Everyone For achieving a high reach on singular knowledge piece you uploaded.","title":"Badges"},{"location":"altmetrics/#downvotes","text":"Although not part of the scores, downvotes have also been introduced. They are intended to indicate a flaw of a data set, flow, task or run that can be fixed, for example a missing description. If you want to indicate something is wrong with a knowledge piece, click the number of issues statistic at the top the page. A panel will open where you either agree with an already raised issue anonymously or submit your own issue (not anonymously). You can also sort search results by the number of downvotes, or issues on the search page .","title":"Downvotes"},{"location":"altmetrics/#opting-out","text":"If you really do not like the altmetrics you can opt-out by changing the setting on your profile. This hides your scores and badges from other users and hides their scores and badges from you. You will still be able to see the number of likes, downloads and downvotes on knowledge pieces, and your likes, downloads and downvotes will still be counted.","title":"Opting out"},{"location":"benchmark/","text":"Benchmarking suites \u00b6 Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. OpenML aims to facilitate the creation of curated, comprehensive suites of machine learning tasks, covering precise sets of conditions. Seamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier: all datasets are uniformly formatted in standardized data formats they can be easily downloaded programmatically through APIs and client libraries they come with machine-readable meta-information , such as the occurrence of missing values, to train algorithms correctly standardized train-test splits are provided to ensure that results can be objectively compared results can be shared in a reproducible way through the APIs results from other users can be easily downloaded and reused Software interfaces \u00b6 To use OpenML Benchmark suites, you can use bindings in several programming languages. These all interface with the OpenML REST API. The default endpoint for this is https://www.openml.org/api/v1/ , but this can change when later versions of the API are released. To use the code examples below, you only need a recent version of one of the following libraries: OpenML Java ApiConnector (version 1.0.22 and up). OpenML Weka (version 0.9.6 and up). This package adds a Weka Integration. OpenML Python (version 0.9.0 and up) OpenML R (version 1.8 and up) Using OpenML Benchmark Suites \u00b6 Below are walk-through instructions for common use cases, as well as code examples. These illustrations use the reference OpenML-CC18 benchmark suite, but you can replace it with any other benchmark suite. Note that a benchmark suite is a set of OpenML tasks , which envelop not only a specific dataset, but also the train-test splits and (for predictive tasks) the target feature. Terminology and current status Benchmark suites are sets of OpenML tasks that you can create and manage yourself. At the same time, it is often useful to also share the set of experiments (runs) with the ensuing benchmarking results. For legacy reasons, such sets of tasks or runs are called studies in the OpenML REST API. In the OpenML bindings (Python, R, Java,...) these are called either sets or studies . When benchmarking, you will probably use two types of sets: Sets of tasks. These can be created, edited, downloaded or deleted via the OpenML API. Website forms will be added soon. Also the set of underlying datasets can be easily retrieved via the API. Sets of runs. Likewise, these can be created, edited, downloaded or deleted via the OpenML API. On the website, these are currently simply called 'studies'. Also the set of underlying tasks, datasets and flows can be easily retrieved. It is possible to link a set of runs to a benchmark study, aimed to collect future runs on that specific set of tasks. Additional information on these will be provided in a separate page. Listing the benchmark suites \u00b6 The current list of benchmark suites is explicitly listed on the bottom of this page. The list of all sets of tasks can also be fetched programmatically. This list includes the suite's ID (and optionally an alias), which can be used to fetch further details. Via the REST API, the list is returned in XML or JSON REST https://www.openml.org/api/v1/xml/study/list/main_entity_type/task/status/all Check out the API docs Python example import openml # using the main entity type task, only benchmark suites are returned # each benchmark suite has an ID, some also have an alias. These can be # used to obtain the full details. studies = openml . study . list_suites ( status = 'all' ) Java example public void listBenchmarksuites () throws Exception { OpenmlConnector openml = new OpenmlConnector (); Map < String , String > filters = new TreeMap < String , String > (); filters . put ( \"status\" , \"all\" ); filters . put ( \"main_entity_type\" , \"task\" ); filters . put ( \"limit\" , \"20\" ); StudyList list = openml . studyList ( filters ); } R example studies = listOMLStudies() Fetching details \u00b6 Using the ID or alias of a benchmark suite, you can retrieve a description and the full list of tasks and the underlying datasets. Via the REST API, a list of all tasks and dataset IDs is returned in XML or JSON REST https://www.openml.org/api/v1/xml/study/OpenML-CC18 Check out the API docs In Python, the data is returned as features, targets numpy arrays: Python example import openml benchmark_suite = openml . study . get_suite ( 'OpenML-CC18' ) # obtain the benchmark suite for task_id in benchmark_suite . tasks : # iterate over all tasks task = openml . tasks . get_task ( task_id ) # download the OpenML task features , targets = task . get_X_and_y () # get the data In Java, the data is returned as a WEKA Instances object: Java example public void downloadDatasets () throws Exception { OpenmlConnector openml = new OpenmlConnector (); Study benchmarksuite = openml . studyGet ( \"OpenML-CC18\" , \"tasks\" ); for ( Integer taskId : benchmarksuite . getTasks ()) { // iterate over all tasks Task t = openml . taskGet ( taskId ); // download the OpenML task // note that InstanceHelper is part of the OpenML-weka package Instances d = InstancesHelper . getDatasetFromTask ( openml , t ); // obtain the dataset } } In R, the data is returned as an R dataframe: R example library ( OpenML ) task.ids = getOMLStudy ( 'OpenML-CC18' ) $ tasks $ task.id # obtain the list of suggested tasks for ( task.id in task.ids ) { # iterate over all tasks task = getOMLTask ( task.id ) # download single OML task data = as.data.frame ( task ) # obtain raw data set Running and sharing benchmarks \u00b6 The code below demonstrates how OpenML benchmarking suites can be conveniently imported for benchmarking using the Python, Java and R APIs. First, the list of tasks is downloaded as already illustrated above. Next, a specific algorithm (or pipeline) can be run on each of them. The OpenML API will automatically evaluate the algorithm using the pre-set train-test splits and store the predictions and scores in a run object. This run object can then be immediately published, pushing the results to the OpenML server, so that they can be compared against all others on the same benchmark set. Uploading results requires an OpenML API key, which can be found in your account details after logging into the OpenML website. REST Requires POST requests: Attaching a new run to a benchmark_study Detaching a run from benchmark_study Python example import openml import sklearn openml . config . apikey = 'FILL_IN_OPENML_API_KEY' # set the OpenML Api Key benchmark_suite = openml . study . get_suite ( 'OpenML-CC18' ) # obtain the benchmark suite # build a scikit-learn classifier clf = sklearn . pipeline . make_pipeline ( sklearn . preprocessing . Imputer (), sklearn . tree . DecisionTreeClassifier ()) for task_id in benchmark_suite . tasks : # iterate over all tasks task = openml . tasks . get_task ( task_id ) # download the OpenML task run = openml . runs . run_model_on_task ( clf , task ) # run the classifier on the task score = run . get_metric_score ( sklearn . metrics . accuracy_score ) # print accuracy score print ( 'Data set: %s ; Accuracy: %0.2f ' % ( task . get_dataset () . name , score . mean ())) run . publish () # publish the experiment on OpenML (optional, requires internet and an API key) print ( 'URL for run: %s /run/ %d ' % ( openml . config . server , run . run_id )) Java example public static void runTasksAndUpload () throws Exception { OpenmlConnector openml = new OpenmlConnector (); openml . setApiKey ( \"FILL_IN_OPENML_API_KEY\" ); // obtain the benchmark suite Study benchmarksuite = openml . studyGet ( \"OpenML-CC18\" , \"tasks\" ); Classifier tree = new REPTree (); // build a Weka classifier for ( Integer taskId : benchmarksuite . getTasks ()) { // iterate over all tasks Task t = openml . taskGet ( taskId ); // download the OpenML task Instances d = InstancesHelper . getDatasetFromTask ( openml , t ); // obtain the dataset int runId = RunOpenmlJob . executeTask ( openml , new WekaConfig (), taskId , tree ); Run run = openml . runGet ( runId ); // retrieve the uploaded run } } R example library ( OpenML ) setOMLConfig ( apikey = 'FILL_IN_OPENML_API_KEY' ) lrn = makeLearner ( 'classif.rpart' ) # construct a simple CART classifier task.ids = getOMLStudy ( 'OpenML-CC18' ) $ tasks $ task.id # obtain the list of suggested tasks for ( task.id in task.ids ) { # iterate over all tasks task = getOMLTask ( task.id ) # download single OML task data = as.data.frame ( task ) # obtain raw data set run = runTaskMlr ( task , learner = lrn ) # run constructed learner upload = uploadOMLRun ( run ) # upload and tag the run } Retrieving runs on a benchmarking suites: \u00b6 Once a benchmark suite has been created, the listing functions can be used to obtain all results on the benchmark suite. Note that there are several other ways to select and bundle runs together. This will be featured in a separate article on reproducible benchmarks. REST (TODO) https://www.openml.org/api/v1/xml/run/list/study/OpenML-CC18 Check out the API docs Python example benchmark_suite = openml . study . get_suite ( 'OpenML-CC18' ) runs = openml . runs . list_runs ( task = benchmark_suite . tasks , limit = 1000 ) Java example public void downloadResultsBenchmarkSuite () throws Exception { Study benchmarkSuite = openml . studyGet ( \"OpenML100\" , \"tasks\" ); Map < String , List < Integer >> filters = new TreeMap < String , List < Integer >> (); filters . put ( \"task\" , Arrays . asList ( benchmarkSuite . getTasks ())); RunList rl = openml . runList ( filters , 200 , null ); assertTrue ( rl . getRuns (). length > 0 ); } R example benchmark.suite = getOMLStudy ( study = \"OpenML-CC18\" ) run.ids = extractOMLStudyIds ( benchmark.suite , \"run.id\" ) runs = rbindlist ( lapply ( run.ids , function ( id ) listOMLRuns ( run.id = id ))) # TODO waiting for REST API Creating new benchmark suites \u00b6 Additional OpenML benchmark suites can be created by defining the precise set of tasks, as well as a textual description. New datasets first need to be registered on OpenML and tasks need to be created on them. We have provided a GitHub repository with additional tools and scripts to build new benchmark studies, e.g. to select all datasets adhering to strict conditions, and to analyse bencharking results. REST Requires POST requests: Creating a benchmark suite Python example import openml # find 250 tasks that we are interested in, e.g., the tasks that have between # 100 and 10000 instances and between 4 and 20 attributes tasks = openml . tasks . list_tasks ( number_instances = '100..10000' , number_features = '4..20' , size = 250 ) task_ids = list ( tasks . keys ()) # create the benchmark suite # the arguments are the alias, name, description, and list of task_ids, respectively. study = openml . study . create_benchmark_suite ( name = \"MidSize Suite\" , alias = None , description = \"illustrating how to create a benchmark suite\" , task_ids = task_ids , ) study_id = study . publish () Java example public void createBenchmarkSuite () throws Exception { OpenmlConnector openml = new OpenmlConnector ( \"FILL_IN_OPENML_API_KEY\" ); // find 250 tasks that we are interested in, e.g., the tasks that have between // 100 and 10000 instances and between 4 and 20 attributes Map < String , String > filtersOrig = new TreeMap < String , String > (); filtersOrig . put ( \"number_instances\" , \"100..10000\" ); filtersOrig . put ( \"number_features\" , \"4..20\" ); filtersOrig . put ( \"limit\" , \"250\" ); Tasks tasksOrig = client_write_test . taskList ( filtersOrig ); // create the study Study study = new Study ( null , \"test\" , \"test\" , null , tasksOrig . getTaskIds (), null ); int studyId = openml . studyUpload ( study ); } R example # find 250 tasks with 100 and 10000 instances and between 4 and 20 attributes tid = listOMLTasks ( number.of.instances = c ( 100 , 10000 ), number.of.features = c ( 4 , 20 ), limit = 250 ) study = makeOMLStudy ( alias = \"test_alias\" , name = \"Test Upload from R\" , description = \"Just testing\" , task.id = tid $ task.id ) id = uploadOMLStudy ( study ) Updating a benchmark suite \u00b6 You can add tasks to a benchmark suite, or remove them. REST Requires POST requests: Attaching a new task Detaching a task Python example import openml # find 250 tasks that we are interested in, e.g., the tasks that have between # 100 and 10000 instances and between 4 and 20 attributes tasks = openml . tasks . list_tasks ( number_instances = '100..10000' , number_features = '4..20' , size = 250 ) task_ids = list ( tasks . keys ()) # create the benchmark suite study = openml . study . create_benchmark_suite ( name = \"MidSize Suite\" , alias = None , description = \"illustrating how to create a benchmark suite\" , task_ids = task_ids , ) study_id = study . publish () # download the study from the server, for verification purposes study = openml . study . get_study ( study_id ) # until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset: tasks_new = openml . tasks . list_tasks ( data_name = 'letter' , size = 1 ) task_ids_new = list ( tasks_new . keys ()) openml . study . attach_to_study ( study_id , task_ids_new ) # or even remove these again openml . study . detach_from_study ( study_id , task_ids_new ) # redownload the study study_prime = openml . study . get_study ( study_id ) assert ( study . tasks == study_prime . tasks ) assert ( study . data == study_prime . data ) Java example public void attachDetachStudy () throws Exception { OpenmlConnector openml = new OpenmlConnector ( \"FILL_IN_OPENML_API_KEY\" ); // find 250 tasks that we are interested in, e.g., the tasks that have between // 100 and 10000 instances and between 4 and 20 attributes Map < String , String > filtersOrig = new TreeMap < String , String > (); filtersOrig . put ( \"number_instances\" , \"100..10000\" ); filtersOrig . put ( \"number_features\" , \"4..20\" ); filtersOrig . put ( \"limit\" , \"250\" ); Tasks tasksOrig = openml . taskList ( filtersOrig ); // create the study Study study = new Study ( null , \"test\" , \"test\" , null , tasksOrig . getTaskIds (), null ); int studyId = openml . studyUpload ( study ); // until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset: Map < String , String > filtersAdd = new TreeMap < String , String > (); filtersAdd . put ( \"data_name\" , \"letter\" ); filtersAdd . put ( \"limit\" , \"1\" ); Tasks tasksAdd = openml . taskList ( filtersAdd ); openml . studyAttach ( studyId , Arrays . asList ( tasksAdd . getTaskIds ())); // or even remove these again openml . studyDetach ( studyId , Arrays . asList ( tasksAdd . getTaskIds ())); // download the study Study studyDownloaded = openml . studyGet ( studyId ); assertArrayEquals ( tasksOrig . getTaskIds (), studyDownloaded . getTasks ()); } R example TODO Further code examples and use cases \u00b6 As mentioned above, we host a GitHub repository with additional tools and scripts to easily create and use new benchmark studies. It includes: A Jupyter Notebook that builds a new benchmark suite with datasets that adhere to strict and complex conditions, as well as automated tests to remove tasks that are too easy for proper benchmarking. A Jupyter Notebook that shows how to pull in the latest state-of-the-art results for any of the benchmark suites A Jupyter Notebook that does a detailed analysis of all results in a benchmark suite, and an example run on the OpenML-CC18. It includes a wide range of plots and rankings to get a deeper insight into the benchmark results. Scripts in Python and R to facilitate common subtasks. We very much welcome new scripts and notebooks, or improvements to the existing ones, that help others to create benchmark suites and analyse benchmarking results. List of benchmarking suites \u00b6 OpenML-CC18 \u00b6 The OpenML-CC18 suite contains all OpenML datasets from mid-2018 that satisfy a large set of clear requirements for thorough yet practical benchmarking. It includes datasets frequently used in benchmarks published over the last years, so it can be used as a drop-in replacement for many benchmarking setups. List of datasets and properties The suite is defined as the set of all verified OpenML datasets that satisfy the following requirements: the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small and not too big, the number of features does not exceed 5000 features to keep the runtime of algorithms low, the target attribute has at least two classes the ratio of the minority class and the majority class is above 0.05, to eliminate highly imbalanced datasets which require special treatment for both algorithms and evaluation measures. We excluded datasets which: are artificially generated (not to confuse with simulated) cannot be randomized via a 10-fold cross-validation due to grouped samples or because they are time series or data streams are a subset of a larger dataset have classes with less than 20 observations have no source or reference available can be perfectly classified by a single attribute or a decision stump allow a decision tree to achieve 100% accuracy on a 10-fold cross-validation task have more than 5000 features after one-hot-encoding categorical features are created by binarization of regression tasks or multiclass classification tasks, or are sparse data (e.g., text mining data sets) Detailed motivation of these decisions We chose the CC18 datasets to allow for practical benchmarking based on the characteristics that might be problematic based on our experience, and to avoid common pitfalls that may invalidate benchmark studies: We used at least 500 data points to allow performing cross-validation while still having a large-enough test split. We limited the datasets to 100.000 data points to allow the algorithms to train machine learning models in a reasonable amount of time. We limited the number of features to 5000 to allow the usage of algorithms which scale unfavourably in the number of features. This limitation, together with the two limitations above aims to allow running all \u201cstandard\u201d machine learning algorithms (naive bayes, linear models, support vector machines, tree-based ensemble methods and neural networks) on the benchmark suite. We required each dataset to have at least two classes to be able to work in a supervised classification setting. We require each class to have at least 20 observations to be able to perform stratified cross-validation where there is at least one observation from each class in each split. We have found that not having all classes present in all training and test sets can make several machine learning packages fail. We require a certain balancedness (ratio of minority class to majority class) to prevent cases where only predicting the majority class would be beneficial. This is most likely the restriction which is most debatable, but we found it very helpful to apply a large set of machine learning algorithms across several libraries to the study. We expect that future studies focus more on imbalanced datasets. Furthermore, we aimed to have the dataset collection as general as possible, rule out as few algorithms as possible and have it usable as easily as possible: We strived to remove artificial datasets as they, for example, come from textbooks and it is hard to reliably assess their difficulty. We admit that there is a blurred line between artificial and simulated datasets and do not have a perfect distinction between them (for example, a lot of phenomena can be simulated, but the outcome might be like a simple, artificial dataset). Therefore, we removed datasets if we were in doubt of whether they are simulated or artificial. We removed datasets which require grouped sampling because they are time series or data streams which should be treated with special care by machine learning algorithms (i.e., taking the time aspect into account). To be on the safe side, we also removed datasets where each sample constitutes a single data stream. We removed datasets which are a subset of larger datasets. Allowing subsets would be very subjective as there is no objective choice of a dataset subset size or a subset of the variables or classes. Therefore, creating dataset subsets would open a Pandora\u2019s Box. We removed datasets which have no source or reference available to potentially learn more about these datasets if we observe unexpected behavior in future studies. In contrast, we would not be able to learn more about the background of a dataset which has no description and publication attached, leaving us with a complete black box. We removed datasets which can be perfectly classified by a single attribute or a decision stump as they do not allow to meaningfully compare machine learning algorithms (they all achieve 100% accuracy unless the hyperparameters are set in a bogus way). We removed datasets where a decision tree could achieve 100% accuracy on a 10-fold cross-validation task to remove datasets which can be solved by a simple algorithm which is prone to overfitting training data. We found that this is a good indicator of too easy datasets. Obviously, other datasets will appear easy for several algorithms, and we aim to learn more about the characteristics of such datasets in future studies. We removed datasets which have more than 5000 features after one-hot-encoding categorical features. One-hot-encoding is the most frequent way to deal with categorical variables across the different machine learning libraries MLR, scikit-learn and WEKA. In order to limit the number of features to 5000 as explained above, we imposed the additional constraint that this should be counted after one-hot-encoding to allow wide applicability of the benchmark suite. We removed datasets which were created by binarization of regression tasks or multiclass classification task for similar reasons as for forbidding dataset subsets. We did not include sparse datasets because not all machine learning libraries (i.e., all machine learning models) can handle them gracefully, which is in contrast to our goal which is wide applicability. Citing the OpenML-CC18 \u00b6 If you have used the OpenML-CC18 in a scientific publication, we would appreciate citations of core OpenML packages as well as a citation of the following paper: Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin. OpenML Benchmarking Suites. arXiv 1708.0373v2 (2019): 1-6 OpenML100 \u00b6 The OpenML100 was a predecessor of the OpenML-CC18, consisting of 100 classification datasets . We recommend that you use the OpenML-CC18 instead, because the OpenML100 suffers from some teething issues in the design of benchmark suites. For instance, it contains several datasets that are too easy to model with today's machine learning algorithms, as well as datasets that represent time series analysis problems. These do not invalidate benchmarks run on the OpenML100, but may obfuscate the interpretation of results. The 'OpenML-CC18' handle is also more descriptive and allows easier versioning. The OpenML100 was first published in the Arxiv preprint OpenML Benchmarking Suites and the OpenML100 . List of datasets and properties For reference, the OpenML100 included datasets satisfying the following requirements: the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small for proper training and not too big for practical experimentation the number of features does not exceed 5000 features to keep the runtime of algorithms low the target attribute has at least two classes he ratio of the minority class and the majority class is above 0.05 to eliminate highly imbalanced datasets that would obfuscate a clear analysis It excluded datasets which: cannot be randomized via a 10-fold cross-validation due to grouped samples have an unknown origin or no clearly defined task are variants of other datasets (e.g. binarized regression tasks) include sparse data (e.g., text mining data sets) Citing the OpenML100 \u00b6 If you have used the OpenML100 in a scientific publication, we would appreciate citations of core OpenML packages as well as a citation of the following paper: Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin. OpenML Benchmarking Suites and the OpenML100. arXiv 1708.0373v1 (2017): 1-6 Need help? \u00b6 We are happy to answer to any suggestion or question you may have. For general questions or issues, please open an issue in the benchmarking issue tracker . If the issue lies with one of the language-specific bindings, please post an issue in the appropriate issue tracker .","title":"Benchmarking"},{"location":"benchmark/#benchmarking-suites","text":"Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. OpenML aims to facilitate the creation of curated, comprehensive suites of machine learning tasks, covering precise sets of conditions. Seamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier: all datasets are uniformly formatted in standardized data formats they can be easily downloaded programmatically through APIs and client libraries they come with machine-readable meta-information , such as the occurrence of missing values, to train algorithms correctly standardized train-test splits are provided to ensure that results can be objectively compared results can be shared in a reproducible way through the APIs results from other users can be easily downloaded and reused","title":"Benchmarking suites"},{"location":"benchmark/#software-interfaces","text":"To use OpenML Benchmark suites, you can use bindings in several programming languages. These all interface with the OpenML REST API. The default endpoint for this is https://www.openml.org/api/v1/ , but this can change when later versions of the API are released. To use the code examples below, you only need a recent version of one of the following libraries: OpenML Java ApiConnector (version 1.0.22 and up). OpenML Weka (version 0.9.6 and up). This package adds a Weka Integration. OpenML Python (version 0.9.0 and up) OpenML R (version 1.8 and up)","title":"Software interfaces"},{"location":"benchmark/#using-openml-benchmark-suites","text":"Below are walk-through instructions for common use cases, as well as code examples. These illustrations use the reference OpenML-CC18 benchmark suite, but you can replace it with any other benchmark suite. Note that a benchmark suite is a set of OpenML tasks , which envelop not only a specific dataset, but also the train-test splits and (for predictive tasks) the target feature. Terminology and current status Benchmark suites are sets of OpenML tasks that you can create and manage yourself. At the same time, it is often useful to also share the set of experiments (runs) with the ensuing benchmarking results. For legacy reasons, such sets of tasks or runs are called studies in the OpenML REST API. In the OpenML bindings (Python, R, Java,...) these are called either sets or studies . When benchmarking, you will probably use two types of sets: Sets of tasks. These can be created, edited, downloaded or deleted via the OpenML API. Website forms will be added soon. Also the set of underlying datasets can be easily retrieved via the API. Sets of runs. Likewise, these can be created, edited, downloaded or deleted via the OpenML API. On the website, these are currently simply called 'studies'. Also the set of underlying tasks, datasets and flows can be easily retrieved. It is possible to link a set of runs to a benchmark study, aimed to collect future runs on that specific set of tasks. Additional information on these will be provided in a separate page.","title":"Using OpenML Benchmark Suites"},{"location":"benchmark/#listing-the-benchmark-suites","text":"The current list of benchmark suites is explicitly listed on the bottom of this page. The list of all sets of tasks can also be fetched programmatically. This list includes the suite's ID (and optionally an alias), which can be used to fetch further details. Via the REST API, the list is returned in XML or JSON REST https://www.openml.org/api/v1/xml/study/list/main_entity_type/task/status/all Check out the API docs Python example import openml # using the main entity type task, only benchmark suites are returned # each benchmark suite has an ID, some also have an alias. These can be # used to obtain the full details. studies = openml . study . list_suites ( status = 'all' ) Java example public void listBenchmarksuites () throws Exception { OpenmlConnector openml = new OpenmlConnector (); Map < String , String > filters = new TreeMap < String , String > (); filters . put ( \"status\" , \"all\" ); filters . put ( \"main_entity_type\" , \"task\" ); filters . put ( \"limit\" , \"20\" ); StudyList list = openml . studyList ( filters ); } R example studies = listOMLStudies()","title":"Listing the benchmark suites"},{"location":"benchmark/#fetching-details","text":"Using the ID or alias of a benchmark suite, you can retrieve a description and the full list of tasks and the underlying datasets. Via the REST API, a list of all tasks and dataset IDs is returned in XML or JSON REST https://www.openml.org/api/v1/xml/study/OpenML-CC18 Check out the API docs In Python, the data is returned as features, targets numpy arrays: Python example import openml benchmark_suite = openml . study . get_suite ( 'OpenML-CC18' ) # obtain the benchmark suite for task_id in benchmark_suite . tasks : # iterate over all tasks task = openml . tasks . get_task ( task_id ) # download the OpenML task features , targets = task . get_X_and_y () # get the data In Java, the data is returned as a WEKA Instances object: Java example public void downloadDatasets () throws Exception { OpenmlConnector openml = new OpenmlConnector (); Study benchmarksuite = openml . studyGet ( \"OpenML-CC18\" , \"tasks\" ); for ( Integer taskId : benchmarksuite . getTasks ()) { // iterate over all tasks Task t = openml . taskGet ( taskId ); // download the OpenML task // note that InstanceHelper is part of the OpenML-weka package Instances d = InstancesHelper . getDatasetFromTask ( openml , t ); // obtain the dataset } } In R, the data is returned as an R dataframe: R example library ( OpenML ) task.ids = getOMLStudy ( 'OpenML-CC18' ) $ tasks $ task.id # obtain the list of suggested tasks for ( task.id in task.ids ) { # iterate over all tasks task = getOMLTask ( task.id ) # download single OML task data = as.data.frame ( task ) # obtain raw data set","title":"Fetching details"},{"location":"benchmark/#running-and-sharing-benchmarks","text":"The code below demonstrates how OpenML benchmarking suites can be conveniently imported for benchmarking using the Python, Java and R APIs. First, the list of tasks is downloaded as already illustrated above. Next, a specific algorithm (or pipeline) can be run on each of them. The OpenML API will automatically evaluate the algorithm using the pre-set train-test splits and store the predictions and scores in a run object. This run object can then be immediately published, pushing the results to the OpenML server, so that they can be compared against all others on the same benchmark set. Uploading results requires an OpenML API key, which can be found in your account details after logging into the OpenML website. REST Requires POST requests: Attaching a new run to a benchmark_study Detaching a run from benchmark_study Python example import openml import sklearn openml . config . apikey = 'FILL_IN_OPENML_API_KEY' # set the OpenML Api Key benchmark_suite = openml . study . get_suite ( 'OpenML-CC18' ) # obtain the benchmark suite # build a scikit-learn classifier clf = sklearn . pipeline . make_pipeline ( sklearn . preprocessing . Imputer (), sklearn . tree . DecisionTreeClassifier ()) for task_id in benchmark_suite . tasks : # iterate over all tasks task = openml . tasks . get_task ( task_id ) # download the OpenML task run = openml . runs . run_model_on_task ( clf , task ) # run the classifier on the task score = run . get_metric_score ( sklearn . metrics . accuracy_score ) # print accuracy score print ( 'Data set: %s ; Accuracy: %0.2f ' % ( task . get_dataset () . name , score . mean ())) run . publish () # publish the experiment on OpenML (optional, requires internet and an API key) print ( 'URL for run: %s /run/ %d ' % ( openml . config . server , run . run_id )) Java example public static void runTasksAndUpload () throws Exception { OpenmlConnector openml = new OpenmlConnector (); openml . setApiKey ( \"FILL_IN_OPENML_API_KEY\" ); // obtain the benchmark suite Study benchmarksuite = openml . studyGet ( \"OpenML-CC18\" , \"tasks\" ); Classifier tree = new REPTree (); // build a Weka classifier for ( Integer taskId : benchmarksuite . getTasks ()) { // iterate over all tasks Task t = openml . taskGet ( taskId ); // download the OpenML task Instances d = InstancesHelper . getDatasetFromTask ( openml , t ); // obtain the dataset int runId = RunOpenmlJob . executeTask ( openml , new WekaConfig (), taskId , tree ); Run run = openml . runGet ( runId ); // retrieve the uploaded run } } R example library ( OpenML ) setOMLConfig ( apikey = 'FILL_IN_OPENML_API_KEY' ) lrn = makeLearner ( 'classif.rpart' ) # construct a simple CART classifier task.ids = getOMLStudy ( 'OpenML-CC18' ) $ tasks $ task.id # obtain the list of suggested tasks for ( task.id in task.ids ) { # iterate over all tasks task = getOMLTask ( task.id ) # download single OML task data = as.data.frame ( task ) # obtain raw data set run = runTaskMlr ( task , learner = lrn ) # run constructed learner upload = uploadOMLRun ( run ) # upload and tag the run }","title":"Running and sharing benchmarks"},{"location":"benchmark/#retrieving-runs-on-a-benchmarking-suites","text":"Once a benchmark suite has been created, the listing functions can be used to obtain all results on the benchmark suite. Note that there are several other ways to select and bundle runs together. This will be featured in a separate article on reproducible benchmarks. REST (TODO) https://www.openml.org/api/v1/xml/run/list/study/OpenML-CC18 Check out the API docs Python example benchmark_suite = openml . study . get_suite ( 'OpenML-CC18' ) runs = openml . runs . list_runs ( task = benchmark_suite . tasks , limit = 1000 ) Java example public void downloadResultsBenchmarkSuite () throws Exception { Study benchmarkSuite = openml . studyGet ( \"OpenML100\" , \"tasks\" ); Map < String , List < Integer >> filters = new TreeMap < String , List < Integer >> (); filters . put ( \"task\" , Arrays . asList ( benchmarkSuite . getTasks ())); RunList rl = openml . runList ( filters , 200 , null ); assertTrue ( rl . getRuns (). length > 0 ); } R example benchmark.suite = getOMLStudy ( study = \"OpenML-CC18\" ) run.ids = extractOMLStudyIds ( benchmark.suite , \"run.id\" ) runs = rbindlist ( lapply ( run.ids , function ( id ) listOMLRuns ( run.id = id ))) # TODO waiting for REST API","title":"Retrieving runs on a benchmarking suites:"},{"location":"benchmark/#creating-new-benchmark-suites","text":"Additional OpenML benchmark suites can be created by defining the precise set of tasks, as well as a textual description. New datasets first need to be registered on OpenML and tasks need to be created on them. We have provided a GitHub repository with additional tools and scripts to build new benchmark studies, e.g. to select all datasets adhering to strict conditions, and to analyse bencharking results. REST Requires POST requests: Creating a benchmark suite Python example import openml # find 250 tasks that we are interested in, e.g., the tasks that have between # 100 and 10000 instances and between 4 and 20 attributes tasks = openml . tasks . list_tasks ( number_instances = '100..10000' , number_features = '4..20' , size = 250 ) task_ids = list ( tasks . keys ()) # create the benchmark suite # the arguments are the alias, name, description, and list of task_ids, respectively. study = openml . study . create_benchmark_suite ( name = \"MidSize Suite\" , alias = None , description = \"illustrating how to create a benchmark suite\" , task_ids = task_ids , ) study_id = study . publish () Java example public void createBenchmarkSuite () throws Exception { OpenmlConnector openml = new OpenmlConnector ( \"FILL_IN_OPENML_API_KEY\" ); // find 250 tasks that we are interested in, e.g., the tasks that have between // 100 and 10000 instances and between 4 and 20 attributes Map < String , String > filtersOrig = new TreeMap < String , String > (); filtersOrig . put ( \"number_instances\" , \"100..10000\" ); filtersOrig . put ( \"number_features\" , \"4..20\" ); filtersOrig . put ( \"limit\" , \"250\" ); Tasks tasksOrig = client_write_test . taskList ( filtersOrig ); // create the study Study study = new Study ( null , \"test\" , \"test\" , null , tasksOrig . getTaskIds (), null ); int studyId = openml . studyUpload ( study ); } R example # find 250 tasks with 100 and 10000 instances and between 4 and 20 attributes tid = listOMLTasks ( number.of.instances = c ( 100 , 10000 ), number.of.features = c ( 4 , 20 ), limit = 250 ) study = makeOMLStudy ( alias = \"test_alias\" , name = \"Test Upload from R\" , description = \"Just testing\" , task.id = tid $ task.id ) id = uploadOMLStudy ( study )","title":"Creating new benchmark suites"},{"location":"benchmark/#updating-a-benchmark-suite","text":"You can add tasks to a benchmark suite, or remove them. REST Requires POST requests: Attaching a new task Detaching a task Python example import openml # find 250 tasks that we are interested in, e.g., the tasks that have between # 100 and 10000 instances and between 4 and 20 attributes tasks = openml . tasks . list_tasks ( number_instances = '100..10000' , number_features = '4..20' , size = 250 ) task_ids = list ( tasks . keys ()) # create the benchmark suite study = openml . study . create_benchmark_suite ( name = \"MidSize Suite\" , alias = None , description = \"illustrating how to create a benchmark suite\" , task_ids = task_ids , ) study_id = study . publish () # download the study from the server, for verification purposes study = openml . study . get_study ( study_id ) # until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset: tasks_new = openml . tasks . list_tasks ( data_name = 'letter' , size = 1 ) task_ids_new = list ( tasks_new . keys ()) openml . study . attach_to_study ( study_id , task_ids_new ) # or even remove these again openml . study . detach_from_study ( study_id , task_ids_new ) # redownload the study study_prime = openml . study . get_study ( study_id ) assert ( study . tasks == study_prime . tasks ) assert ( study . data == study_prime . data ) Java example public void attachDetachStudy () throws Exception { OpenmlConnector openml = new OpenmlConnector ( \"FILL_IN_OPENML_API_KEY\" ); // find 250 tasks that we are interested in, e.g., the tasks that have between // 100 and 10000 instances and between 4 and 20 attributes Map < String , String > filtersOrig = new TreeMap < String , String > (); filtersOrig . put ( \"number_instances\" , \"100..10000\" ); filtersOrig . put ( \"number_features\" , \"4..20\" ); filtersOrig . put ( \"limit\" , \"250\" ); Tasks tasksOrig = openml . taskList ( filtersOrig ); // create the study Study study = new Study ( null , \"test\" , \"test\" , null , tasksOrig . getTaskIds (), null ); int studyId = openml . studyUpload ( study ); // until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset: Map < String , String > filtersAdd = new TreeMap < String , String > (); filtersAdd . put ( \"data_name\" , \"letter\" ); filtersAdd . put ( \"limit\" , \"1\" ); Tasks tasksAdd = openml . taskList ( filtersAdd ); openml . studyAttach ( studyId , Arrays . asList ( tasksAdd . getTaskIds ())); // or even remove these again openml . studyDetach ( studyId , Arrays . asList ( tasksAdd . getTaskIds ())); // download the study Study studyDownloaded = openml . studyGet ( studyId ); assertArrayEquals ( tasksOrig . getTaskIds (), studyDownloaded . getTasks ()); } R example TODO","title":"Updating a benchmark suite"},{"location":"benchmark/#further-code-examples-and-use-cases","text":"As mentioned above, we host a GitHub repository with additional tools and scripts to easily create and use new benchmark studies. It includes: A Jupyter Notebook that builds a new benchmark suite with datasets that adhere to strict and complex conditions, as well as automated tests to remove tasks that are too easy for proper benchmarking. A Jupyter Notebook that shows how to pull in the latest state-of-the-art results for any of the benchmark suites A Jupyter Notebook that does a detailed analysis of all results in a benchmark suite, and an example run on the OpenML-CC18. It includes a wide range of plots and rankings to get a deeper insight into the benchmark results. Scripts in Python and R to facilitate common subtasks. We very much welcome new scripts and notebooks, or improvements to the existing ones, that help others to create benchmark suites and analyse benchmarking results.","title":"Further code examples and use cases"},{"location":"benchmark/#list-of-benchmarking-suites","text":"","title":"List of benchmarking suites"},{"location":"benchmark/#openml-cc18","text":"The OpenML-CC18 suite contains all OpenML datasets from mid-2018 that satisfy a large set of clear requirements for thorough yet practical benchmarking. It includes datasets frequently used in benchmarks published over the last years, so it can be used as a drop-in replacement for many benchmarking setups. List of datasets and properties The suite is defined as the set of all verified OpenML datasets that satisfy the following requirements: the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small and not too big, the number of features does not exceed 5000 features to keep the runtime of algorithms low, the target attribute has at least two classes the ratio of the minority class and the majority class is above 0.05, to eliminate highly imbalanced datasets which require special treatment for both algorithms and evaluation measures. We excluded datasets which: are artificially generated (not to confuse with simulated) cannot be randomized via a 10-fold cross-validation due to grouped samples or because they are time series or data streams are a subset of a larger dataset have classes with less than 20 observations have no source or reference available can be perfectly classified by a single attribute or a decision stump allow a decision tree to achieve 100% accuracy on a 10-fold cross-validation task have more than 5000 features after one-hot-encoding categorical features are created by binarization of regression tasks or multiclass classification tasks, or are sparse data (e.g., text mining data sets) Detailed motivation of these decisions We chose the CC18 datasets to allow for practical benchmarking based on the characteristics that might be problematic based on our experience, and to avoid common pitfalls that may invalidate benchmark studies: We used at least 500 data points to allow performing cross-validation while still having a large-enough test split. We limited the datasets to 100.000 data points to allow the algorithms to train machine learning models in a reasonable amount of time. We limited the number of features to 5000 to allow the usage of algorithms which scale unfavourably in the number of features. This limitation, together with the two limitations above aims to allow running all \u201cstandard\u201d machine learning algorithms (naive bayes, linear models, support vector machines, tree-based ensemble methods and neural networks) on the benchmark suite. We required each dataset to have at least two classes to be able to work in a supervised classification setting. We require each class to have at least 20 observations to be able to perform stratified cross-validation where there is at least one observation from each class in each split. We have found that not having all classes present in all training and test sets can make several machine learning packages fail. We require a certain balancedness (ratio of minority class to majority class) to prevent cases where only predicting the majority class would be beneficial. This is most likely the restriction which is most debatable, but we found it very helpful to apply a large set of machine learning algorithms across several libraries to the study. We expect that future studies focus more on imbalanced datasets. Furthermore, we aimed to have the dataset collection as general as possible, rule out as few algorithms as possible and have it usable as easily as possible: We strived to remove artificial datasets as they, for example, come from textbooks and it is hard to reliably assess their difficulty. We admit that there is a blurred line between artificial and simulated datasets and do not have a perfect distinction between them (for example, a lot of phenomena can be simulated, but the outcome might be like a simple, artificial dataset). Therefore, we removed datasets if we were in doubt of whether they are simulated or artificial. We removed datasets which require grouped sampling because they are time series or data streams which should be treated with special care by machine learning algorithms (i.e., taking the time aspect into account). To be on the safe side, we also removed datasets where each sample constitutes a single data stream. We removed datasets which are a subset of larger datasets. Allowing subsets would be very subjective as there is no objective choice of a dataset subset size or a subset of the variables or classes. Therefore, creating dataset subsets would open a Pandora\u2019s Box. We removed datasets which have no source or reference available to potentially learn more about these datasets if we observe unexpected behavior in future studies. In contrast, we would not be able to learn more about the background of a dataset which has no description and publication attached, leaving us with a complete black box. We removed datasets which can be perfectly classified by a single attribute or a decision stump as they do not allow to meaningfully compare machine learning algorithms (they all achieve 100% accuracy unless the hyperparameters are set in a bogus way). We removed datasets where a decision tree could achieve 100% accuracy on a 10-fold cross-validation task to remove datasets which can be solved by a simple algorithm which is prone to overfitting training data. We found that this is a good indicator of too easy datasets. Obviously, other datasets will appear easy for several algorithms, and we aim to learn more about the characteristics of such datasets in future studies. We removed datasets which have more than 5000 features after one-hot-encoding categorical features. One-hot-encoding is the most frequent way to deal with categorical variables across the different machine learning libraries MLR, scikit-learn and WEKA. In order to limit the number of features to 5000 as explained above, we imposed the additional constraint that this should be counted after one-hot-encoding to allow wide applicability of the benchmark suite. We removed datasets which were created by binarization of regression tasks or multiclass classification task for similar reasons as for forbidding dataset subsets. We did not include sparse datasets because not all machine learning libraries (i.e., all machine learning models) can handle them gracefully, which is in contrast to our goal which is wide applicability.","title":"OpenML-CC18"},{"location":"benchmark/#citing-the-openml-cc18","text":"If you have used the OpenML-CC18 in a scientific publication, we would appreciate citations of core OpenML packages as well as a citation of the following paper: Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin. OpenML Benchmarking Suites. arXiv 1708.0373v2 (2019): 1-6","title":"Citing the OpenML-CC18"},{"location":"benchmark/#openml100","text":"The OpenML100 was a predecessor of the OpenML-CC18, consisting of 100 classification datasets . We recommend that you use the OpenML-CC18 instead, because the OpenML100 suffers from some teething issues in the design of benchmark suites. For instance, it contains several datasets that are too easy to model with today's machine learning algorithms, as well as datasets that represent time series analysis problems. These do not invalidate benchmarks run on the OpenML100, but may obfuscate the interpretation of results. The 'OpenML-CC18' handle is also more descriptive and allows easier versioning. The OpenML100 was first published in the Arxiv preprint OpenML Benchmarking Suites and the OpenML100 . List of datasets and properties For reference, the OpenML100 included datasets satisfying the following requirements: the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small for proper training and not too big for practical experimentation the number of features does not exceed 5000 features to keep the runtime of algorithms low the target attribute has at least two classes he ratio of the minority class and the majority class is above 0.05 to eliminate highly imbalanced datasets that would obfuscate a clear analysis It excluded datasets which: cannot be randomized via a 10-fold cross-validation due to grouped samples have an unknown origin or no clearly defined task are variants of other datasets (e.g. binarized regression tasks) include sparse data (e.g., text mining data sets)","title":"OpenML100"},{"location":"benchmark/#citing-the-openml100","text":"If you have used the OpenML100 in a scientific publication, we would appreciate citations of core OpenML packages as well as a citation of the following paper: Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin. OpenML Benchmarking Suites and the OpenML100. arXiv 1708.0373v1 (2017): 1-6","title":"Citing the OpenML100"},{"location":"benchmark/#need-help","text":"We are happy to answer to any suggestion or question you may have. For general questions or issues, please open an issue in the benchmarking issue tracker . If the issue lies with one of the language-specific bindings, please post an issue in the appropriate issue tracker .","title":"Need help?"},{"location":"mlr/","text":"Machine Learning in R (mlr) \u00b6 OpenML is readily integrated with mlr through the R API . Example library ( OpenML ) library ( mlr ) task = getOMLTask ( 10 ) lrn = makeLearner ( \"classif.rpart\" ) run = runTaskMlr ( task , lrn ) run.id = uploadOMLRun ( run ) Key features: Query and download OpenML datasets and use them however you like Build any mlr learner, run it on any task and save the experiment as run objects Upload your runs for collaboration or publishing Query, download and reuse all shared runs For many more details and examples, see the R tutorial .","title":"mlr"},{"location":"mlr/#machine-learning-in-r-mlr","text":"OpenML is readily integrated with mlr through the R API . Example library ( OpenML ) library ( mlr ) task = getOMLTask ( 10 ) lrn = makeLearner ( \"classif.rpart\" ) run = runTaskMlr ( task , lrn ) run.id = uploadOMLRun ( run ) Key features: Query and download OpenML datasets and use them however you like Build any mlr learner, run it on any task and save the experiment as run objects Upload your runs for collaboration or publishing Query, download and reuse all shared runs For many more details and examples, see the R tutorial .","title":"Machine Learning in R (mlr)"},{"location":"resources/","text":"Resources \u00b6 Database snapshots \u00b6 Everything uploaded to OpenML is available to the community. The nightly snapshot of the public database contains all experiment runs, evaluations and links to datasets, implementations and result files. In SQL format (gzipped). You can also download the Database schema . Nightly database SNAPSHOT If you want to work on the website locally, you'll also need the schema for the 'private' database with non-public information. Private database schema Legacy Resources \u00b6 OpenML is always evolving, but we keep hosting the resources that were used in prior publications so that others may still build on them. The experiment database used in Vanschoren et al. (2012) Experiment databases. Machine Learning 87(2), pp 127-158 . You'll need to import this database (we used MySQL) to run queries. The database structure is described in the paper. Note that most of the experiments in this database have been rerun using OpenML, using newer algorithm implementations and stored in much more detail. The Expos\u00e9 ontology used in the same paper, and described in more detail here and here . Expos\u00e9 is used in designing our databases, and we aim to use it to export all OpenML data as Linked Open Data.","title":"Resources"},{"location":"resources/#resources","text":"","title":"Resources"},{"location":"resources/#database-snapshots","text":"Everything uploaded to OpenML is available to the community. The nightly snapshot of the public database contains all experiment runs, evaluations and links to datasets, implementations and result files. In SQL format (gzipped). You can also download the Database schema . Nightly database SNAPSHOT If you want to work on the website locally, you'll also need the schema for the 'private' database with non-public information. Private database schema","title":"Database snapshots"},{"location":"resources/#legacy-resources","text":"OpenML is always evolving, but we keep hosting the resources that were used in prior publications so that others may still build on them. The experiment database used in Vanschoren et al. (2012) Experiment databases. Machine Learning 87(2), pp 127-158 . You'll need to import this database (we used MySQL) to run queries. The database structure is described in the paper. Note that most of the experiments in this database have been rerun using OpenML, using newer algorithm implementations and stored in much more detail. The Expos\u00e9 ontology used in the same paper, and described in more detail here and here . Expos\u00e9 is used in designing our databases, and we aim to use it to export all OpenML data as Linked Open Data.","title":"Legacy Resources"},{"location":"sklearn/","text":"scikit-learn \u00b6 OpenML is readily integrated with scikit-learn through the Python API . Example from sklearn import ensemble from openml import tasks , flows , Runs task = tasks . get_task ( 3954 ) clf = ensemble . RandomForestClassifier () flow = flows . sklearn_to_flow ( clf ) run = runs . run_flow_on_task ( task , flow ) result = run . publish () Key features: Query and download OpenML datasets and use them however you like Build any sklearn estimator or pipeline and convert to OpenML flows Run any flow on any task and save the experiment as run objects Upload your runs for collaboration or publishing Query, download and reuse all shared runs For many more details and examples, see the Python tutorial .","title":"scikit-learn"},{"location":"sklearn/#scikit-learn","text":"OpenML is readily integrated with scikit-learn through the Python API . Example from sklearn import ensemble from openml import tasks , flows , Runs task = tasks . get_task ( 3954 ) clf = ensemble . RandomForestClassifier () flow = flows . sklearn_to_flow ( clf ) run = runs . run_flow_on_task ( task , flow ) result = run . publish () Key features: Query and download OpenML datasets and use them however you like Build any sklearn estimator or pipeline and convert to OpenML flows Run any flow on any task and save the experiment as run objects Upload your runs for collaboration or publishing Query, download and reuse all shared runs For many more details and examples, see the Python tutorial .","title":"scikit-learn"},{"location":"terms/","text":"Honor Code \u00b6 By joining OpenML , you join a special worldwide community of data scientists building on each other's results and connecting their minds as efficiently as possible. This community depends on your motivation to share data, tools and ideas, and to do so with honesty. In return, you will gain trust, visibility and reputation, igniting online collaborations and studies that otherwise may not have happened. By using any part of OpenML, you agree to: Give credit where credit is due . Cite the authors whose work you are building on, or build collaborations where appropriate. Give back to the community by sharing your own data as openly and as soon as possible, or by helping the community in other ways. In doing so, you gain visibility and impact (citations). Share data according to your best efforts . Everybody make mistakes, but we trust you to correct them as soon as possible. Remove or flag data that cannot be trusted. Be polite and constructive in all discussions. Criticism of methods is welcomed, but personal criticisms should be avoided. Respect circles of trust . OpenML allows you to collaborate in 'circles' of trusted people to share unpublished results. Be considerate in sharing data with people outside this circle. Do not steal the work of people who openly share it. OpenML makes it easy to find all shared data (and when it was shared), thus everybody will know if you do this. Terms of Use \u00b6 You agree that you are responsible for your own use of OpenML.org and all content submitted by you, in accordance with the Honor Code and all applicable local, state, national and international laws. By submitting or distributing content from OpenML.org, you affirm that you have the necessary rights, licenses, consents and/or permissions to reproduce and publish this content. You cannot upload sensitive or confidential data. You, and not the developers of OpenML.org, are solely responsible for your submissions. By submitting content to OpenML.org, you grant OpenML.org the right to host, transfer, display and use this content, in accordance with your sharing settings and any licences granted by you. You also grant to each user a non-exclusive license to access and use this content for their own research purposes, in accordance with any licences granted by you. You may maintain one user account and not let anyone else use your username and/or password. You may not impersonate other persons. You will not intend to damage, disable, or impair any OpenML server or interfere with any other party's use and enjoyment of the service. You may not attempt to gain unauthorized access to the Site, other accounts, computer systems or networks connected to any OpenML server. You may not obtain or attempt to obtain any materials or information not intentionally made available through OpenML. Strictly prohibited are content that defames, harasses or threatens others, that infringes another's intellectual property, as well as indecent or unlawful content, advertising, or intentionally inaccurate information posted with the intent of misleading others. It is also prohibited to post code containing viruses, malware, spyware or any other similar software that may damage the operation of another's computer or property.","title":"Terms"},{"location":"terms/#honor-code","text":"By joining OpenML , you join a special worldwide community of data scientists building on each other's results and connecting their minds as efficiently as possible. This community depends on your motivation to share data, tools and ideas, and to do so with honesty. In return, you will gain trust, visibility and reputation, igniting online collaborations and studies that otherwise may not have happened. By using any part of OpenML, you agree to: Give credit where credit is due . Cite the authors whose work you are building on, or build collaborations where appropriate. Give back to the community by sharing your own data as openly and as soon as possible, or by helping the community in other ways. In doing so, you gain visibility and impact (citations). Share data according to your best efforts . Everybody make mistakes, but we trust you to correct them as soon as possible. Remove or flag data that cannot be trusted. Be polite and constructive in all discussions. Criticism of methods is welcomed, but personal criticisms should be avoided. Respect circles of trust . OpenML allows you to collaborate in 'circles' of trusted people to share unpublished results. Be considerate in sharing data with people outside this circle. Do not steal the work of people who openly share it. OpenML makes it easy to find all shared data (and when it was shared), thus everybody will know if you do this.","title":"Honor Code"},{"location":"terms/#terms-of-use","text":"You agree that you are responsible for your own use of OpenML.org and all content submitted by you, in accordance with the Honor Code and all applicable local, state, national and international laws. By submitting or distributing content from OpenML.org, you affirm that you have the necessary rights, licenses, consents and/or permissions to reproduce and publish this content. You cannot upload sensitive or confidential data. You, and not the developers of OpenML.org, are solely responsible for your submissions. By submitting content to OpenML.org, you grant OpenML.org the right to host, transfer, display and use this content, in accordance with your sharing settings and any licences granted by you. You also grant to each user a non-exclusive license to access and use this content for their own research purposes, in accordance with any licences granted by you. You may maintain one user account and not let anyone else use your username and/or password. You may not impersonate other persons. You will not intend to damage, disable, or impair any OpenML server or interfere with any other party's use and enjoyment of the service. You may not attempt to gain unauthorized access to the Site, other accounts, computer systems or networks connected to any OpenML server. You may not obtain or attempt to obtain any materials or information not intentionally made available through OpenML. Strictly prohibited are content that defames, harasses or threatens others, that infringes another's intellectual property, as well as indecent or unlawful content, advertising, or intentionally inaccurate information posted with the intent of misleading others. It is also prohibited to post code containing viruses, malware, spyware or any other similar software that may damage the operation of another's computer or property.","title":"Terms of Use"}]}