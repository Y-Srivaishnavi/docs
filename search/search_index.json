{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"An open, automated, and frictionless machine learning environment. <p> \u00a0 1000s of data sets, uniformly formatted, easy to load, organized online</p> <p> \u00a0Models and pipelines automatically uploaded from machine learning libraries</p> <p> Extensive APIs to integrate OpenML into your tools and scripts</p> <p>  Easily reproducible results (e.g. models, evaluations) for comparison and reuse</p> <p>\u00a0 Stand on the shoulders of giants, and collaborate in real time</p> <p>\u00a0 Make your work more visible and reusable</p> <p>\u00a0 Built for automation: streamline your experiments and model building</p>"},{"location":"#installation","title":"Installation","text":"<p>The OpenML package is available in many languages and across libraries. For more information about them, see the Integrations page.</p> Python/sklearnPytorchKerasTensorFlowRJuliaRUST.Net <ul> <li>Python/sklearn repository</li> <li><code>pip install openml</code></li> </ul> <ul> <li>Pytorch repository</li> <li><code>pip install openml-pytorch</code></li> </ul> <ul> <li>Keras repository</li> <li><code>pip install openml-keras</code></li> </ul> <ul> <li>TensorFlow repository</li> <li><code>pip install openml-tensorflow</code></li> </ul> <ul> <li>R repository</li> <li><code>install.packages(\"mlr3oml\")</code></li> </ul> <ul> <li>Julia repository</li> <li><code>using Pkg;Pkg.add(\"OpenML\")</code></li> </ul> <ul> <li>RUST repository</li> <li>Install from source</li> </ul> <ul> <li>.Net repository</li> <li><code>Install-Package openMl</code></li> </ul> <p>You might also need to set up the API key. For more information, see Authentication.</p>"},{"location":"#learning-openml","title":"Learning OpenML","text":"<p>Aside from the individual package documentations, you can learn more about OpenML through the following resources: The core concepts of OpenML are explained in the Concepts page. These concepts include the principle behind using Datasets, Runs, Tasks, Flows, Benchmarking and much more. Going through them will help you leverage OpenML even better in your work.</p>"},{"location":"#contributing-to-openml","title":"Contributing to OpenML","text":"<p>OpenML is an open source project, hosted on GitHub. We welcome everybody to help improve OpenML, and make it more useful for everyone. For more information on how to contribute, see the Contributing page.</p> <p>We want to make machine learning and data analysis simple, accessible, collaborative and open with an optimal division of labour between computers and humans.</p>"},{"location":"#want-to-get-involved","title":"Want to get involved?","text":"<p>Awesome, we're happy to have you! </p> <p>OpenML is dependent on the community. If you want to help, please email us (openmlHQ@googlegroups.com). If you feel already comfortable you can help by opening issues or make a pull request on GitHub. We also have regular workshops you can join (they are announced on openml.org).</p>"},{"location":"benchmark/benchmark/","title":"Benchmarking suites","text":"<p>Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. OpenML aims to facilitate the creation of curated, comprehensive suites of machine learning tasks, covering precise sets of conditions.</p> <p>Seamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier:  </p> <ul> <li> <p>all datasets are uniformly formatted in standardized data formats </p> </li> <li> <p>they can be easily downloaded programmatically through APIs and client libraries</p> </li> <li> <p>they come with machine-readable meta-information, such as the occurrence of missing values, to train algorithms correctly</p> </li> <li> <p>standardized train-test splits are provided to ensure that results can be objectively compared</p> </li> <li> <p>results can be shared in a reproducible way through the APIs</p> </li> <li> <p>results from other users can be easily downloaded and reused</p> </li> </ul>"},{"location":"benchmark/benchmark/#software-interfaces","title":"Software interfaces","text":"<p>To use OpenML Benchmark suites, you can use bindings in several programming languages. These all interface with the OpenML REST API. The default endpoint for this is <code>https://www.openml.org/api/v1/</code>, but this can change when later versions of the API are released. To use the code examples below, you only need a recent version of one of the following libraries:</p> <ul> <li>OpenML Java ApiConnector (version <code>1.0.22</code> and up).</li> <li>OpenML Weka (version <code>0.9.6</code> and up). This package adds a Weka Integration.</li> <li>OpenML Python (version <code>0.9.0</code> and up)</li> <li>OpenML R (version <code>1.8</code> and up)</li> </ul>"},{"location":"benchmark/benchmark/#using-openml-benchmark-suites","title":"Using OpenML Benchmark Suites","text":"<p>Below are walk-through instructions for common use cases, as well as code examples. These illustrations use the reference OpenML-CC18 benchmark suite, but you can replace it with any other benchmark suite. Note that a benchmark suite is a set of OpenML <code>tasks</code>, which envelop not only a specific dataset, but also the train-test splits and (for predictive tasks) the target feature.</p> Terminology and current status <p>Benchmark suites are sets of OpenML tasks that you can create and manage yourself. At the same time, it is often useful to also share the set of experiments (runs) with the ensuing benchmarking results. For legacy reasons, such sets of tasks or runs are called <code>studies</code> in the OpenML REST API. In the OpenML bindings (Python, R, Java,...) these are called either <code>sets</code> or <code>studies</code>.</p> <p>When benchmarking, you will probably use two types of sets:</p> <ul> <li>Sets of tasks. These can be created, edited, downloaded or deleted via the OpenML API. Website forms will be added soon. Also the set of underlying datasets can be easily retrieved via the API.</li> <li>Sets of runs. Likewise, these can be created, edited, downloaded or deleted via the OpenML API. On the website, these are currently simply called 'studies'. Also the set of underlying tasks, datasets and flows can be easily retrieved. It is possible to link a set of runs to a benchmark study, aimed to collect future runs on that specific set of tasks. Additional information on these will be provided in a separate page.</li> </ul>"},{"location":"benchmark/benchmark/#listing-the-benchmark-suites","title":"Listing the benchmark suites","text":"<p>The current list of benchmark suites is explicitly listed on the bottom of this page. The list of all sets of tasks can also be fetched programmatically. This list includes the suite's ID (and optionally an alias), which can be used to fetch further details.</p> <p>Via the REST API, the list is returned in XML or JSON</p> REST <p>https://www.openml.org/api/v1/xml/study/list/main_entity_type/task/status/all</p> <p>Check out the API docs</p> Python example <pre><code>import openml\n\n# using the main entity type task, only benchmark suites are returned\n# each benchmark suite has an ID, some also have an alias. These can be\n# used to obtain the full details. \nstudies = openml.study.list_suites(status = 'all')\n</code></pre> Java example <pre><code>public void listBenchmarksuites() throws Exception {\n    OpenmlConnector openml = new OpenmlConnector();\n    Map&lt;String, String&gt; filters = new TreeMap&lt;String, String&gt;();\n    filters.put(\"status\", \"all\");\n    filters.put(\"main_entity_type\", \"task\");\n    filters.put(\"limit\", \"20\");\n    StudyList list = openml.studyList(filters);\n}\n</code></pre> R example <pre><code>studies = listOMLStudies()\n</code></pre>"},{"location":"benchmark/benchmark/#fetching-details","title":"Fetching details","text":"<p>Using the ID or alias of a benchmark suite, you can retrieve a description and the full list of tasks and the underlying datasets.</p> <p>Via the REST API, a list of all tasks and dataset IDs is returned in XML or JSON</p> REST <p>https://www.openml.org/api/v1/xml/study/OpenML-CC18</p> <p>Check out the API docs</p> <p>In Python, the data is returned as <code>features, targets</code> numpy arrays:</p> Python example <pre><code>import openml\n\nbenchmark_suite = openml.study.get_suite('OpenML-CC18') # obtain the benchmark suite\n\nfor task_id in benchmark_suite.tasks:  # iterate over all tasks\n    task = openml.tasks.get_task(task_id)  # download the OpenML task\n    features, targets = task.get_X_and_y()  # get the data\n</code></pre> <p>In Java, the data is returned as a WEKA Instances object:</p> Java example <pre><code>public void downloadDatasets() throws Exception {\n    OpenmlConnector openml = new OpenmlConnector();\n    Study benchmarksuite = openml.studyGet(\"OpenML-CC18\", \"tasks\");\n    for (Integer taskId : benchmarksuite.getTasks()) { // iterate over all tasks\n        Task t = openml.taskGet(taskId); // download the OpenML task\n        // note that InstanceHelper is part of the OpenML-weka package\n        Instances d = InstancesHelper.getDatasetFromTask(openml, t); // obtain the dataset\n    }\n}\n</code></pre> <p>In R, the data is returned as an R dataframe:</p> R example <pre><code>library(OpenML)\ntask.ids = getOMLStudy('OpenML-CC18')$tasks$task.id # obtain the list of suggested tasks\nfor (task.id in task.ids) { # iterate over all tasks\n  task = getOMLTask(task.id) # download single OML task\n  data = as.data.frame(task) # obtain raw data set\n</code></pre>"},{"location":"benchmark/benchmark/#running-and-sharing-benchmarks","title":"Running and sharing benchmarks","text":"<p>The code below demonstrates how OpenML benchmarking suites can be conveniently imported for benchmarking using the Python, Java and R APIs.</p> <p>First, the list of tasks is downloaded as already illustrated above. Next, a specific algorithm (or pipeline) can be run on each of them. The OpenML API will automatically evaluate the algorithm using the pre-set train-test splits and store the predictions and scores in a run object. This run object can then be immediately published, pushing the results to the OpenML server, so that they can be compared against all others on the same benchmark set. Uploading results requires an OpenML API key, which can be found in your account details after logging into the OpenML website.</p> REST <p>Requires POST requests: Attaching a new run to a benchmark_study Detaching a run from benchmark_study </p> Python example <pre><code>import openml\nimport sklearn\n\nopenml.config.apikey = 'FILL_IN_OPENML_API_KEY'  # set the OpenML Api Key\nbenchmark_suite = openml.study.get_suite('OpenML-CC18')  # obtain the benchmark suite\n\n# build a scikit-learn classifier\nclf = sklearn.pipeline.make_pipeline(sklearn.preprocessing.Imputer(),\n                                     sklearn.tree.DecisionTreeClassifier())\n\nfor task_id in benchmark_suite.tasks:  # iterate over all tasks\n\n    task = openml.tasks.get_task(task_id)  # download the OpenML task\n    run = openml.runs.run_model_on_task(clf, task)  # run the classifier on the task\n    score = run.get_metric_fn(sklearn.metrics.accuracy_score)  # print accuracy score\n    print('Data set: %s; Accuracy: %0.2f' % (task.get_dataset().name,score.mean()))\n    run.publish()  # publish the experiment on OpenML (optional, requires internet and an API key)\n    print('URL for run: %s/run/%d' %(openml.config.server,run.run_id))\n</code></pre> Java example <pre><code>public static void runTasksAndUpload() throws Exception {\n  OpenmlConnector openml = new OpenmlConnector();\n  openml.setApiKey(\"FILL_IN_OPENML_API_KEY\");\n  // obtain the benchmark suite\n  Study benchmarksuite = openml.studyGet(\"OpenML-CC18\", \"tasks\");\n  Classifier tree = new REPTree(); // build a Weka classifier\n  for (Integer taskId : benchmarksuite.getTasks()) { // iterate over all tasks\n    Task t = openml.taskGet(taskId); // download the OpenML task\n    Instances d = InstancesHelper.getDatasetFromTask(openml, t); // obtain the dataset\n    int runId = RunOpenmlJob.executeTask(openml, new WekaConfig(), taskId, tree);\n    Run run = openml.runGet(runId);   // retrieve the uploaded run\n  }\n}\n</code></pre> R example <pre><code>library(OpenML)\nsetOMLConfig(apikey = 'FILL_IN_OPENML_API_KEY')\nlrn = makeLearner('classif.rpart') # construct a simple CART classifier\ntask.ids = getOMLStudy('OpenML-CC18')$tasks$task.id # obtain the list of suggested tasks\nfor (task.id in task.ids) { # iterate over all tasks\n  task = getOMLTask(task.id) # download single OML task\n  data = as.data.frame(task) # obtain raw data set\n  run = runTaskMlr(task, learner = lrn) # run constructed learner\n  upload = uploadOMLRun(run) # upload and tag the run\n}\n</code></pre>"},{"location":"benchmark/benchmark/#retrieving-runs-on-a-benchmarking-suites","title":"Retrieving runs on a benchmarking suites:","text":"<p>Once a benchmark suite has been created, the listing functions can be used to  obtain all results on the benchmark suite. Note that there are several other ways to select and bundle runs together. This will be featured in  a separate article on reproducible benchmarks. </p> REST (TODO) <p>https://www.openml.org/api/v1/xml/run/list/study/OpenML-CC18</p> <p>Check out the API docs</p> Python example <pre><code>benchmark_suite = openml.study.get_suite('OpenML-CC18')\nruns = openml.runs.list_runs(task=benchmark_suite.tasks, limit=1000)\n</code></pre> Java example <pre><code>public void downloadResultsBenchmarkSuite()  throws Exception {\n    Study benchmarkSuite = openml.studyGet(\"OpenML100\", \"tasks\");\n\n    Map&lt;String, List&lt;Integer&gt;&gt; filters = new TreeMap&lt;String, List&lt;Integer&gt;&gt;();\n    filters.put(\"task\", Arrays.asList(benchmarkSuite.getTasks()));\n    RunList rl = openml.runList(filters, 200, null);\n\n    assertTrue(rl.getRuns().length &gt; 0); \n}\n</code></pre> R example <pre><code>benchmark.suite = getOMLStudy(study = \"OpenML-CC18\")\nrun.ids = extractOMLStudyIds(benchmark.suite, \"run.id\")\nruns = rbindlist(lapply(run.ids, function(id) listOMLRuns(run.id = id)))\n# TODO waiting for REST API\n</code></pre>"},{"location":"benchmark/benchmark/#creating-new-benchmark-suites","title":"Creating new benchmark suites","text":"<p>Additional OpenML benchmark suites can be created by defining the precise set of tasks, as well as a textual description. New datasets first need to be registered on OpenML and tasks need to be created on them.</p> <p>We have provided a GitHub repository with additional tools and scripts to build new benchmark studies, e.g. to select all datasets adhering to strict conditions, and to analyse bencharking results.</p> REST <p>Requires POST requests: Creating a benchmark suite </p> Python example <pre><code>import openml\n\n# find 250 tasks that we are interested in, e.g., the tasks that have between\n# 100 and 10000 instances and between 4 and 20 attributes\ntasks = openml.tasks.list_tasks(number_instances='100..10000', number_features='4..20', size=250)\ntask_ids = list(tasks.keys())\n\n# create the benchmark suite\n# the arguments are the alias, name, description, and list of task_ids, respectively.\nstudy = openml.study.create_benchmark_suite( \n    name=\"MidSize Suite\", \n    alias=None,\n    description=\"illustrating how to create a benchmark suite\", \n    task_ids=task_ids,\n)\nstudy_id = study.publish()\n</code></pre> Java example <pre><code>public void createBenchmarkSuite() throws Exception {\n    OpenmlConnector openml = new OpenmlConnector(\"FILL_IN_OPENML_API_KEY\");\n    // find 250 tasks that we are interested in, e.g., the tasks that have between\n    // 100 and 10000 instances and between 4 and 20 attributes\n    Map&lt;String, String&gt; filtersOrig = new TreeMap&lt;String, String&gt;();\n    filtersOrig.put(\"number_instances\", \"100..10000\");\n    filtersOrig.put(\"number_features\", \"4..20\");\n    filtersOrig.put(\"limit\", \"250\");\n    Tasks tasksOrig = client_write_test.taskList(filtersOrig);\n\n    // create the study\n    Study study = new Study(null, \"test\", \"test\", null, tasksOrig.getTaskIds(), null);\n    int studyId = openml.studyUpload(study);\n}\n</code></pre> R example <pre><code># find 250 tasks with 100 and 10000 instances and between 4 and 20 attributes\ntid = listOMLTasks(number.of.instances = c(100, 10000), number.of.features = c(4, 20), limit = 250)\nstudy = makeOMLStudy(alias = \"test_alias\", name = \"Test Upload from R\", description = \"Just testing\", task.id = tid$task.id)\nid = uploadOMLStudy(study)\n</code></pre>"},{"location":"benchmark/benchmark/#updating-a-benchmark-suite","title":"Updating a benchmark suite","text":"<p>You can add tasks to a benchmark suite, or remove them.</p> REST <p>Requires POST requests: Attaching a new task Detaching a task </p> Python example <pre><code>import openml\n\n# find 250 tasks that we are interested in, e.g., the tasks that have between\n# 100 and 10000 instances and between 4 and 20 attributes\ntasks = openml.tasks.list_tasks(number_instances='100..10000', number_features='4..20', size=250)\ntask_ids = list(tasks.keys())\n\n# create the benchmark suite\nstudy = openml.study.create_benchmark_suite( \n    name=\"MidSize Suite\", \n    alias=None,\n    description=\"illustrating how to create a benchmark suite\", \n    task_ids=task_ids,\n)\nstudy_id = study.publish()\n\n# download the study from the server, for verification purposes\nstudy = openml.study.get_study(study_id)\n\n# until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset:\ntasks_new = openml.tasks.list_tasks(data_name='letter', size=1)\ntask_ids_new = list(tasks_new.keys())\nopenml.study.attach_to_study(study_id, task_ids_new)\n\n# or even remove these again\nopenml.study.detach_from_study(study_id, task_ids_new)\n\n# redownload the study\nstudy_prime = openml.study.get_study(study_id)\n\nassert(study.tasks == study_prime.tasks)\nassert(study.data == study_prime.data)\n</code></pre> Java example <pre><code>public void attachDetachStudy()  throws Exception {\n    OpenmlConnector openml = new OpenmlConnector(\"FILL_IN_OPENML_API_KEY\");\n    // find 250 tasks that we are interested in, e.g., the tasks that have between\n    // 100 and 10000 instances and between 4 and 20 attributes\n    Map&lt;String, String&gt; filtersOrig = new TreeMap&lt;String, String&gt;();\n    filtersOrig.put(\"number_instances\", \"100..10000\");\n    filtersOrig.put(\"number_features\", \"4..20\");\n    filtersOrig.put(\"limit\", \"250\");\n    Tasks tasksOrig = openml.taskList(filtersOrig);\n\n    // create the study\n    Study study = new Study(null, \"test\", \"test\", null, tasksOrig.getTaskIds(), null);\n    int studyId = openml.studyUpload(study);\n\n    // until the benchmark suite is activated, we can also add some more tasks. Search for the letter dataset:\n    Map&lt;String, String&gt; filtersAdd = new TreeMap&lt;String, String&gt;();\n    filtersAdd.put(\"data_name\", \"letter\");\n    filtersAdd.put(\"limit\", \"1\");\n    Tasks tasksAdd = openml.taskList(filtersAdd);\n    openml.studyAttach(studyId, Arrays.asList(tasksAdd.getTaskIds()));\n\n    // or even remove these again\n    openml.studyDetach(studyId, Arrays.asList(tasksAdd.getTaskIds()));\n\n    // download the study\n    Study studyDownloaded = openml.studyGet(studyId);\n    assertArrayEquals(tasksOrig.getTaskIds(), studyDownloaded.getTasks());\n}\n</code></pre> R example <pre><code>TODO\n</code></pre>"},{"location":"benchmark/benchmark/#further-code-examples-and-use-cases","title":"Further code examples and use cases","text":"<p>As mentioned above, we host a GitHub repository with additional tools and scripts to easily create and use new benchmark studies. It includes:</p> <ul> <li>A Jupyter Notebook that builds a new benchmark suite with datasets that adhere to strict and complex conditions, as well as automated tests to remove tasks that are too easy for proper benchmarking.</li> <li>A Jupyter Notebook that shows how to pull in the latest state-of-the-art results for any of the benchmark suites</li> <li>A Jupyter Notebook that does a detailed analysis of all results in a benchmark suite, and an example run on the OpenML-CC18. It includes a wide range of plots and rankings to get a deeper insight into the benchmark results.</li> <li>Scripts in Python and R to facilitate common subtasks.</li> </ul> <p>We very much welcome new scripts and notebooks, or improvements to the existing ones, that help others to create benchmark suites and analyse benchmarking results.</p>"},{"location":"benchmark/benchmark/#list-of-benchmarking-suites","title":"List of benchmarking suites","text":""},{"location":"benchmark/benchmark/#openml-cc18","title":"OpenML-CC18","text":"<p>The OpenML-CC18 suite contains all OpenML datasets from mid-2018 that satisfy a large set of clear requirements for thorough yet practical benchmarking. It includes datasets frequently used in benchmarks published over the last years, so it can be used as a drop-in replacement for many benchmarking setups.</p> <p>List of datasets and properties</p> <p>The suite is defined as the set of all verified OpenML datasets that satisfy the following requirements:</p> <ul> <li>the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small and not too big,</li> <li>the number of features does not exceed 5000 features to keep the runtime of algorithms low,</li> <li>the target attribute has at least two classes</li> <li>the ratio of the minority class and the majority class is above 0.05, to eliminate highly imbalanced datasets which require special treatment for both algorithms and evaluation measures.</li> </ul> <p>We excluded datasets which:</p> <ul> <li>are artificially generated (not to confuse with simulated)</li> <li>cannot be randomized via a 10-fold cross-validation due to grouped samples or because they are time series or data streams</li> <li>are a subset of a larger dataset</li> <li>have classes with less than 20 observations</li> <li>have no source or reference available</li> <li>can be perfectly classified by a single attribute or a decision stump</li> <li>allow a decision tree to achieve 100% accuracy on a 10-fold cross-validation task</li> <li>have more than 5000 features after one-hot-encoding categorical features</li> <li>are created by binarization of regression tasks or multiclass classification tasks, or</li> <li>are sparse data (e.g., text mining data sets)</li> </ul> Detailed motivation of these decisions <p>We chose the CC18 datasets to allow for practical benchmarking based on the characteristics that might be problematic based on our experience, and to avoid common pitfalls that may invalidate benchmark studies:  </p> <ul> <li>We used at least 500 data points to allow performing cross-validation while still having a large-enough test split.</li> <li>We limited the datasets to 100.000 data points to allow the algorithms to train machine learning models in a reasonable amount of time.</li> <li>We limited the number of features to 5000 to allow the usage of algorithms which scale unfavourably in the number of features. This limitation, together with the two limitations above aims to allow running all \u201cstandard\u201d machine learning algorithms (naive bayes, linear models, support vector machines, tree-based ensemble methods and neural networks) on the benchmark suite.</li> <li>We required each dataset to have at least two classes to be able to work in a supervised classification setting.</li> <li>We require each class to have at least 20 observations to be able to perform stratified cross-validation where there is at least one observation from each class in each split. We have found that not having all classes present in all training and test sets can make several machine learning packages fail.</li> <li>We require a certain balancedness (ratio of minority class to majority class) to prevent cases where only predicting the majority class would be beneficial. This is most likely the restriction which is most debatable, but we found it very helpful to apply a large set of machine learning algorithms across several libraries to the study. We expect that future studies focus more on imbalanced datasets. </li> </ul> <p>Furthermore, we aimed to have the dataset collection as general as possible, rule out as few algorithms as possible and have it usable as easily as possible:</p> <ul> <li>We strived to remove artificial datasets as they, for example, come from textbooks and it is hard to reliably assess their difficulty. We admit that there is a blurred line between artificial and simulated datasets and do not have a perfect distinction between them (for example, a lot of phenomena can be simulated, but the outcome might be like a simple, artificial dataset). Therefore, we removed datasets if we were in doubt of whether they are simulated or artificial. </li> <li>We removed datasets which require grouped sampling because they are time series or data streams which should be treated with special care by machine learning algorithms (i.e., taking the time aspect into account). To be on the safe side, we also removed datasets where each sample constitutes a single data stream.</li> <li>We removed datasets which are a subset of larger datasets. Allowing subsets would be very subjective as there is no objective choice of a dataset subset size or a subset of the variables or classes. Therefore, creating dataset subsets would open a Pandora\u2019s Box.</li> <li>We removed datasets which have no source or reference available to potentially learn more about these datasets if we observe unexpected behavior in future studies. In contrast, we would not be able to learn more about the background of a dataset which has no description and publication attached, leaving us with a complete black box.</li> <li>We removed datasets which can be perfectly classified by a single attribute or a decision stump as they do not allow to meaningfully compare machine learning algorithms (they all achieve 100% accuracy unless the hyperparameters are set in a bogus way).</li> <li>We removed datasets where a decision tree could achieve 100% accuracy on a 10-fold cross-validation task to remove datasets which can be solved by a simple algorithm which is prone to overfitting training data. We found that this is a good indicator of too easy datasets. Obviously, other datasets will appear easy for several algorithms, and we aim to learn more about the characteristics of such datasets in future studies.</li> <li>We removed datasets which have more than 5000 features after one-hot-encoding categorical features. One-hot-encoding is the most frequent way to deal with categorical variables across the different machine learning libraries MLR, scikit-learn and WEKA. In order to limit the number of features to 5000 as explained above, we imposed the additional constraint that this should be counted after one-hot-encoding to allow wide applicability of the benchmark suite.</li> <li>We removed datasets which were created by binarization of regression tasks or multiclass classification task for similar reasons as for forbidding dataset subsets.</li> <li>We did not include sparse datasets because not all machine learning libraries (i.e., all machine learning models) can handle them gracefully, which is in contrast to our goal which is wide applicability.</li> </ul>"},{"location":"benchmark/benchmark/#citing-the-openml-cc18","title":"Citing the OpenML-CC18","text":"<p>If you have used the OpenML-CC18 in a scientific publication, we would appreciate citations of core OpenML packages as well as a citation of the following paper:</p> <p>Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin. OpenML Benchmarking Suites. arXiv 1708.0373v2 (2019): 1-6</p>"},{"location":"benchmark/benchmark/#openml100","title":"OpenML100","text":"<p>The OpenML100 was a predecessor of the OpenML-CC18, consisting of 100 classification datasets. We recommend that you use the OpenML-CC18 instead, because the OpenML100 suffers from some teething issues in the design of benchmark suites. For instance, it contains several datasets that are too easy to model with today's machine learning algorithms, as well as datasets that represent time series analysis problems. These do not invalidate benchmarks run on the OpenML100, but may obfuscate the interpretation of results. The 'OpenML-CC18' handle is also more descriptive and allows easier versioning. The OpenML100 was first published in the Arxiv preprint OpenML Benchmarking Suites and the OpenML100.</p> <p>List of datasets and properties</p> <p>For reference, the OpenML100 included datasets satisfying the following requirements:</p> <ul> <li>the number of observations are between 500 and 100000 to focus on medium-sized datasets, that are not too small for proper training and not too big for practical experimentation</li> <li>the number of features does not exceed 5000 features to keep the runtime of algorithms low</li> <li>the target attribute has at least two classes</li> <li>he ratio of the minority class and the majority class is above 0.05 to eliminate highly imbalanced datasets that would obfuscate a clear analysis</li> </ul> <p>It excluded datasets which:</p> <ul> <li>cannot be randomized via a 10-fold cross-validation due to grouped samples</li> <li>have an unknown origin or no clearly defined task</li> <li>are variants of other datasets (e.g. binarized regression tasks)</li> <li>include sparse data (e.g., text mining data sets)</li> </ul>"},{"location":"benchmark/benchmark/#citing-the-openml100","title":"Citing the OpenML100","text":"<p>If you have used the OpenML100 in a scientific publication, we would appreciate citations of core OpenML packages as well as a citation of the following paper:</p> <p>Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin. OpenML Benchmarking Suites and the OpenML100. arXiv 1708.0373v1 (2017): 1-6</p>"},{"location":"benchmark/benchmark/#need-help","title":"Need help?","text":"<p>We are happy to answer to any suggestion or question you may have. For general questions or issues, please open an issue in the benchmarking issue tracker. If the issue lies with one of the language-specific bindings, please post an issue in the appropriate issue tracker.</p>"},{"location":"benchmark/automl/AutoML-Benchmark/","title":"Getting Started","text":"<p>The AutoML Benchmark is a tool for benchmarking AutoML frameworks on tabular data. It automates the installation of AutoML frameworks, passing it data, and evaluating their predictions.  Our paper describes the design and showcases  results from an evaluation using the benchmark.  This guide goes over the minimum steps needed to evaluate an AutoML framework on a toy dataset.</p> <p>Full instructions can be found in the API Documentation.</p>"},{"location":"benchmark/automl/AutoML-Benchmark/#installation","title":"Installation","text":"<p>These instructions assume that Python 3.9 (or higher)  and git are installed, and are available under the alias <code>python</code> and <code>git</code>, respectively. We recommend Pyenv for managing multiple Python installations, if applicable. We support Ubuntu 22.04, but many linux and MacOS versions likely work (for MacOS, it may be necessary to have <code>brew</code> installed).</p> <p>First, clone the repository:</p> <pre><code>git clone https://github.com/openml/automlbenchmark.git --branch stable --depth 1\ncd automlbenchmark\n</code></pre> <p>Create a virtual environments to install the dependencies in:</p>"},{"location":"benchmark/automl/AutoML-Benchmark/#linux","title":"Linux","text":"<pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"benchmark/automl/AutoML-Benchmark/#macos","title":"MacOS","text":"<pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre>"},{"location":"benchmark/automl/AutoML-Benchmark/#windows","title":"Windows","text":"<pre><code>python -m venv ./venv\nvenv/Scripts/activate\n</code></pre> <p>Then install the dependencies:</p> <pre><code>python -m pip install --upgrade pip\npython -m pip install -r requirements.txt\n</code></pre> Note for Windows users <p>The automated installation of AutoML frameworks is done using shell script, which doesn't work on Windows. We recommend you use Docker to run the examples below. First, install and run <code>docker</code>.  Then, whenever there is a <code>python runbenchmark.py ...</code>  command in the tutorial, add <code>-m docker</code> to it (<code>python runbenchmark.py ... -m docker</code>).</p> Problem with the installation? <p>On some platforms, we need to ensure that requirements are installed sequentially. Use <code>xargs -L 1 python -m pip install &lt; requirements.txt</code> to do so. If problems  persist, open an issue with the error and information about your environment (OS, Python version, pip version).</p>"},{"location":"benchmark/automl/AutoML-Benchmark/#running-the-benchmark","title":"Running the Benchmark","text":"<p>To run a benchmark call the <code>runbenchmark.py</code> script specifying the framework to evaluate.</p> <p>See the API Documentation. for more information on the parameters available.</p>"},{"location":"benchmark/automl/basic_example/","title":"Random Forest Baseline","text":"<p>Let's try evaluating the <code>RandomForest</code> baseline, which uses scikit-learn's random forest:</p>"},{"location":"benchmark/automl/basic_example/#running-the-benchmark","title":"Running the Benchmark","text":""},{"location":"benchmark/automl/basic_example/#linux","title":"Linux","text":"<pre><code>python runbenchmark.py randomforest \n</code></pre>"},{"location":"benchmark/automl/basic_example/#macos","title":"MacOS","text":"<pre><code>python runbenchmark.py randomforest \n</code></pre>"},{"location":"benchmark/automl/basic_example/#windows","title":"Windows","text":"<p>As noted above, we need to install the AutoML frameworks (and baselines) in a container. Add <code>-m docker</code> to the command as shown: <pre><code>python runbenchmark.py randomforest -m docker\n</code></pre></p> <p>Important</p> <p>Future example usages will only show invocations without <code>-m docker</code> mode, but Windows users will need to run in some non-local mode.</p>"},{"location":"benchmark/automl/basic_example/#results","title":"Results","text":"<p>After running the command, there will be a lot of output to the screen that reports on what is currently happening. After a few minutes final results are shown and should  look similar to this:</p> <pre><code>Summing up scores for current run:\n               id        task  fold    framework constraint     result      metric  duration      seed\nopenml.org/t/3913         kc2     0 RandomForest       test   0.865801         auc      11.1 851722466\nopenml.org/t/3913         kc2     1 RandomForest       test   0.857143         auc       9.1 851722467\n  openml.org/t/59        iris     0 RandomForest       test  -0.120755 neg_logloss       8.7 851722466\n  openml.org/t/59        iris     1 RandomForest       test  -0.027781 neg_logloss       8.5 851722467\nopenml.org/t/2295 cholesterol     0 RandomForest       test -44.220800    neg_rmse       8.7 851722466\nopenml.org/t/2295 cholesterol     1 RandomForest       test -55.216500    neg_rmse       8.7 851722467\n</code></pre> <p>The result denotes the performance of the framework on the test data as measured by the metric listed in the metric column. The result column always denotes performance  in a way where higher is better (metrics which normally observe \"lower is better\" are converted, which can be observed from the <code>neg_</code> prefix).</p> <p>While running the command, the AutoML benchmark performed the following steps:</p> <ol> <li>Create a new virtual environment for the Random Forest experiment.      This environment can be found in <code>frameworks/randomforest/venv</code> and will be re-used      when you perform other experiments with <code>RandomForest</code>.</li> <li>It downloaded datasets from OpenML complete with a      \"task definition\" which specifies cross-validation folds.</li> <li>It evaluated <code>RandomForest</code> on each (task, fold)-combination in a separate subprocess, where:<ol> <li>The framework (<code>RandomForest</code>) is initialized.</li> <li>The training data is passed to the framework for training.</li> <li>The test data is passed to the framework to make predictions on.</li> <li>It passes the predictions back to the main process</li> </ol> </li> <li>The predictions are evaluated and reported on. They are printed to the console and      are stored in the <code>results</code> directory. There you will find:<ol> <li><code>results/results.csv</code>: a file with all results from all benchmarks conducted on your machine.</li> <li><code>results/randomforest.test.test.local.TIMESTAMP</code>: a directory with more information about the run,     such as logs, predictions, and possibly other artifacts.</li> </ol> </li> </ol> <p>Docker Mode</p> <p>When using docker mode (with <code>-m docker</code>) a docker image will be made that contains the virtual environment. Otherwise, it functions much the same way.</p>"},{"location":"benchmark/automl/basic_example/#important-parameters","title":"Important Parameters","text":"<p>As you can see from the results above, the  default behavior is to execute a short test benchmark. However, we can specify a different benchmark, provide different constraints, and even run the experiment in a container or on AWS. There are many parameters for the <code>runbenchmark.py</code> script, but the most important ones are:</p>"},{"location":"benchmark/automl/basic_example/#framework-required","title":"Framework (required)","text":"<ul> <li>The AutoML framework or baseline to evaluate and is not case-sensitive. See   integrated frameworks for a list of supported frameworks.    In the above example, this benchmarked framework <code>randomforest</code>.</li> </ul>"},{"location":"benchmark/automl/basic_example/#benchmark-optional-defaulttest","title":"Benchmark (optional, default='test')","text":"<ul> <li>The benchmark suite is the dataset or set of datasets to evaluate the framework on.   These can be defined as on OpenML as a study or task    (formatted as <code>openml/s/X</code> or <code>openml/t/Y</code> respectively) or in a local file.   The default is a short evaluation on two folds of <code>iris</code>, <code>kc2</code>, and <code>cholesterol</code>.</li> </ul>"},{"location":"benchmark/automl/basic_example/#constraints-optional-defaulttest","title":"Constraints (optional, default='test')","text":"<ul> <li> <p>The constraints applied to the benchmark as defined by default in constraints.yaml.   These include time constraints, memory constrains, the number of available cpu cores, and more.   Default constraint is <code>test</code> (2 folds for 10 min each). </p> <p>Constraints are not enforced!</p> <p>These constraints are forwarded to the AutoML framework if possible but, except for runtime constraints, are generally not enforced. It is advised when benchmarking to use an environment that mimics the given constraints.</p> Constraints can be overriden by <code>benchmark</code> <p>A benchmark definition can override constraints on a task level. This is useful if you want to define a benchmark which has different constraints for different tasks. The default \"test\" benchmark does this to limit runtime to 60 seconds instead of 600 seconds, which is useful to get quick results for its small datasets. For more information, see defining a benchmark.</p> </li> </ul>"},{"location":"benchmark/automl/basic_example/#mode-optional-defaultlocal","title":"Mode (optional, default='local')","text":"<ul> <li> <p>The benchmark can be run in four modes:</p> <ul> <li><code>local</code>: install a local virtual environment and run the benchmark on your machine.</li> <li><code>docker</code>: create a docker image with the virtual environment and run the benchmark in a container on your machine.               If a local or remote image already exists, that will be used instead. Requires Docker.</li> <li><code>singularity</code>: create a singularity image with the virtual environment and run the benchmark in a container on your machine. Requires Singularity.</li> <li><code>aws</code>: run the benchmark on AWS EC2 instances.           It is possible to run directly on the instance or have the EC2 instance run in <code>docker</code> mode.           Requires valid AWS credentials to be configured, for more information see Running on AWS.</li> </ul> </li> </ul> <p>For a full list of parameters available, run:</p> <pre><code>python runbenchmark.py --help\n</code></pre>"},{"location":"benchmark/automl/benchmark_on_openml/","title":"Example: Benchmarks on OpenML","text":"<p>In the previous examples, we used benchmarks which were defined in a local file (test.yaml and  validation.yaml, respectively).  However, we can also use tasks and benchmarking suites defined on OpenML directly from the command line. When referencing an OpenML task or suite, we can use <code>openml/t/ID</code> or <code>openml/s/ID</code> respectively as  argument for the benchmark parameter. Running on the iris task:</p> <pre><code>python runbenchmark.py randomforest openml/t/59\n</code></pre> <p>or on the entire AutoML benchmark classification suite (this will take hours!):</p> <pre><code>python runbenchmark.py randomforest openml/s/271\n</code></pre> <p>Large-scale Benchmarking</p> <p>For large scale benchmarking it is advised to parallelize your experiments, as otherwise it may take months to run the experiments. The benchmark currently only supports native parallelization in <code>aws</code> mode (by using the <code>--parallel</code> parameter), but using the <code>--task</code> and <code>--fold</code> parameters  it is easy to generate scripts that invoke individual jobs on e.g., a SLURM cluster. When you run in any parallelized fashion, it is advised to run each process on separate hardware to ensure experiments can not interfere with each other.</p>"},{"location":"benchmark/automl/important_params/","title":"Important Parameters","text":"<p>As you can see from the results above, the  default behavior is to execute a short test benchmark. However, we can specify a different benchmark, provide different constraints, and even run the experiment in a container or on AWS. There are many parameters for the <code>runbenchmark.py</code> script, but the most important ones are:</p> <p><code>Framework (required)</code></p> <ul> <li>The AutoML framework or baseline to evaluate and is not case-sensitive. See   integrated frameworks for a list of supported frameworks.    In the above example, this benchmarked framework <code>randomforest</code>.</li> </ul> <p><code>Benchmark (optional, default='test')</code></p> <ul> <li>The benchmark suite is the dataset or set of datasets to evaluate the framework on.   These can be defined as on OpenML as a study or task    (formatted as <code>openml/s/X</code> or <code>openml/t/Y</code> respectively) or in a local file.   The default is a short evaluation on two folds of <code>iris</code>, <code>kc2</code>, and <code>cholesterol</code>.</li> </ul> <p><code>Constraints (optional, default='test')</code></p> <ul> <li> <p>The constraints applied to the benchmark as defined by default in constraints.yaml.   These include time constraints, memory constrains, the number of available cpu cores, and more.   Default constraint is <code>test</code> (2 folds for 10 min each). </p> <p>Constraints are not enforced!</p> <p>These constraints are forwarded to the AutoML framework if possible but, except for runtime constraints, are generally not enforced. It is advised when benchmarking to use an environment that mimics the given constraints.</p> Constraints can be overriden by <code>benchmark</code> <p>A benchmark definition can override constraints on a task level. This is useful if you want to define a benchmark which has different constraints for different tasks. The default \"test\" benchmark does this to limit runtime to 60 seconds instead of 600 seconds, which is useful to get quick results for its small datasets. For more information, see defining a benchmark.</p> </li> </ul> <p><code>Mode (optional, default='local')</code></p> <ul> <li> <p>The benchmark can be run in four modes:</p> <ul> <li><code>local</code>: install a local virtual environment and run the benchmark on your machine.</li> <li><code>docker</code>: create a docker image with the virtual environment and run the benchmark in a container on your machine.               If a local or remote image already exists, that will be used instead. Requires Docker.</li> <li><code>singularity</code>: create a singularity image with the virtual environment and run the benchmark in a container on your machine. Requires Singularity.</li> <li><code>aws</code>: run the benchmark on AWS EC2 instances.           It is possible to run directly on the instance or have the EC2 instance run in <code>docker</code> mode.           Requires valid AWS credentials to be configured, for more information see Running on AWS.</li> </ul> </li> </ul> <p>For a full list of parameters available, run:</p> <pre><code>python runbenchmark.py --help\n</code></pre>"},{"location":"benchmark/automl/specific_task_fold_example/","title":"Example: AutoML on a specific task and fold","text":"<p>The defaults are very useful for performing a quick test, as the datasets are small and cover different task types (binary classification, multiclass classification, and  regression). We also have a \"validation\" benchmark suite for more elaborate testing that also includes missing data, categorical data,  wide data, and more. The benchmark defines 9 tasks, and evaluating two folds with a 10-minute time constraint would take roughly 3 hours (=9 tasks * 2 folds * 10 minutes, plus overhead). Let's instead use the <code>--task</code> and <code>--fold</code> parameters to run only a specific task and fold in the <code>benchmark</code> when evaluating the  flaml AutoML framework:</p> <pre><code>python runbenchmark.py flaml validation test -t eucalyptus -f 0\n</code></pre> <p>This should take about 10 minutes plus the time it takes to install <code>flaml</code>. Results should look roughly like this:</p> <pre><code>Processing results for flaml.validation.test.local.20230711T122823\nSumming up scores for current run:\n               id       task  fold framework constraint    result      metric  duration       seed\nopenml.org/t/2079 eucalyptus     0     flaml       test -0.702976 neg_logloss     611.0 1385946458\n</code></pre> <p>Similarly to the test run, you will find additional files in the <code>results</code> directory.</p>"},{"location":"concepts/","title":"Concepts","text":""},{"location":"concepts/#openml-concepts","title":"OpenML concepts","text":"<p>OpenML operates on a number of core concepts which are important to understand: </p> <p> Datasets Datasets are pretty straight-forward. Tabular datasets are self-contained, consisting of a number of rows (instances) and columns (features), including their data types. Other  modalities (e.g. images) are included via paths to files stored within the same folder. Datasets are uniformly formatted (S3 buckets with Parquet tables, JSON metadata, and media files), and are auto-converted and auto-loaded in your desired format by the APIs (e.g. in Python) in a single line of code. Example: The Iris dataset or the Plankton dataset</p> <p> Tasks A task consists of a dataset, together with a machine learning task to perform, such as classification or clustering and an evaluation method. For supervised tasks, this also specifies the target column in the data. Example: Classifying different iris species from other attributes and evaluate using 10-fold cross-validation.</p> <p> Flows A flow identifies a particular machine learning algorithm (a pipeline or untrained model) from a particular library or framework, such as scikit-learn, pyTorch, or MLR. It contains details about the structure of the model/pipeline, dependencies (e.g. the library and its version) and a list of settable hyperparameters. In short, it is a serialized description of the algorithm that in many cases can also be deserialized to reinstantiate the exact same algorithm in a particular library.  Example: scikit-learn's RandomForest or a simple TensorFlow model</p> <p> Runs A run is an experiment - it evaluates a particular flow (pipeline/model) with particular hyperparameter settings, on a particular task. Depending on the task it will include certain results, such as model evaluations (e.g. accuracies), model predictions, and other output files (e.g. the trained model). Example: Classifying Gamma rays with scikit-learn's RandomForest</p>"},{"location":"concepts/authentication/","title":"Authentication","text":"<p>OpenML is as open as possible. You can download and inspect all datasets, tasks, flows and runs through the website or the API without creating an account.</p> <p>However, if you want to upload datasets or experiments, you need to create an account, sign in, and find your API key on your profile page. This key can then be used with any of the OpenML APIs.</p>"},{"location":"concepts/authentication/#api-keys","title":"API keys","text":"<p>If you don\u2019t have an account yet, sign up now. You will receive an API key, which will authenticate you to the server and allow you to download and upload datasets, tasks, runs and flows.</p> <ul> <li>Create an OpenML account (free) on https://www.openml.org.</li> <li>After logging in, open your profile page. Click on the avatar on the top right, and choose 'Your Profile'.</li> <li>Click on 'API key' to find your API key. You can also reset it if needed.</li> </ul> <p>To store your API key locally (to permanently authenticate), create a plain text file ~/.openml/config with the line 'apikey=MYKEY', replacing MYKEY with your API key. The config file must be in the directory ~/.openml/config and exist prior to importing the openml module.</p>"},{"location":"concepts/benchmarking/","title":"Collections and benchmarks","text":"<p>You can combine tasks and runs into collections, to run experiments across many tasks at once and collect all results. Each collection gets its own page, which can be linked to publications so that others can find all the details online.</p>"},{"location":"concepts/benchmarking/#benchmarking-suites","title":"Benchmarking suites","text":"<p>Collections of tasks can be published as benchmarking suites. Seamlessly integrated into the OpenML platform, benchmark suites standardize the setup, execution, analysis, and reporting of benchmarks. Moreover, they make benchmarking a whole lot easier: - all datasets are uniformly formatted in standardized data formats - they can be easily downloaded programmatically through APIs and client libraries - they come with machine-readable meta-information, such as the occurrence of missing  values, to train algorithms correctly - standardized train-test splits are provided to ensure that results can be objectively compared - results can be shared in a reproducible way through the APIs - results from other users can be easily downloaded and reused </p> <p>You can search for all existing benchmarking suites or create your own. For all further details, see the benchmarking guide.</p> <p></p>"},{"location":"concepts/benchmarking/#benchmark-studies","title":"Benchmark studies","text":"<p>Collections of runs can be published as benchmarking studies. They contain the results of all runs (possibly millions) executed on a specific benchmarking suite. OpenML allows you to easily download all such results at once via the APIs, but also visualized them online in the Analysis tab (next to the complete list of included tasks and runs). Below is an example of a benchmark study for AutoML algorithms.</p> <p></p>"},{"location":"concepts/data/","title":"Data","text":""},{"location":"concepts/data/#discovery","title":"Discovery","text":"<p>OpenML allows fine-grained search over thousands of machine learning datasets. Via the website, you can filter by many dataset properties, such as size, type, format, and many more. Via the APIs you have access to many more filters, and you can download a complete table with statistics of all datasest. Via the APIs you can also load datasets directly into your preferred data structures such as numpy (example in Python). We are also working on better organization of all datasets by topic </p> <p></p>"},{"location":"concepts/data/#sharing","title":"Sharing","text":"<p>You can upload and download datasets through the website or though the APIs (recommended). You can share data directly from common data science libraries, e.g. from Python or R dataframes, in a few lines of code. The OpenML APIs will automatically extract lots of meta-data and store all datasets in a uniform format.</p> <pre><code>    import pandas as pd\n    import openml as oml\n\n    # Create an OpenML dataset from a pandas dataframe\n    df = pd.DataFrame(data, columns=attribute_names)\n    my_data = oml.datasets.functions.create_dataset(\n        name=\"covertype\", description=\"Predicting forest cover ...\",\n        licence=\"CC0\", data=df\n    )\n\n    # Share the dataset on OpenML\n    my_data.publish()\n</code></pre> <p>Every dataset gets a dedicated page on OpenML with all known information, and can be edited further online.</p> <p></p> <p>Data hosted elsewhere can be referenced by URL. We are also working on interconnecting OpenML with other machine learning data set repositories </p>"},{"location":"concepts/data/#automated-analysis","title":"Automated analysis","text":"<p>OpenML will automatically analyze the data and compute a range of data quality characteristics. These include simple statistics such as the number of examples and features, but also potential quality issues (e.g. missing values) and more advanced statistics (e.g. the mutual information in the features and benchmark performances of simple models). These can be useful to find, filter and compare datasets, or to automate data preprocessing.  We are also working on simple metrics and automated dataset quality reports </p> <p>The Analysis tab (see image below, or try it live) also shows an automated and interactive analysis of all datasets. This runs on open-source Python code via Dash and we welcome all contributions </p> <p></p> <p>The third tab, 'Tasks', lists all tasks created on the dataset. More on that below.</p>"},{"location":"concepts/data/#dataset-id-and-versions","title":"Dataset ID and versions","text":"<p>A dataset can be uniquely identified by its dataset ID, which is shown on the website and returned by the API. It's <code>1596</code> in the <code>covertype</code> example above. They can also be referenced by name and ID. OpenML assigns incremental version numbers per upload with the same name. You can also add a free-form <code>version_label</code> with every upload.</p>"},{"location":"concepts/data/#dataset-status","title":"Dataset status","text":"<p>When you upload a dataset, it will be marked <code>in_preparation</code> until it is (automatically) verified. Once approved, the dataset will become <code>active</code> (or <code>verified</code>). If a severe issue has been found with a dataset, it can become <code>deactivated</code> (or <code>deprecated</code>) signaling that it should not be used. By default, dataset search only returns verified datasets, but you can access and download datasets with any status.</p>"},{"location":"concepts/data/#special-attributes","title":"Special attributes","text":"<p>Machine learning datasets often have special attributes that require special handling in order to build useful models. OpenML marks these as special attributes.</p> <p>A <code>target</code> attribute is the column that is to be predicted, also known as dependent variable. Datasets can have a default target attribute set by the author, but OpenML tasks can also overrule this. Example: The default target variable for the MNIST dataset is to predict the class from pixel values, and most supervised tasks will have the class as their target. However, one can also create a task aimed at predicting the value of pixel257 given all the other pixel values and the class column.</p> <p><code>Row id</code> attributes indicate externally defined row IDs (e.g. <code>instance</code> in dataset 164). <code>Ignore</code> attributes are other columns that should not be included in training data (e.g. <code>Player</code> in dataset 185). OpenML will clearly mark these, and will (by default) drop these columns when constructing training sets.</p>"},{"location":"concepts/flows/","title":"Flows","text":"<p>Flows are machine learning pipelines, models, or scripts. They are typically uploaded directly from machine learning libraries (e.g. scikit-learn, pyTorch, TensorFlow, MLR, WEKA,...) via the corresponding APIs. Associated code (e.g., on GitHub) can be referenced by URL.</p>"},{"location":"concepts/flows/#analysing-algorithm-performance","title":"Analysing algorithm performance","text":"<p>Every flow gets a dedicated page with all known information. The Analysis tab shows an automated interactive analysis of all collected results. For instance, below are the results of a scikit-learn pipeline including missing value imputation, feature encoding, and a RandomForest model. It shows the results across multiple tasks, and how the AUC score is affected by certain hyperparameters.</p> <p></p> <p>This helps to better understand specific models, as well as their strengths and weaknesses.</p>"},{"location":"concepts/flows/#automated-sharing","title":"Automated sharing","text":"<p>When you evaluate algorithms and share the results, OpenML will automatically extract all the details of the algorithm (dependencies, structure, and all hyperparameters), and upload them in the background.</p> <pre><code>    from sklearn import ensemble\n    from openml import tasks, runs\n\n    # Build any model you like.\n    clf = ensemble.RandomForestClassifier()\n\n    # Evaluate the model on a task\n    run = runs.run_model_on_task(clf, task)\n\n    # Share the results, including the flow and all its details.\n    run.publish()\n</code></pre>"},{"location":"concepts/flows/#reproducing-algorithms-and-experiments","title":"Reproducing algorithms and experiments","text":"<p>Given an OpenML run, the exact same algorithm or model, with exactly the same hyperparameters, can be reconstructed within the same machine learning library to easily reproduce earlier results. </p> <pre><code>    from openml import runs\n\n    # Rebuild the (scikit-learn) pipeline from run 9864498\n    model = openml.runs.initialize_model_from_run(9864498)\n</code></pre> <p>Note</p> <p>You may need the exact same library version to reconstruct flows. The API will always state the required version. We aim to add support for VMs so that flows can be easily (re)run in any environment </p>"},{"location":"concepts/runs/","title":"Runs","text":""},{"location":"concepts/runs/#automated-reproducible-evaluations","title":"Automated reproducible evaluations","text":"<p>Runs are experiments (benchmarks) evaluating a specific flows on a specific task. As shown above, they are typically submitted automatically by machine learning libraries through the OpenML APIs), including lots of automatically extracted meta-data, to create reproducible experiments. With a few for-loops you can easily run (and share) millions of experiments.</p>"},{"location":"concepts/runs/#online-organization","title":"Online organization","text":"<p>OpenML organizes all runs online, linked to the underlying data, flows, parameter settings, people, and other details. See the many examples above, where every dot in the scatterplots is a single OpenML run.</p>"},{"location":"concepts/runs/#independent-server-side-evaluation","title":"Independent (server-side) evaluation","text":"<p>OpenML runs include all information needed to independently evaluate models. For most tasks, this includes all predictions, for all train-test splits, for all instances in the dataset, including all class confidences. When a run is uploaded, OpenML automatically evaluates every run using a wide array of evaluation metrics. This makes them directly comparable with all other runs shared on OpenML. For completeness, OpenML will also upload locally computed evaluation metrics and runtimes. </p> <p>New metrics can also be added to OpenML's evaluation engine, and computed for all runs afterwards. Or, you can download OpenML runs and analyse the results any way you like.</p> <p>Note</p> <p>Please note that while OpenML tries to maximise reproducibility, exactly reproducing all results may not always be possible because of changes in numeric libraries,  operating systems, and hardware.</p>"},{"location":"concepts/sharing/","title":"Sharing (under construction)","text":"<p>Currently, anything on OpenML can be shared publicly or kept private to a single user. We are working on sharing features that allow you to share your materials with other users without making them entirely public. Watch this space </p>"},{"location":"concepts/tagging/","title":"Tagging","text":"<p>Datasets, tasks, runs and flows can be assigned tags, either via the web interface or the API. These tags can be used to search and annotate datasets, or simply to better organize your own datasets and experiments.</p> <p>For example, the tag OpenML-CC18 refers to all tasks included in the OpenML-CC18 benchmarkign suite.</p>"},{"location":"concepts/tasks/","title":"Tasks","text":"<p>Tasks describe what to do with the data. OpenML covers several task types, such as classification and clustering. Tasks are containers including the data and other information such as train/test splits, and define what needs to be returned. They are machine-readable so that you can automate machine learning experiments, and easily compare algorithms evaluations (using the exact same train-test splits) against all other benchmarks shared by others on OpenML.</p>"},{"location":"concepts/tasks/#collaborative-benchmarks","title":"Collaborative benchmarks","text":"<p>Tasks are real-time, collaborative benchmarks (e.g. see MNIST below). In the Analysis tab, you can view timelines and leaderboards, and learn from all prior submissions to design even better algorithms.</p> <p></p>"},{"location":"concepts/tasks/#discover-the-best-algorithms","title":"Discover the best algorithms","text":"<p>All algorithms evaluated on the same task (with the same train-test splits) can be directly compared to each other, so you can easily look up which algorithms perform best overall, and download their exact configurations. Likewise, you can look up the best algorithms for similar tasks to know what to try first.</p> <p></p>"},{"location":"concepts/tasks/#automating-benchmarks","title":"Automating benchmarks","text":"<p>You can search and download existing tasks, evaluate your algorithms, and automatically share the results (which are stored in a run). Here's what this looks like in the Python API. You can do the same across hundreds of tasks at once.</p> <pre><code>    from sklearn import ensemble\n    from openml import tasks, runs\n\n    # Build any model you like\n    clf = ensemble.RandomForestClassifier()\n\n    # Download any OpenML task (includes the datasets)\n    task = tasks.get_task(3954)\n\n    # Automatically evaluate your model on the task\n    run = runs.run_model_on_task(clf, task)\n\n    # Share the results on OpenML.\n    run.publish()\n</code></pre> <p>You can create new tasks via the website or via the APIs as well.</p>"},{"location":"contributing/","title":"How to Contribute","text":"<p>OpenML is an open source project, hosted on GitHub. We welcome everybody to help improve OpenML, and make it more useful for everyone.</p> <p>Mission</p> <p>We want to make machine learning open and accessible for the benefit of all of humanity. OpenML offers an entirely open online platform for machine learning datasets, models, and experiments, making them easy to use and share to facilitate global collaboration and extensive automation.</p>"},{"location":"contributing/#want-to-get-involved","title":"Want to get involved?","text":"<p>Awesome, we're happy to have you! </p>"},{"location":"contributing/#who-are-we","title":"Who are we?","text":"<p>We are a group of friendly people who are excited about open science and machine learning. </p> <p>Read more about who we are, what we stand for, and how to get in touch.</p>"},{"location":"contributing/#we-need-help","title":"We need help!","text":"<p>We are currently looking for help with:</p> <p> User feedback (best via GitHub issues, but email or Slack is also fine)</p> <ul> <li>Frontend / UX / Design of the website</li> <li>Backend / API</li> <li>Outreach / making OpenML better known (especially in non-ML-communities, where people have data but no analysis experise)</li> <li>Helping with the interfaces (Python,R,Julia,Java) and tool integrations</li> <li>Helping with documenting the interfaces or the API</li> <li>What could we do better to get new users started? Help us to figure out what is difficult to understand about OpenML. If you are a new user, you are the perfect person for this!</li> </ul>"},{"location":"contributing/#beginner-issues","title":"Beginner issues","text":"<p>Check out the issues labeled Good first issue or help wanted (you need to be logged into GitHub to see these)</p>"},{"location":"contributing/#change-the-world","title":"Change the world","text":"<p>If you have your own ideas on how you want to contribute, please get in touch! We are very friendly and open to new ideas </p>"},{"location":"contributing/#communication-channels","title":"Communication channels:","text":"<p>We have several communication channels set up for different purposes:</p>"},{"location":"contributing/#github","title":"GitHub","text":"<p>https://github.com/openml</p> <ul> <li>Issues (members and users can complain)</li> <li>Request new features</li> </ul> <p>Anyone with a GitHub account can write issues. We are happy if people get involved by writing issues, so don't be shy </p> <p>Please post issues in the relevant issue tracker.</p> <ul> <li> OpenML Core - Web services and API</li> <li> Website - The (new) OpenML website</li> <li> Docs - The documentation pages</li> <li> Python API - The Python API</li> <li> R API - The OpenML R package</li> <li> Java API - The Java API and Java-based plugins</li> <li> Datasets - For issues about datasets</li> <li> Blog - The OpenML Blog</li> </ul>"},{"location":"contributing/#slack","title":"Slack","text":"<p>https://openml.slack.com</p> <ul> <li>Informal communication</li> </ul> <p>We use slack for day to day discussions and news. If you want to join the OpenML slack chat, please message us (openmlHQ@googlegroups.com).</p>"},{"location":"contributing/#twitter-open_ml","title":"Twitter (@open_ml)","text":"<p>https://twitter.com/open_ml</p> <ul> <li>News</li> <li>Publicly relevant information</li> </ul>"},{"location":"contributing/#blog","title":"Blog","text":"<p>https://blog.openml.org</p> <ul> <li>Tutorials</li> <li>News</li> <li>Open discussions</li> </ul>"},{"location":"contributing/#contributors-bot","title":"Contributors bot","text":"<p>We use all contributors bot to add contributors to the repository README. You can check how to use this here. You can contribute in a lot of ways including code, blogs, content, design and talks. You can find the emoji key here .</p>"},{"location":"contributing/OpenML-Docs/","title":"Documentation","text":""},{"location":"contributing/OpenML-Docs/#general-documentation","title":"General Documentation","text":"<p>High-quality and up-to-date documentation are crucial. If you notice any mistake in these documentation pages, click the  button (on the top right). It will open up an editing page on GitHub (you do need to be logged in). When you are done, add a small message explaining the change and click 'commit changes'. On the next page, just launch the pull request. We will then review it and approve the changes, or discuss them if necessary.</p> <p>The sources are generated by MkDocs, using the Material theme. Check these docs to see what is possible in terms of styling.</p> <p>OpenML is a big project with multiple repositories. To keep the documentation close to the code, it will always be kept in the relevant repositories (see below), and  combined into these documentation pages using MkDocs multirepo.</p> <p>Developer note</p> <p>To work on the documentation locally, do the following: <pre><code>git clone https://github.com/openml/docs.git\npip install -r requirements.txt\n</code></pre> To build the documentation, run <code>mkdocs serve</code> in the top directory (with the <code>mkdocs.yml</code> file). Any changes made after that will be hot-loaded.</p> <p>The documentation will be auto-deployed with every push or merge with the master branch of <code>https://www.github.com/openml/docs/</code>. In the background, a CI job will run <code>mkdocs gh-deploy</code>, which will build the HTML files and push them to the gh-pages branch of openml/docs. <code>https://docs.openml.org</code> is just a reverse proxy for <code>https://openml.github.io/docs/</code>.</p>"},{"location":"contributing/OpenML-Docs/#python-api","title":"Python API","text":"<p>To edit the tutorial, you have to edit the <code>reStructuredText</code> files on openml-python/doc. When done, you can do a pull request.</p> <p>To edit the documentation of the python functions, edit the docstrings in the Python code. When done, you can do a pull request.</p> <p>Developer note</p> <p>A CircleCI job will automatically render the documentation on every GitHub commit, using Sphinx. For inclusion in these documentation pages, it will also be rendered in markdown and imported.</p>"},{"location":"contributing/OpenML-Docs/#r-api","title":"R API","text":"<p>To edit the tutorial, you have to edit the <code>Rmarkdown</code> files on openml-r/vignettes.</p> <p>To edit the documentation of the R functions, edit the Roxygen documention next to the functions in the R code.</p> <p>Developer note</p> <p>A Travis job will automatically render the documentation on every GitHub commit, using knitr. The Roxygen documentation is updated every time a new version is released on CRAN.</p>"},{"location":"contributing/OpenML-Docs/#java-api","title":"Java API","text":"<p>The Java Tutorial is written in markdown and can be edited the usual way (see above).</p> <p>To edit the documentation of the Java functions, edit the documentation next to the functions in the Java code.</p> <ul> <li>Javadocs: https://www.openml.org/docs/</li> </ul> <p>Developer note</p> <p>A Travis job will automatically render the documentation on every GitHub commit, using Javadoc.</p>"},{"location":"contributing/OpenML-Docs/#rest-api","title":"REST API","text":"<p>The REST API is documented using Swagger.io, in YAML. This generates a nice web interface that also allows trying out the API calls using your own API key (when you are logged in).</p> <p>You can edit the sources on SwaggerHub. When you are done, export to json and replace the downloads/swagger.json file in the OpenML main GitHub repository. You need to do a pull request that is then reviewed by us. When we merge the new file the changes are immediately available.</p> <p>The data API can be edited in the same way.</p>"},{"location":"contributing/Style/","title":"Style guide","text":"<p>These are some (non-mandatory) style guidelines to make the OpenML experience more pleasant and consistent for everyone.</p>"},{"location":"contributing/Style/#logos","title":"Logos","text":"<p> (SVG)</p>"},{"location":"contributing/Style/#colors","title":"Colors","text":"<p>We use the Material Design color system, and especially the colors green[400], yellow[800], blue[800], red[400], green[400], yellow[800], pink[400], and purple[400].</p> <p>Primary colors are #1E88E5 (general), #000482 (dark), and #b5b7ff (light).</p>"},{"location":"contributing/resources/","title":"Resources","text":""},{"location":"contributing/resources/#resources","title":"Resources","text":""},{"location":"contributing/resources/#database-snapshots","title":"Database snapshots","text":"<p>Everything uploaded to OpenML is available to the community. The nightly snapshot of the public database contains all experiment runs, evaluations and links to datasets, implementations and result files. In SQL format (gzipped). You can also download the Database schema.</p> <p> Nightly database SNAPSHOT</p> <p>If you want to work on the website locally, you'll also need the schema for the 'private' database with non-public information.</p> <p> Private database schema</p>"},{"location":"contributing/resources/#legacy-resources","title":"Legacy Resources","text":"<p>OpenML is always evolving, but we keep hosting the resources that were used in prior publications so that others may still build on them.</p> <p> The experiment database used in Vanschoren et al. (2012) Experiment databases. Machine Learning 87(2), pp 127-158. You'll need to import this database (we used MySQL) to run queries. The database structure is described in the paper. Note that most of the experiments in this database have been rerun using OpenML, using newer algorithm implementations and stored in much more detail.</p> <p> The Expos\u00e9 ontology used in the same paper, and described in more detail here and here. Expos\u00e9 is used in designing our databases, and we aim to use it to export all OpenML data as Linked Open Data.</p>"},{"location":"contributing/resources/#other-dataset-repositories","title":"Other dataset repositories","text":"<p>We keep a list of other dataset repositories all over the world</p>"},{"location":"contributing/backend/API-development/","title":"Code structure (phasing out)","text":"<p>Phasing out</p> <p>This documentation is about the old PHP-based API, which wil be phased out in favor of (e.g. the new Python-based API (using FastAPI)). See the 'Server' tab for more information.</p>"},{"location":"contributing/backend/API-development/#important-resources","title":"Important resources","text":"<p>REST API docs: www.openml.org/apis</p> <p>Controller: https://github.com/openml/OpenML/blob/master/openml_OS/controllers/Api_new.php</p> <p>Models: https://github.com/openml/OpenML/tree/master/openml_OS/models/api/v1</p> <p>Templates: https://github.com/openml/OpenML/tree/master/openml_OS/views/pages/api_new/v1</p>"},{"location":"contributing/backend/API-development/#golden-rules-for-development","title":"Golden Rules for Development","text":"<ol> <li>Code Maintainability before anything else. The code has to be understandable, and if not conflicting with that, short. Avoid code duplications as much as possible.</li> <li>The API controller is the only entity giving access to the API models. Therefore, the responsibility for API access can be handled by the controller</li> <li>Read-Only operations are of the type GET. Operations that make changes in the database are of type POST or DELETE. Important, because this is the way the controller determines to allow users with a given set of privileges to access functions.</li> <li>Try to avoid direct queries to the database. Instead, use the respective models functions: 'get()', 'getWhere()', 'getById()', insert(), etc (Please make yourself familiar with the basic model: read-only and write)</li> <li>No external program/script execution during API calls (with one exception: data split generation). This makes the API unnecessarily slow, hard to debug and vulnerable to crashes. If necessary, make a cronjob that executes the program / script</li> </ol>"},{"location":"contributing/backend/API-development/#backend-code-structure","title":"Backend code structure","text":"<p>The high-level architecture of the website, including the controllers for different parts of the website (REST API, html, ...) and connections to the database.</p>"},{"location":"contributing/backend/API-development/#code","title":"Code","text":"<p>The source code is available in the 'OpenML' repository: https://github.com/openml/OpenML </p>"},{"location":"contributing/backend/API-development/#important-files-and-folders","title":"Important files and folders","text":"<p>In this section we go through all important files and folder of the system.</p>"},{"location":"contributing/backend/API-development/#root-directory","title":"Root directory","text":"<p>The root directory of OpenML contains the following files and folders.</p> <ul> <li> <p>system: This folder contains all files provided by   CodeIgniter 2.1.3. The contents of this folder is   beyond the scope of this document, and not relevant for extending   OpenML. All the files in this folder are in the same state as they   were provided by Ellislabs, and none of these files should ever be   changed.</p> </li> <li> <p>sparks: Sparks is a package management system for   Codeigniter that allows for instant installation of libraries into   the application. This folder contains two libraries provided by   third party software developers, oauth1 (based on version 1 the   oauth protocol) and oauth2 (similarly, based on version 2 of the   oauth protocol). The exact contents of this folder is beyond the   scope of this document and not relevant for extending OpenML.</p> </li> <li> <p>openml_OS: All files in this folder are written specifically   for OpenML. When extending the functionality OpenML, usually one of   the files in this folder needs to be adjusted. As a thorough   understanding of the contents of this folder is vital for extending   OpenML, we will discuss the contents of this folder in   [[URL Mapping]] in more detail.</p> </li> <li> <p>index.php: This is the \u201cbootstrap\u201d file of the system.   Basically, every page request on OpenML goes through this file (with   the css, images and javascript files as only exception). It then   determines which CodeIgniter and OpenML files need to be included.   This file should not be edited.</p> </li> <li> <p>.htaccess: This file (which configures the Apache Rewrite   Engine) makes sure that all URL requests will be directed to   <code>index.php</code>. Without this file, we would need to include <code>index.php</code>   explicitly in every URL request. This file makes sure that all other   URL requests without <code>index.php</code> embedded in it automatically will   be transformed to <code>index.php</code>. Eg.,   http://www.openml.org/frontend/page/home will be rewritten to   http://www.openml.org/index.php/frontend/page/home. This will be   explained in detail in [[URL Mapping]].</p> </li> <li> <p>css: A folder containing all stylesheets. These are important   for the layout of OpenML.</p> </li> <li> <p>data: A folder containing data files, e.g., datasets,   implementation files, uploaded content. Please note that this folder   does not necessarily needs to be present in the root directory. The   OpenML Base Config file determines the   exact location of this folder.</p> </li> <li> <p>downloads: Another data folder, containing files like the most   recent database snapshot.</p> </li> <li> <p>img: A folder containing all static images shown on the webpage.</p> </li> <li> <p>js: A folder containing all used Javascript files and libraries,   including third party libraries like jQuery and datatables.</p> </li> <li> <p>Various other files, like .gitignore, favicon.ico, etc.</p> </li> </ul>"},{"location":"contributing/backend/API-development/#openml_os","title":"openml_OS","text":"<p>This folder is (in CodeIgniter jargon) the \u201cApplication folder\u201d, and contains all files relevant to OpenML. Within this folder, the following folders should be present: (And also some other folders, but these are not used by OpenML)</p> <ul> <li> <p>config: A folder containing all config files. Most notably, it   contains the file BASE_CONFIG.php, in which all system   specific variables are set; the config items within this file   differs over various installations (e.g., on localhost,   <code>openml.org</code>). Most other config files, like   database.php, will receive their values from   BASE_CONFIG.php. Other important config files are   autoload.php, determining which CodeIgniter / OpenML   files will be loaded on any request, openML.php,   containing config items specific to OpenML, and   routes.php, which will be explained in   [[URL Mapping]].</p> </li> <li> <p>controllers: In the Model/View/Controller design pattern, all   user interaction goes through controllers. In a webapplication   setting this means that every time a URL gets requested, exactly one   controller gets invoked. The exact dynamics of this will be   explained in [[URL Mapping]].</p> </li> <li> <p>core: A folder that contains CodeIgniter specific files. These   are not relevant for the understanding of OpenML.</p> </li> <li> <p>helpers: This folder contains many convenience functions.   Wikipedia states: \u201cA convenience function is a non-essential   subroutine in a programming library or framework which is intended   to ease commonly performed tasks\u201d. For example the   file_upload_helper.php contains many functions that   assist with uploading of files. Please note that a helper function   must be explicitly loaded in either the autoload config or the files   that uses its functions.</p> </li> <li> <p>libraries: Similar to sparks, this folder contains libraries   specifically written for CodeIgniter. For example, the library used   for all user management routines is in this folder.</p> </li> <li> <p>models: In the Model/View/Controller design pattern, models   represent the state of the system. In a webapplication setting, you   could say that a model is the link to the database. In OpenML,   almost all tables of the database are represented by a model. Each   model has general functionality applicable to all models (e.g.,   retrieve all records, retrieve record with constraints, insert   record) and functionality specific to that model (e.g., retrieve a   dataset that has certain data properties). Most models extend an   (abstract) base class, located in the abstract folder.   This way, all general functionality is programmed and maintained in   one place.</p> </li> <li> <p>third_party: Although the name might suggests differently, this   folder contains all OpenML Java libraries.</p> </li> <li> <p>views: In the Model/View/Controller design pattern, the views   are the way information is presented on the screen. In a   webapplication setting, a view usually is a block of (PHP generated)   HTML code. The most notable view is frontend_main.php,   which is the template file determining the main look and feel of   OpenML. Every single page also has its own specific view (which is   parsed within frontend_main.php). These pages can be   found (categorized by controller and name) in the pages   folder. More about this structure is explained in   [[URL Mapping]].</p> </li> </ul>"},{"location":"contributing/backend/API-development/#frontend-code-structure","title":"Frontend code structure","text":"<p>Architecture and libraries involved in generating the frontend functions.</p> <p>Code: https://github.com/openml/website/tree/master/openml_OS/views</p>"},{"location":"contributing/backend/API-development/#high-level","title":"High-level","text":"<p>All pages are generated by first loading frontend_main.php. This creates the 'shell' in which the content is loaded. It loads all css and javascript libraries, and contains the html for displaying headers and footers.</p>"},{"location":"contributing/backend/API-development/#create-new-page","title":"Create new page","text":"<p>The preferred method is creating a new folder into the folder <code>&lt;root_directory&gt;/openml_OS/views/pages/frontend</code> This page can be requested by <code>http://www.openml.org/frontend/page/&lt;folder_name&gt;</code> or just <code>http://www.openml.org/&lt;folder_name&gt;</code> This method is preferred for human readable webpages, where the internal actions are simple, and the output is complex. We will describe the files that can be in this folder.</p> <ul> <li> <p>pre.php: Mandatory file. Will be executed first. Do not make   this file produce any output! Can be used to pre-render data, or set   some variables that are used in other files.</p> </li> <li> <p>body.php: Highly recommended file. Intended for displaying the   main content of this file. Will be rendered at the right location   within the template file (<code>frontend_main.php</code>).</p> </li> <li> <p>javascript.php: Non-mandatory file. Intended for javascript   function on which <code>body.php</code> relies. Will be rendered within a   javascript block in the header of the page.</p> </li> <li> <p>post.php: Non mandatory file. Will only be executed when a POST   request is done (e.g., when a HTML form was send using the POST   protocol). Will be executed after <code>pre.php</code>, but before the   rendering process (and thus, before <code>body.php</code> and   <code>javascript.php</code>). Should handle the posted input, e.g., file   uploads.</p> </li> </ul> <p>It is also recommended to add the newly created folder to the mapping in the <code>routes.php</code> config file. This way it can also be requested by the shortened version of the URL. (Note that we deliberately avoided to auto-load all pages into this file using a directory scan, as this makes the webplatform slow.)</p>"},{"location":"contributing/backend/API-development/#url-to-page-mapping","title":"URL to Page Mapping","text":"<p>Most pages in OpenML are represented by a folder in /openml_OS/views/pages/frontend The contents of this folder will be parsed in the template <code>frontend_main.php</code> template, as described in [[backend]]. In this section we explain the way an URL is mapped to a certain OpenML page."},{"location":"contributing/backend/API-development/#url-anatomy","title":"URL Anatomy","text":"<p>By default, CodeIgniter (and OpenML) accepts a URL in the following form: <code>http://www.openml.org/index.php/&lt;controller&gt;/&lt;function&gt;/&lt;p1&gt;/&lt;pN&gt;/&lt;free&gt;</code> The various parts in the URL are divided by slashes. Every URL starts with the protocol and server name (in the case of OpenML this is <code>http://www.openml.org/</code>). This is followed by the bootstrap file, which is always the same, i.e., <code>index.php</code>. The next part indicates the controller that needs to be invoked; typically this is <code>frontend</code>, <code>rest_api</code> or <code>data</code>, but it can be any file from the <code>openml_OS</code> folder <code>controllers</code>. Note that the suffix <code>.php</code> should not be included in the URL.</p> <p>The next part indicates which function of the controller should be invoked. This should be a existing, public function from the controller that is indicated in the controller part. These functions might have one or more parameters that need to be set. This is the following part of the URL (indicated by <code>p1</code> and <code>pN</code>). The parameters can be followed by anything in free format. Typically, this free format is used to pass on additional parameters in <code>name</code> - <code>value</code> format, or just a way of adding a human readable string to the URL for SEO purposes.</p> <p>For example, the following URL <code>http://www.openml.org/index.php/frontend/page/home</code> invokes the function <code>page</code> from the <code>frontend</code> controller and sets the only parameter of this function, <code>$indicator</code>, to value <code>home</code>. The function <code>page</code> loads the content of the specified folder (<code>$indicator</code>) into the main template. In this sense, the function <code>page</code> can be seen as some sort of specialized page loader.</p>"},{"location":"contributing/backend/API-development/#url-shortening","title":"URL Shortening","text":"<p>Since it is good practice to have URL\u2019s as short as possible, we have introduced some logic that shortens the URL\u2019s. Most importantly, the URL part that invokes <code>index.php</code> can be removed at no cost, since this file is always invoked. For this, we use Apache\u2019s rewrite engine. Rules for rewriting URL\u2019s can be found in the <code>.htaccess</code> file, but is suffices to say that any URL in the following format <code>http://www.openml.org/index.php/&lt;controller&gt;/&lt;function&gt;/&lt;params&gt;</code> can due to the rewrite engine also be requested with <code>http://www.openml.org/&lt;controller&gt;/&lt;function&gt;/&lt;params&gt;</code></p> <p>Furthermore, since most of the pages are invoked by the function <code>page</code> of the <code>frontend</code> controller (hence, they come with the suffix <code>frontend/page/page_name</code>) we also created a mapping that maps URL\u2019s in the following form <code>http://www.openml.org/&lt;page_name&gt;</code> to <code>http://www.openml.org/frontend/page/&lt;page_name&gt;</code> Note that Apache\u2019s rewrite engine will also add <code>index.php</code> to this. The exact mapping can be found in <code>routes.php</code> config file.</p>"},{"location":"contributing/backend/API-development/#additional-mappings","title":"Additional Mappings","text":"<p>Additionally, a mapping is created from the following type of URL: <code>http://www.openml.org/api/&lt;any_query_string&gt;</code> to <code>http://www.openml.org/rest_api/&lt;any_query_string&gt;</code> This was done for backwards compatibility. Many plugins make calls to the not-existing <code>api</code> controller, which are automatically redirected to the <code>rest_api</code> controller.</p>"},{"location":"contributing/backend/API-development/#exceptions","title":"Exceptions","text":"<p>It is important to note that not all pages do have a specific page folder. The page folders are a good way of structuring complex GUI\u2019s that need to be presented to the user, but in cases where the internal state changes are more important than the GUI\u2019s, it might be preferable to make the controller function print the output directly. This happens for example in the functions of <code>rest_api.php</code> and <code>free_query.php</code> (although the former still has some files in the views folder that it refers to).</p>"},{"location":"contributing/backend/API-development/#xsd-schemas","title":"XSD Schemas","text":"<p>In order to ensure data integrity on the server, data that passed to upload functions is checked against XSD schema's. This ensures that the data that is uploaded is in the correct format, and does not contain any illegal characters. XSD schema's can be obtained through the API (exact links are provided in the API docs, but for example: https://www.openml.org/api/v1/xsd/openml.data.upload (where openml.data.upload can be replaced by any other schema's name). Also XML examples are provided, e.g., https://www.openml.org/api/v1/xml_example/data . The XSD schema's are exactly the same as used on the server. Whenever an upload fails and the server mentions an XML/XSD verification error, please run the uploaded xml against one of the provided XSD schema's, for example on this webtool: http://www.freeformatter.com/xml-validator-xsd.html</p> <p>In order to maintain one XSD schema for both uploading and downloading stuff, the XSD sometimes contains more fields than seem necessary from the offset. Usually, the additional fields that are indicated as such in the comments (for example, in the upload dataset xsd this are the id, upload_date, etc fields). The XSD's maintain basically three consistencies functions:</p> <ul> <li>Ensure that the correct fields are uploaded</li> <li>Ensure that the fields contain the correct data types.</li> <li>Ensure that the fields do not contain to much characters for the database to upload.</li> </ul> <p>For the latter two, it is important to note that the XSD seldom accept default string content (i.e., xs:string). Rather, we use self defined data types, that use regular expressions to ensure the right content. Examples of these are oml:system_string128, oml:casual_string128, oml:basic_latin128, where the oml prefix is used, the name indicates the level of restriction and the number indicates the maximum size of the field.</p> <p>IMPORTANT: The maximum field sizes are (often) chosen with great care. Do not extend them without consulting other team members.</p>"},{"location":"contributing/backend/API-development/#user-authentication","title":"User authentication","text":"<p>Authentication towards the server goes by means of a so-called api_key (a hexa-decimal string which uniquely identifies a user). Upon interaction with the server, the client passes this api_key to the server, and the server checks the rights of the user. Currently this goes by means of a get or post variable, but in the future we might want to use a header field (because of security). It is recommended to refresh your api_key every month.</p> <p>IMPORTANT: Most authentication operations are handled by the ION_Auth library (http://benedmunds.com/ion_auth/). DO NOT alter information directly in the user table, always use the ION_Auth API.</p> <p>A user can be part of one or many groups. The following user groups exists:</p> <ol> <li>Admin Group: With great power comes great responsibility. Admin users can overrule all security checks on the server, i.e., delete a dataset or run that is not theirs, or even delete a flow that contains runs.</li> <li>Normal Group: Level that is required for read/write interaction with the server. Almost all users are part of this group.</li> <li>Read-only Group: Level that can be used for read interaction with the server. If a user is part of this group, but not part of 'Normal Group', he is allowed to download content, but can not upload or delete content.</li> <li>Backend Group: (Work in Progress) Level that has more privileges than 'Normal Group'. Can submit Data Qualities and Evaluations.</li> </ol> <p>The ION_Auth functions in_group(), add_to_group(), remove_from_group() and get_users_groups() are key towards interaction with these tables.</p>"},{"location":"contributing/backend/Datasets/","title":"Datasets","text":""},{"location":"contributing/backend/Datasets/#data-formats","title":"Data Formats","text":"<p>OpenML aims to achieve full data interoperability, meaning that you can load all datasets in a uniform way (a 'universal dataloader'). This requires that all datasets are stored in the same dataformat (or a set of interoperable formats), or at least have a version of it stored in that format. After an intensive study, which you can read on our blog, we settled on the Parquet format.</p> <p>This means that all OpenML datasets can be retrieved in the Parquet format. They are also stored on our servers in this format. Oftentimes, you will not notice this, as the OpenML clients can automatically convert data into your preferred data structures, and be fed directly into machine learning workflows. For example:</p> <pre><code>import openml\ndataset = openml.datasets.get_dataset(\"Fashion-MNIST\")     # Returns the dataset meta-data \nX, y, _, _ = dataset.get_data(dataset_format=\"dataframe\",  # Downloads the data and returns a Pandas dataframe\n                target=dataset.default_target_attribute)\n\nfrom sklearn.ensemble import GradientBoostingClassifier         # Using a sklearn model as an example\nmodel = GradientBoostingClassifier(n_estimators=10).fit(X, y)   # Set hyperparameters and train the model \n</code></pre> <p>To guarantee interoperability, we focus on a limited set of data formats.</p>"},{"location":"contributing/backend/Datasets/#tabular-data","title":"Tabular data","text":"<p>OpenML has historically focussed on tabular data, and has extensive support for all kinds of tabular data. As explained above, we store all data in the Parquet format. You can upload data from many different data structures, such as Pandas dataframes and R dataframes, after which they will be converted and stored in Parquet. You can also upload datasets as CSV files or ARFF files, and we aim to allow direct Parquet uploads soon.</p> <p>ARFF legacy</p> <p>At the moment, some aspects of OpenML still has a dependency on the ARFF format. This will be fully phased out in favor of Parquet.</p>"},{"location":"contributing/backend/Datasets/#image-data","title":"Image data","text":"<p>OpenML generally supports other data types by requiring a 'header table', a table (stored in Parquet) listing all data instances with additional meta-data (e.g. classes, bounding boxes,...) and references to data files, such as images (e.g. JPGs), stored in seperate folders. See our blog post for details. We will provide more detailed guidelines here as soon as possible.</p>"},{"location":"contributing/backend/Datasets/#data-repositories","title":"Data repositories","text":"<p>This is a list of public dataset repositories that offer additional useful machine learning datasets. These have widely varying data formats, so they require manual selection, parsing and meta-data extraction.</p> <p>A collection of sources made by different users</p> <ul> <li>https://github.com/caesar0301/awesome-public-datasets</li> <li>https://dreamtolearn.com/ryan/1001_datasets</li> <li>https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research</li> <li>https://pathmind.com/wiki/open-datasets</li> <li>https://paperswithcode.com/</li> <li>https://medium.com/towards-artificial-intelligence/best-datasets-for-machine-learning-data-science-computer-vision-nlp-ai-c9541058cf4f</li> <li>https://lionbridge.ai/datasets/the-50-best-free-datasets-for-machine-learning/</li> <li>https://www.v7labs.com/open-datasets?utm_source=v7&amp;utm_medium=email&amp;utm_campaign=edu_outreach</li> </ul> <p>Machine learning dataset repositories (mostly already in OpenML)</p> <ul> <li>UCI: https://archive.ics.uci.edu/ml/index.html</li> <li>KEEL: http://sci2s.ugr.es/keel/datasets.php</li> <li>LIBSVM: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/</li> <li>AutoWEKA datasets: http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/</li> <li>skData package: https://github.com/jaberg/skdata/tree/master/skdata</li> <li>Rdatasets: http://vincentarelbundock.github.io/Rdatasets/datasets.html</li> <li>DataBrewer: https://github.com/rmax/databrewer</li> <li>liac-arff: https://github.com/renatopp/arff-datasets</li> </ul> <p>MS Open datasets:</p> <ul> <li>https://azure.microsoft.com/en-us/services/open-datasets/catalog/</li> </ul> <p>APIs (mostly defunct):</p> <ul> <li>databrewer (Python): https://pypi.org/project/databrewer/</li> <li>PyDataset (Python): https://github.com/iamaziz/PyDataset (wrapper for Rdatasets?)</li> <li>RDatasets (R): https://github.com/vincentarelbundock/Rdatasets</li> </ul> <p>Time series / Geo data:</p> <ul> <li>Data commons: https://datacommons.org/</li> <li>UCR: http://timeseriesclassification.com/</li> <li>Older version: http://www.cs.ucr.edu/~eamonn/time_series_data/</li> </ul> <p>Deep learning datasets (mostly image data)</p> <ul> <li>https://www.tensorflow.org/datasets/catalog/overview</li> <li>http://deeplearning.net/datasets/</li> <li>https://deeplearning4j.org/opendata</li> <li>http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html</li> <li>https://paperswithcode.com/datasets</li> </ul> <p>Extreme classification:</p> <ul> <li>http://manikvarma.org/downloads/XC/XMLRepository.html</li> </ul> <p>MLData (down)</p> <ul> <li>http://mldata.org/</li> </ul> <p>AutoWEKA datasets:</p> <ul> <li>http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/</li> </ul> <p>Kaggle public datasets</p> <ul> <li>https://www.kaggle.com/datasets</li> </ul> <p>RAMP Challenge datasets</p> <ul> <li>http://www.ramp.studio/data_domains</li> </ul> <p>Wolfram data repository</p> <ul> <li>http://datarepository.wolframcloud.com/</li> </ul> <p>Data.world</p> <ul> <li>https://data.world/</li> </ul> <p>Figshare (needs digging, lots of Excel files)</p> <ul> <li>https://figshare.com/search?q=dataset&amp;quick=1</li> </ul> <p>KDNuggets list of data sets (meta-list, lots of stuff here):</p> <ul> <li>http://www.kdnuggets.com/datasets/index.html</li> </ul> <p>Benchmark Data Sets for Highly Imbalanced Binary Classification</p> <ul> <li>http://www.cs.gsu.edu/~zding/research/imbalance-data/x19data.txt</li> </ul> <p>Feature Selection Challenge Datasets</p> <ul> <li>http://www.nipsfsc.ecs.soton.ac.uk/datasets/</li> <li>http://featureselection.asu.edu/datasets.php</li> </ul> <p>BigML's list of 1000+ data sources</p> <ul> <li>http://blog.bigml.com/2013/02/28/data-data-data-thousands-of-public-data-sources/</li> </ul> <p>Massive list from Data Science Central.</p> <ul> <li>http://www.datasciencecentral.com/profiles/blogs/data-sources-for-cool-data-science-projects</li> </ul> <p>R packages (also see https://github.com/openml/openml-r/issues/185)</p> <ul> <li>http://stat.ethz.ch/R-manual/R-patched/library/datasets/html/00Index.html</li> <li>mlbench</li> <li>Stata datasets: http://www.stata-press.com/data/r13/r.html</li> </ul> <p>UTwente Activity recognition datasets:</p> <ul> <li>http://ps.ewi.utwente.nl/Datasets.php</li> </ul> <p>Vanderbilt:</p> <ul> <li>http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets</li> </ul> <p>Quandl</p> <ul> <li>https://www.quandl.com</li> </ul> <p>Microarray data:</p> <ul> <li>http://genomics-pubs.princeton.edu/oncology/</li> <li>http://svitsrv25.epfl.ch/R-doc/library/multtest/html/golub.html</li> </ul> <p>Medical data:</p> <ul> <li>http://www.healthdata.gov/</li> <li>http://homepages.inf.ed.ac.uk/rbf/IAPR/researchers/PPRPAGES/pprdat.htm</li> <li>http://hcup-us.ahrq.gov/</li> <li>https://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-Provider-Charge-Data/Physician-and-Other-Supplier.html</li> <li>https://nsduhweb.rti.org/respweb/homepage.cfm</li> <li>http://orwh.od.nih.gov/resources/policyreports/womenofcolor.asp</li> </ul> <p>Nature.com Scientific data repositories list</p> <ul> <li>https://www.nature.com/sdata/policies/repositories</li> </ul>"},{"location":"contributing/backend/Java-App/","title":"Evaluation Engine (phasing out)","text":"<p>When you submit datasets or experiments (runs) to OpenML, they will be processed by set of server-side processes, combined in the 'Evaluation Engine':</p> <ul> <li>It extracts the features in tabular datasets and their statistical types</li> <li>It computes a set of dataset characteristics (meta-features), e.g. the number of features and classes, that help with search and filtering, or to compute dataset similarity measures</li> <li>It evaluates experiments using a set of server-side evaluation metrics that are computed uniformly for all experiments so that they are comparable</li> <li>It creates consistent train-test splits based on task characteristics.</li> </ul> <p>Phasing out</p> <p>This documentation is about the older Java-based version of the OpenML evaluation engine, which will be phased out. These parts are being rewritten as a set of independent services in Python.</p> <p>The application that implements the evaluation engine was originally implemented in Java because it bulds on the Weka API. It is invoked from the OpenML API by means of a CLI interface. Typically, a call looks like this:</p> <p><code>java -jar webapplication.jar -config \"api_key=S3CR3T_AP1_K3Y\" -f evaluate_run -r 500</code></p> <p>Which in this case executes the webapplication jar, invokes the function \"evaluate run\" and gives it parameter run id 500. The config parameter can be used to set some config items, in this case the api_key is mandatory. Every OpenML user has an api_key, which can be downloaded from their OpenML profile page. The response of this function is a call to the OpenML API uploading evaluation results to the OpenML database. Note that in this case the PHP website invokes the Java webapplication, which makes a call to the PHP website again, albeit another endpoint. </p> <p>The webapplication does not have direct writing rights into the database. All communication to the database goes by means of the OpenML Connector, which communicates with the OpenML API. As a consequence, the webapplication could run on any system, i.e., there is no formal need for the webapplication to be on the same server as the website code. This is important, since this created modularity, and not all servers provide a command line interface to PHP scripts.</p> <p>Another example is the following:</p> <p><code>java -jar webapplication -config \"api_key=S3CR3T_AP1_K3Y\" -f all_wrong -r 81,161 -t 59</code></p> <p>Which takes a comma separated list of run ids (no spaces) and a task id as input and outputs the test examples on the dataset on which all algorithms used in the runs produced wrong examples (in this case, weka.BayesNet_K2 and weka.SMO, respectively). An error will be displayed if there are runs not consistent with the task id in there. </p>"},{"location":"contributing/backend/Java-App/#extending-the-java-app","title":"Extending the Java App","text":"<p>The bootstrap class of the webapplication is</p> <p><code>org.openml.webapplication.Main</code></p> <p>It automatically checks authentication settings (such as api_key) and the determines which function to invoke. </p> <p>It uses a switch-like if - else contruction to facilitate the functionalities of the various functions. Additional functions can be added to this freely. From there on, it is easy to add functionality to the webapplication. </p> <p>Parameters are handled using the Apache Commons CommandLineParser class, which makes sure that the passed parameters are available to the program. </p> <p>In order to make new functionalities available to the website, there also needs to be programmed an interface to the function, somewhere in the website. The next section details on that. </p>"},{"location":"contributing/backend/Java-App/#interfacing-from-the-openml-api","title":"Interfacing from the OpenML API","text":"<p>By design, the REST API is not allowed to communicate with the Java App. All interfaces with the Java webapplication should go through other controllers of the PHP CodeIgniter framework., for example api_splits. Currently, the website features two main API's. These are represented by a Controller. Controllers can be found in the folder openml_OS/controllers. Here we see: * api_new.php, representing the REST API * api_splits.php, representing an API interfacing to the Java webapplication. </p>"},{"location":"contributing/backend/Java-App/#helper-functions","title":"Helper functions","text":"<p>The Java code is available in the 'OpenML' repository: https://github.com/openml/OpenML/tree/master/Java</p>"},{"location":"contributing/backend/Java-App/#components","title":"Components","text":"<p>Support for tasks:  </p> <ul> <li>foldgeneration: Java code for generating cross-validation folds. Can be used from command line.</li> <li>splitgeneration: Split generator for cross validation and holdout. Unsure what's the difference with the previous?</li> <li>generate_predictions: Helper class to build prediction files based on WEKA output. Move to WEKA repository?</li> <li>evaluate_predictions: The evaluation engine computing evaluation scores based on submitted predictions</li> </ul>"},{"location":"contributing/backend/Local-Installation/","title":"Local Installation","text":"<p>Test server</p> <p>OpenML has a fully functional test server accessible at <code>test.openml.org</code> that you can use to develop against. For many cases, this is sufficient for development, and a full local installation is not required.</p> <p>Backend evolution</p> <p>OpenML has grown organically, since before the current ecosystem of python tools for platform building. We are currently rewriting the entire backend using state-of-the-art Python tools (e.g. FastAPI) so that the entire platform can be easily installed locally in one go. We plan this to be available early/mid 2025. Please get in touch  if you want to know more or want to contribute.</p>"},{"location":"contributing/backend/Local-Installation/#using-docker-compose","title":"Using Docker Compose","text":"<p>The easiest way to set up a local version of OpenML is to use Docker Compose following the instructions here (thanks to Jos van der Velde!): https://github.com/openml/openml-services.</p> <p>If you run into problems, please post an issue in the same github repo.</p> <p></p>"},{"location":"contributing/backend/Local-Installation/#installation-from-scratch","title":"Installation from scratch","text":"<p>If you want to install a local version of OpenML from scratch please follow the steps mentioned below. Note that this does not include the Kubernetes and S3 Object storage components that we use in production.</p>"},{"location":"contributing/backend/Local-Installation/#requirements","title":"Requirements","text":"<p>You'll need to have the following software running: * Apache Webserver, (with the rewrite module enabled. Is installed by default, not enabled.) * MySQL Server. * PHP 5.5 or higher (comes also with Apache) Or just a XAMP (Mac), LAMP (Linux) or WAMP (Windows) package, which conveniently contains all these applications.</p>"},{"location":"contributing/backend/Local-Installation/#databases","title":"Databases","text":"<p>Next, OpenML runs on two databases, a public database with all experiment information, and a private database, with information like user accounts etc. The latest version of both databases can be downloaded here: https://docs.openml.org/resources</p> <p>Obviously, the private database does not include any actual user account info.</p>"},{"location":"contributing/backend/Local-Installation/#backend","title":"Backend","text":"<p>The source code is available in the 'OpenML' repository: https://github.com/openml/OpenML</p> <p>OpenML is written in PHP, and can be 'installed' by copying all files in the 'www' or 'public_html' directory of Apache.</p> <p>After that, you need to provide your local paths and database accounts and passwords using the config file in: 'APACHE_WWW_DIR'/openml_OS/config/BASE_CONFIG.php.</p> <p>If everything is configured correctly, OpenML should now be running.</p>"},{"location":"contributing/backend/Local-Installation/#search-indices","title":"Search Indices","text":"<p>If you want to run your own (separate) OpenML instance, and store your own data, you'll also want to build your own search indices to show all data on the website. The OpenML website is based on the ElasticSearch stack. To install it, follow the instructions here: http://knowm.org/how-to-set-up-the-elk-stack-elasticsearch-logstash-and-kibana/</p>"},{"location":"contributing/backend/Local-Installation/#initialization","title":"Initialization","text":"<p>This script wipes all OpenML server data and rebuilds the database and search index. Replace 'openmldir' with the directory where you want OpenML to store files.</p> <pre><code># delete data from server\nsudo rm -rf /openmldir/*\nmkdir /openmldir/log\n\n# delete database\nmysqladmin -u \"root\" -p\"yourpassword\" DROP openml_expdb\nmysql -h localhost -u root -p\"yourpassword\" -e \"TRUNCATE openml.file;\"\n\n# reset ES search index\necho \"Deleting and recreating the ES index: \"\ncurl -XDELETE http://localhost:9200/openml\ncurl -XPUT 'localhost:9200/openml?pretty' -H 'Content-Type: application/json' -d'\n{\n    \"settings\" : {\n        \"index\" : {\n            \"number_of_shards\" : 3,\n            \"number_of_replicas\" : 2\n        }\n    }\n}\n'\n\n# go to directory with the website source code\ncd /var/www/openml.org/public_html/\n\n# reinitiate the database\nmysql -u root -p\"yourpassword!\" &lt; downloads/openml_expdb.sql\n\n# fill important columns\nsudo php index.php cron install_database\n\n# rebuild search index\nsudo php index.php cron initialize_es_indices\nsudo php index.php cron build_es_indices\n\nsudo chown apache:apache /openmldir/log\nsudo chown apache:apache /openmldir/log/*\n</code></pre>"},{"location":"contributing/clients/Client-API-Standards/","title":"Client development","text":""},{"location":"contributing/clients/Client-API-Standards/#building-clients","title":"Building clients","text":"<p>You can access OpenML datasets, pipelines, benchmarks, and much more, through a range of client APIs. Well-developed clients exist in Python, R, Java, and several other languages. Please see their documentation (in the other tabs) for more guidance of how to contribute to them.</p> <p>If you want to develop your own client (e.g. for a new language), please check out the following resources:  </p> <ul> <li>REST API: all endpoints to GET, POST, or DELETE resources</li> <li>Metadata Standard: how we describe datasets and all other OpenML resources</li> <li>Minimal standards (below) for uniform client configuration and caching mechanisms, to make the client behavior more uniform across languages.</li> </ul> <p>Integrating tools</p> <p>If you want to integrate OpenML into machine learning and data science tools, it's often easier to build on one of the existing clients,  which often can be used as is or extended. For instance, see how to extend the Python API to integrate OpenML into Python tools. </p>"},{"location":"contributing/clients/Client-API-Standards/#minimal-standards","title":"Minimal standards","text":""},{"location":"contributing/clients/Client-API-Standards/#configuration-file","title":"Configuration file","text":"<p>The configuration file resides in a directory <code>.openml</code> in the home directory of the user and is called config. It consists of <code>key = value</code> pairs which are seperated by newlines. The following keys are defined:</p> <ul> <li>apikey:<ul> <li>required to access the server</li> </ul> </li> <li>server:<ul> <li>default: <code>http://www.openml.org</code></li> </ul> </li> <li>verbosity:<ul> <li>0: normal output</li> <li>1: info output</li> <li>2: debug output</li> </ul> </li> <li>cachedir:<ul> <li>if not given, will default to <code>file.path(tempdir(), \"cache\")</code>.</li> </ul> </li> </ul>"},{"location":"contributing/clients/Client-API-Standards/#caching","title":"Caching","text":""},{"location":"contributing/clients/Client-API-Standards/#cache-invalidation","title":"Cache invalidation","text":"<p>All parts of the entities which affect experiments are immutable. The entities dataset and task have a flag <code>status</code> which tells the user whether they can be used safely.</p>"},{"location":"contributing/clients/Client-API-Standards/#file-structure","title":"File structure","text":"<p>Caching should be implemented for</p> <ul> <li>datasets</li> <li>tasks</li> <li>splits</li> <li>predictions</li> </ul> <p>and further entities might follow in the future. The cache directory <code>$cache</code> should be specified by the user when invoking the API. The structure in the cache directory should be as following:</p> <ul> <li>One directory for the following entities:<ul> <li><code>$cache/datasets</code></li> <li><code>$cache/tasks</code></li> <li><code>$cache/runs</code></li> </ul> </li> <li>For every dataset there is an extra directory for which the name is the dataset ID, e.g. <code>$cache/datasets/2</code> for the dataset with OpenML ID 2.<ul> <li>The dataset should be called <code>dataset.pq</code> or <code>dataset.arff</code></li> <li>Every other file should be named by the API call which was used to obtain it. The XML returned by invoking <code>openml.data.qualities</code> should therefore be called qualities.xml.</li> </ul> </li> <li>For every task there is an extra directory for which the name is the task ID, e.g. <code>$cache/tasks/1</code><ul> <li>The task file should be called <code>task.xml</code>.</li> <li>The splits accompanying a task are stored in a file <code>datasplits.arff</code>.</li> </ul> </li> <li>For every run there is an extra directory for which the name is the run ID, e.g. <code>$cache/run/1</code><ul> <li>The predictions should be called <code>predictions.arff</code>.</li> </ul> </li> </ul>"},{"location":"contributing/clients/Rest/","title":"REST API","text":"<p>OpenML offers a RESTful Web API, with predictive URLs, for uploading and downloading machine learning resources. Try the REST API Documentation to see examples of all calls, and test them right in your browser.</p>"},{"location":"contributing/clients/Rest/#getting-started","title":"Getting started","text":"<ul> <li>REST services can be called using simple HTTP GET or POST actions.</li> <li>The REST Endpoint URL is <code>https://www.openml.org/api/v1/</code></li> <li>The default endpoint returns data in XML. If you prefer JSON, use the endpoint <code>https://www.openml.org/api/v1/json/</code>. Note that, to upload content, you still need to use XML (at least for now).</li> </ul>"},{"location":"contributing/clients/Rest/#testing","title":"Testing","text":"<p>For continuous integration and testing purposes, we have a test server offering the same API, but which does not affect the production server.</p> <ul> <li>The test server REST Endpoint URL is <code>https://test.openml.org/api/v1/</code></li> </ul>"},{"location":"contributing/clients/Rest/#error-messages","title":"Error messages","text":"<p>Error messages will look like this:</p> <pre><code>&lt;oml:error xmlns:oml=\"http://openml.org/error\"&gt;\n&lt;oml:code&gt;100&lt;/oml:code&gt;\n&lt;oml:message&gt;Please invoke legal function&lt;/oml:message&gt;\n&lt;oml:additional_information&gt;Additional information, not always available.&lt;/oml:additional_information&gt;\n&lt;/oml:error&gt;\n</code></pre> <p>All error messages are listed in the API documentation. E.g. try to get a non-existing dataset:</p> <ul> <li>in XML: https://www.openml.org/api_new/v1/data/99999</li> <li>in JSON: https://www.openml.org/api_new/v1/json/data/99999</li> </ul>"},{"location":"contributing/clients/Rest/#examples","title":"Examples","text":"<p>You need to be logged in for these examples to work.</p>"},{"location":"contributing/clients/Rest/#download-a-dataset","title":"Download a dataset","text":"<ul> <li>User asks for a dataset using the /data/{id} service. The <code>dataset id</code> is typically part of a task, or can be found on OpenML.org.</li> <li>OpenML returns a description of the dataset as an XML file (or JSON). Try it now</li> <li>The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.</li> <li>The dataset is returned by the server hosting the dataset. This can be OpenML, but also any other data repository. Try it now</li> </ul>"},{"location":"contributing/clients/Rest/#download-a-flow","title":"Download a flow","text":"<ul> <li>User asks for a flow using the /flow/{id} service and a <code>flow id</code>. The <code>flow id</code> can be found on OpenML.org.</li> <li>OpenML returns a description of the flow as an XML file (or JSON). Try it now</li> <li>The flow description contains the URL where the flow can be downloaded (e.g. GitHub), either as source, binary or both, as well as additional information on history, dependencies and licence. The user calls the right URL to download it.</li> <li>The flow is returned by the server hosting it. This can be OpenML, but also any other code repository. Try it now</li> </ul>"},{"location":"contributing/clients/Rest/#download-a-task","title":"Download a task","text":"<ul> <li>User asks for a task using the /task/{id} service and a <code>task id</code>. The <code>task id</code> is typically returned when searching for tasks.</li> <li>OpenML returns a description of the task as an XML file (or JSON). Try it now</li> <li>The task description contains the <code>dataset id</code>(s) of the datasets involved in this task. The user asks for the dataset using the /data/{id} service and the <code>dataset id</code>.</li> <li>OpenML returns a description of the dataset as an XML file (or JSON). Try it now</li> <li>The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.</li> <li>The dataset is returned by the server hosting it. This can be OpenML, but also any other data repository. Try it now</li> <li>The task description may also contain links to other resources, such as the train-test splits to be used in cross-validation. The user calls that URL to download the train-test splits.</li> <li>The train-test splits are returned by OpenML. Try it now</li> </ul>"},{"location":"contributing/clients/metadata_definition/","title":"Metadata definition","text":"<p>OpenML is at its core a meta-database, from which datasets, pipelines (flows), experiments (runs) and other entities can be downloaded and uploaded, all described using a clearly defined meta-data standard. In this document, we describe the standard how to upload entities to OpenML and what the resulting database state will be.</p> <p> Croissant</p> <p>OpenML has partnered with MLCommons, Google, Kaggle, HuggingFace, and a consortium of other partners to define a new metadata standard for machine learning datasets:  Croissant! You can already download all OpenML datasets in the Croissant format, and we're working further supporting and extending Croissant.</p> <p>Below is the OpenML metadata standard for version 1 of the API.</p>"},{"location":"contributing/clients/metadata_definition/#data","title":"Data","text":"<p>Data is uploaded through the function post data. The following files are needed:</p> <ul> <li><code>description</code>: An XML adhiring to the XSD schema.</li> <li><code>dataset</code>: An ARFF file containing the data (optional, if not set, there should be an URL in the description, pointing to this file).   Uploading any other files will result in an error.</li> </ul>"},{"location":"contributing/clients/metadata_definition/#tasks","title":"Tasks","text":"<p>Tasks are uploaded through the function post task. The following files are needed:</p> <ul> <li><code>description</code>: An XML adhering to the XSD schema.   Uploading any other files will result in an error.</li> </ul> <p>The task file should contain several input fields. These are a name and value combination of fields that are marked to be relevant by the task type definition. There are several task type definitions, e.g.:</p> <ul> <li>Supervised Classification</li> <li>Supervised Regression</li> <li>Learning Curve</li> <li>Data Stream Classification</li> </ul> <p>Note that the task types themselves are flexible content (ideally users can contribute task types) and therefore the documents are not part of the OpenML definition. The task types define which input fields should be set, when creating a task.</p> <p>Duplicate tasks (i.e., same value for <code>task_type_id</code> and all <code>input</code> fields equal) will be rejected.</p> <p>When creating a task, the API checks for all of the input fields whether the input is legitimate. (Todo: describe the checks and what they depend on).</p>"},{"location":"contributing/clients/metadata_definition/#flow","title":"Flow","text":"<p>Flows are uploaded through the function post flow. The following file is needed:</p> <ul> <li><code>description</code>: An XML adhering to the XSD schema.   Uploading any other files will result in an error.</li> </ul> <p>Duplicate flows (i.e., same values for <code>name</code> and <code>external_version</code>) will be rejected.</p>"},{"location":"contributing/clients/metadata_definition/#runs","title":"Runs","text":"<p>Runs are uploaded through the function post run. The following files are needed:</p> <ul> <li><code>description</code>: An XML adhering to the XSD schema.</li> <li><code>predictions</code>: An ARFF file containing the predictions (optional, depending on the task).</li> <li><code>trace</code>: An ARFF file containing the run trace (optional, depending on the flow).   Uploading any other files will result in an error.</li> </ul>"},{"location":"contributing/clients/metadata_definition/#predictions","title":"Predictions","text":"<p>The contents of the prediction file depends on the task type.</p>"},{"location":"contributing/clients/metadata_definition/#task-type-supervised-classification","title":"Task type: Supervised classification","text":"<p>Example predictions file</p> <ul> <li>repeat NUMERIC</li> <li>fold NUMERIC</li> <li>row_id NUMERIC</li> <li>confidence.{$classname}: optional. various columns, describing the confidence per class. The values of these columns should add to 1 (precision 1e-6).</li> <li>(proposal) decision_function.{$classname}: optional. various columns, describing decision function per class.</li> <li>prediction {$classname}   Runs that have a different set of columns will be rejected.</li> </ul>"},{"location":"contributing/clients/metadata_definition/#trace","title":"Trace","text":"<p>Example trace file</p> <ul> <li>repeat: cross-validation repeat</li> <li>fold: cross-validation fold</li> <li>iteration: the index order within this repeat/fold combination</li> <li>evaluation (float): the evaluation score that was attached based on the validation set</li> <li>selected {True, False}: Whether in this repeat/run combination this was the selected hyperparameter configuration (exactly one should be tagged with True)</li> <li>Per optimized parameter a column that has the name of the parameter and the prefix \"parameter_\"</li> <li>setup_string: Due to legacy reasons accepted, but will be ignored by the default evaluation engine</li> </ul> <p>Traces that have a different set of columns will be rejected.</p>"},{"location":"contributing/website/Dash/","title":"Dash visualization","text":"<p>Dash is a python framework which is suitable for building data visualization dashboards using pure python. Dash is written on top of plotly, react and flask and the graphs are defined using plotly python. The dash application is composed of two major parts :</p> <ul> <li><code>Layout</code> - Describes how the dashboard looks like</li> <li><code>Callbacks</code> - Used to update graphs, tables in the layout and makes the dashboard interactive.</li> </ul>"},{"location":"contributing/website/Dash/#files","title":"Files","text":"<p>The dash application is organized as follows:</p> <ul> <li> <p><code>dashapp.py</code></p> </li> <li> <p>Creates the dash application</p> </li> <li>The dash app is embedded in the flask app passed to <code>create_dash_app</code> function</li> <li> <p>This file need not be modified to create a new plot</p> </li> <li> <p><code>layouts.py</code></p> </li> <li> <p>contains the layout for all the pages</p> </li> <li><code>get_layout_from_data</code>- returns layout of data visualization</li> <li><code>get_layout_from_task</code>- returns layout of taskvisualization</li> <li><code>get_layout_from_flow</code>- returns layout of flow visualization</li> <li><code>get_layout_from_run</code> - returns layout of run visualization</li> <li> <p>This file needs to be modified to add a new plot (data, task, flow, run)</p> </li> <li> <p><code>callbacks.py</code></p> </li> <li>Registers all the callbacks for the dash application</li> <li>This file needs to be modified to add a new plot, especially if the plot needs to be interactive</li> </ul>"},{"location":"contributing/website/Dash/#how-the-dashboard-works","title":"How the dashboard works","text":"<p>In this dash application, we need to create the layout of the page dynamically based on the entered URL. For example, [http://127.0.0.1:5000/dashboard/data/5] needs to return the layout for dataset id #5 whereas [http://127.0.0.1:5000/dashboard/run/5] needs to return the layout for run id #5.</p> <p>Hence , the dash app is initially created with a dummy <code>app.layout</code> by dashapp.py and the callbacks are registered for the app using <code>register_callbacks</code> function.</p> <ul> <li> <p>render_layout is the callback which dynamically renders layout. Once the dash app is running, the first callback which is fired is <code>render_layout.</code>   This is the main callback invoked when a URL with a data , task, run or flow ID is entered.   Based on the information in the URL, this method returns the layout.</p> </li> <li> <p>Based on the URL, get_layout_from_data, get_layout_from_task, get_layout_from_flow, get_layout_from_run are called.   These functions define the layout of the page - tables, html Divs, tabs, graphs etc.</p> </li> <li> <p>The callbacks corresponding to each component in the layout are invoked to update the components dynamically and   make the graphs interactive. For example, update_scatter_plot in <code>data_callbacks.py</code> updates the scatter plot   component in the data visualization dashboard.</p> </li> </ul>"},{"location":"contributing/website/Flask/","title":"Flask backend","text":"<p>We use Flask as our web framework. It handles user authentication, dataset upload, task creation, and other aspects that require server-side interaction. It is designed to be independent from the OpenML API. This means that you can use it to create your own personal frontend for OpenML, using the main OpenML server to provide the data. Of course, you can also link it to your own local OpenML setup.</p>"},{"location":"contributing/website/Flask/#design","title":"Design","text":"<p>Out flask app follows Application factories design pattern. A new app instance can be created by: <pre><code>    from autoapp import create_app\n    app = create_app(config_object)\n</code></pre></p> <p>The backend is designed in a modular fashion with flask Blueprints. Currently, the flask app consists of two blueprints public and user:</p> <ul><li>Public blueprint: contains routes that do not require user authentication or authorization. like signup and forgot password.</li> <li>User blueprint: Contains routes which require user authentication like login, changes in profile and fetching API key.</li></ul> <p>New blueprints can be registered in `server/app.py` with register_blueprints function:</p> <pre><code>    def register_blueprints(app):\n        app.register_blueprint(new_blueprint)\n</code></pre>"},{"location":"contributing/website/Flask/#database-setup","title":"Database setup","text":"<p>If you want o setup a local user database similar to OpenML then follow these steps:</p> <ol> <li>Install MySQL</li> <li>Create a new database 'openml'</li> <li>Set current database to 'openml' via use method</li> <li>Download users.sql file from openml.org github repo and add it in the openml db via \"mysql -u root -p openml &lt; users.sql\"</li> <li>Edit the database path in `server/extensions.py` and `server/config.py`</li> </ol> <p>Note: Remember to add passwords and socket extension address(if any) in both in <code>server/extensions.py</code> and <code>server/config.py</code> </p>"},{"location":"contributing/website/Flask/#security","title":"Security","text":"<p>Flask backend uses JSON web tokens for all the user handling tasks. Flask JWT extended library is used to bind JWT with the flask app. Current Mechanism is :</p> <ol> <li> User logs in.</li> <li> JWT token is assigned to user and sent with every request to frontend.</li> <li> All the user information can only be accessed with a JWT token like edit profile and API-key.</li> <li> The JWT token is stored in local memory of the browser.</li> <li> The token get expired after 2 hours or get blacklisted after logout.</li> </ol> <p>JWT is registered as an extension in `server/extensions.py`. All the user password hash are saved in Argon2 format with the new backend.</p>"},{"location":"contributing/website/Flask/#registering-extensions","title":"Registering Extensions","text":"<p>To register a new extension to flask backend extension has to be added in <code>server/extensions.py</code> and initialized in server/app.py. Current extensions are : flask_argon2, flask_bcrypt, flask_jwt_extended and flask_sqlalchemy.</p>"},{"location":"contributing/website/Flask/#configuring-app","title":"Configuring App","text":"<p>Configuration variables like secret keys, Database URI and extension configurations are specified in  <code>server/config.py</code> with Config object, which is supplied to the flask app during initialization.</p>"},{"location":"contributing/website/Flask/#creating-a-new-route","title":"Creating a new route","text":"<p>To create a new route in backend you can add the route in <code>server/public/views.py</code> or <code>server/user/views.py</code> (if it requires user authorisation or JWT usage in any way).  </p>"},{"location":"contributing/website/Flask/#bindings-to-openml-server","title":"Bindings to OpenML server","text":"<p>You can specify which OpenML server to connect to. This is stored in the <code>.env</code> file in the main directory. It is set to the main OpenML server by default:</p> <pre><code>    ELASTICSEARCH_SERVER=https://www.openml.org/es\n    OPENML_SERVER=https://www.openml.org\n</code></pre> <p>The ElasticSearch server is used to download information about datasets, tasks, flows and runs, as well as to power the frontend search. The OpenML server is used for uploading datasets, tasks, and anything else that requires calls to the OpenML API.</p>"},{"location":"contributing/website/Flask/#bindings-to-frontend","title":"Bindings to frontend","text":"<p>The frontend is generated by React. See below for more information. The React app is loaded as a static website. This is done in Flask setup in file <code>server.py</code>.</p> <pre><code>    app = Flask(__name__, static_url_path='', static_folder='src/client/app/build')\n</code></pre> <p>It will find the React app there and load it.</p>"},{"location":"contributing/website/Flask/#email-server","title":"Email Server","text":"<p>OpenML uses its own mail server, You can use basically any mail server compatible with python SMTP library. Our suggestion is to use mailtrap.io for local testing. You can configure email server configurations in .env file. Currently we only use emails for confirmation email and forgotten password emails.</p>"},{"location":"contributing/website/React/","title":"React App","text":""},{"location":"contributing/website/React/#app-structure","title":"App structure","text":"<p>The structure of the source code looks as follows</p> <pre><code>App.js\nindex.js\ncomponents\n|-- Sidebar.js\n|-- Header.js\n|-- ...\nlayouts\n|-- Clear.js\n|-- Main.js\npages\n|-- auth\n|-- cover\n|-- docs\n|-- search\nroutes\n|-- index.js\n|-- Routes.js\nthemes\n</code></pre> <p>The website is designed as a single-page application. The top level files bootstrap the app. <code>index.js</code> simply renders the top component, and <code>App.js</code> adds the relevant subcomponents based on the current theme and state.</p> <p><code>Routes.js</code> links components to the possible routes (based on the URL). The list of possible routes is defined in <code>routes/index.js</code>.</p> <p><code>pages</code> contain the various pages of the website. It has subdirectories for:</p> <ul> <li><code>auth</code>: All pages that require authorization (login). These routes are protected.</li> <li><code>cover</code>: The front page of the website</li> <li><code>docs</code>: All normal information pages (e.g. 'About', 'API',...)</li> <li><code>search</code>: All pages related to searching for datasets, tasks, flows, runs, etc.</li> </ul> <p><code>layout</code> contains the possible layouts, <code>Main</code> or <code>Clear</code> (see below). You define the layout of a page by adding its route to either <code>mainRoutes</code> or <code>clearRoutes</code> in <code>routes/index.js</code>. The default is the <code>Main</code> layout.</p> <p><code>themes</code> contains the overall theme styling for the entire website. Currently, there is a dark and a light theme. They can be set using <code>setTheme</code> in the MainContext, see <code>App.js</code>.</p>"},{"location":"contributing/website/React/#component-structure","title":"Component structure","text":"<p>The component structure is shown above, for the <code>Main</code> layout. The <code>App</code> component also holds the state of the website using React's native Context API (see below). Next to the header and sidebar, the main component of the website (in yellow) shows the contents of the current <code>page</code>. In this image, this is the search page, which has several subcomponents as explained below.</p>"},{"location":"contributing/website/React/#search-page","title":"Search page","text":"<p>The search page is structured as follows:</p> <ul> <li> <p><code>SearchPanel</code>: the main search panel. Also contains callbacks for sorting and filtering, and lists what can be filtered or sorted on.</p> </li> <li> <p><code>FilterBar</code>: The top bar with the search statistics and functionality to add filters and sort results</p> </li> <li> <p><code>SearchResultsPanel</code>: The list of search results on the left. It shows a list of <code>Card</code> elements which are uniformly styled but their contents may vary. Depending on the selected type of result (selected in the left navigation bar) it is instantiated with different properties. E.g. a <code>DataListPanel</code> is a simple wrapper around <code>SearchResultsPanel</code> which defines the dataset-specific statistics to be shown in the cards.</p> <ul> <li>Search tabs: The tabs that allow you to choose between different aspects of the results (Statistics, Overview (Dash)) or the different views on the selected dataset, task, etc. (Details, Analysis (Dash),...)</li> <li><code>ItemDetail</code>: When a search result is selected, this will show the details of the selection, e.g. the dataset details. Depending on the passed <code>type</code> prop, it will render the <code>Dataset</code>, <code>Task</code>, ... component.</li> </ul> </li> </ul> <p>The <code>api.js</code> file contains the <code>search</code> function, which translates a search query, filters, and other constraints into an ElasticSearch query and returns the results.</p>"},{"location":"contributing/website/React/#style-guide","title":"Style guide","text":"<p>To keep a consistent style and minimize dependencies and complexity, we build on Material UI components and FontAwesome icons. Theming is defined in <code>themes/index.js</code> and loaded in as a context (<code>ThemeContext</code>) in <code>App.js</code>. More specific styling is always defined through styled components in the corresponding pages.</p>"},{"location":"contributing/website/React/#layouts","title":"Layouts","text":"<p>There are two top level layouts: <code>Main</code> loads the main layout with a <code>Sidebar</code>, <code>Header</code>, and a certain page with all the contents. The <code>Clear.js</code> layout has no headers or sidebars, but has a colored gradient background. It is used mainly for user login and registration or other quick forms.</p> <p>The layout of the page content should use the Material UI grid layout. This makes sure it will adapt to different device screen sizes. Test using your browsers development tools whether the layout adapts correctly to different screens, including recent smartphones.</p>"},{"location":"contributing/website/React/#styled-components","title":"Styled components","text":"<p>Any custom styling (beyond the Material UI default styling) is defined in styled components which are defined within the file for each page. Keep this as minimal as possible. Check if you can import styled components already defined for other pages, avoid duplication.</p> <p>Styled div's are defined as follows:</p> <pre><code>const OpenMLTitle = styled.div`\n  color: white;\n  font-size: 3em;\n`;\n</code></pre> <p>Material UI components can be styled the same way:</p> <pre><code>const WhiteButton = styled(Button)`\n  display: inline-block;\n  color: #fff;\n`;\n</code></pre>"},{"location":"contributing/website/React/#color-palette","title":"Color palette","text":"<p>We follow the general Material UI color palette with shade 400, except when that doesn't give sufficient contrast. The main colors used (e.g. for the icons in the sidebar are: 'green[400]', 'yellow[700]', 'blue[800]', 'red[400]', 'purple[400]', 'orange[400]', 'grey[400]'. Backgrounds are generally kept white (or dark grey for the dark theme). The global context (see below) has a <code>getColor</code> function to get the colors of the search types, e.g. <code>context.getColor(\"run\")</code> returns <code>red[400]</code>.</p>"},{"location":"contributing/website/React/#handling-state","title":"Handling state","text":"<p>There are different levels of state management:</p> <ul> <li>Global state is handled via React's native Context API (we don't use Redux). Contexts are defined in the component tree where needed (usually higher up) by a context provider component, and is accessed lower in the component tree by a context consumer. For instance, see the <code>ThemeContext.Provider</code> in <code>App.js</code> and the <code>ThemeContext.Consumer</code> in <code>Sidebar.js</code>. There is a <code>MainContext</code> which contains global state values such as the logged in user details, and the current state of the search.</li> <li>Lower level components can pass state to their child components via props.</li> <li>Local state changes should, when possible, be defined by React Hooks.</li> </ul> <p>Note that changing the global state will re-render the entire website. Hence, do this only when necessary.</p>"},{"location":"contributing/website/React/#state-and-search","title":"State and search","text":"<p>Most global state variables have to do with search. The search pages typically work by changing the <code>query</code> and <code>filters</code> variables (see <code>App.js</code>). There is a <code>setSearch</code> function in the main context that can be called to change the search parameters. It checks whether the query has changed and whether updating the global state and re-rendering the website is necessary.</p>"},{"location":"contributing/website/React/#lifecycle-methods","title":"Lifecycle Methods","text":"<p>These are the React lifecycle methods and how we use them. When a component mounts, methods 1,2,4,7 will be called. When it updates, methods 2-6 will be called.</p> <ol> <li>constructor(): Set the initial state of the components</li> <li>getDerivedStateFromProps(props, state): Static method, only for changing the local state based on props. It returns the new state.</li> <li>shouldComponentUpdate(nextProps, nextState): Decides whether a state change requires a re-rendering or not. Used to optimize performance.</li> <li>render(): Returns the JSX to be rendered. It should NOT change the state.</li> <li>getSnapshotBeforeUpdate(prevProps,prevState): Used to save 'old' DOM information right before an update. Returns a 'snapshot'.</li> <li>componentDidUpdate(prevProps,prevState,snapshot): For async requests or other operations right after component update.</li> <li>componentDidMount(): For async requests (e.g. API calls) right after the component mounted.</li> <li>componentWillUnMount(): Cleanup before the component is destroyed.</li> <li>componentDidCatch(error,info): For updating the state after an error is thrown.</li> </ol>"},{"location":"contributing/website/React/#forms-and-events","title":"Forms and Events","text":"<p>React wraps native browser events into synthetic events to handle interactions in a cross-browser compatible way. After being wrapped, they are sent to all event handlers, usually defined as callbacks. Note: for performance reasons, synthetic events are pooled and reused, so their properties are nullified after being consumed. If you want to use them asynchronously, you need to call <code>event.persist()</code>.</p> <p>HTML forms are different than other DOM elements because they keep their own state in plain HTML. To make sure that we can control the state we need to set the input field's <code>value</code> to a component state value.</p> <p>Here's an example of using an input field to change the title displayed in the component.</p> <pre><code>const titles: {mainTitle: 'OpenML'};\n\nclass App extends Component {\n  this.state = {titles};\n\n  // Receive synthetic event\n  onTitleChange = (event) =&gt; {\n    this.setState({titles.mainTitle : event.target.value});\n  }\n\n  render(){\n    return (\n      &lt;div classname=\"App\"&gt;\n        &lt;h1&gt;{this.state.titles.mainTitle}&lt;/h1&gt;\n        &lt;form&gt;\n          &lt;input type=\"text\"\n          value={this.state.titles.mainTitle} // control state\n          onChange={this.onTitleChange} // event handler callback\n          /&gt;\n        &lt;/form&gt;\n      &lt;/div&gt;\n    );\n  }\n}\n</code></pre>"},{"location":"contributing/website/Website/","title":"Getting started","text":""},{"location":"contributing/website/Website/#installation","title":"Installation","text":"<p>The OpenML website runs on Flask, React, and Dash. You need to install these first.</p> <ul> <li> <p>Download or clone the source code for the OpenML website from GitHub. Then, go into that folder (it should have the <code>requirements.txt</code> and <code>package.json</code> files). <pre><code>git clone https://github.com/openml/openml.org.git\ncd openml.org\n</code></pre></p> </li> <li> <p>Install Flask, Dash, and dependencies using PIP <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Install React and dependencies using NPM (8 or higher) <pre><code>cd server/src/client/app/\nnpm install\n</code></pre></p> </li> </ul>"},{"location":"contributing/website/Website/#building-and-running","title":"Building and running","text":"<p>Go back to the home directory. Build a production version of the website with:</p> <pre><code>npm run build --prefix server/src/client/app/\n</code></pre> <p>Start the server by running:</p> <pre><code>flask run\n</code></pre> <p>You should now see the app running in your browser at <code>localhost:5000</code></p> <p>Note: If you run the app using HTTPS, add the SSL context or use 'adhoc' to use on-the-fly certificates or you can specify your own certificates.</p> <pre><code>flask run --cert='adhoc'\n</code></pre> <p>As flask server is not suitable for production we recommend you to use some other server if you want to deploy your openml installation in production. We currently use gunicorn for production server. You can install the gunicorn server and run it: <pre><code>gunicorn --certfile cert.pem --keyfile key.pem -b localhost:5000 autoapp:app\n</code></pre></p>"},{"location":"contributing/website/Website/#development","title":"Development","text":"<p>To start the React frontend in developer mode, go to <code>server/src/client/app</code> and run:</p> <pre><code>npm run start\n</code></pre> <p>The app should automatically open at <code>localhost:3000</code> and any changes made to the code will automatically reload the website (hot loading).</p> <p>For the new Next.js frontend, install and run like this: <pre><code>cd app\nnpm install\nnpm run dev\n</code></pre></p>"},{"location":"contributing/website/Website/#structure","title":"Structure","text":"<p>The website is built on the following components:  </p> <ul> <li>A Flask backend. Written in Python, the backend takes care of all communication with the OpenML server. It builds on top of the OpenML Python API. It also takes care of user authentication and keeps the search engine (ElasticSearch) up to date with the latest information from the server. Files are located in the <code>server</code> folder.</li> <li>A React frontend. Written in JavaScript, this takes care of rendering the website. It pulls in information from the search engine, and shows plots rendered by Dash. It also contains forms (e.g. for logging in or uploading new datasets), which will be sent off to the backend for processing. Files are located in <code>server/src/client/app</code>.</li> <li>Dash dashboards. Written in Python, Dash is used for writing interactive plots. It pulls in data from the Python API, and renders the plots as React components. Files are located in <code>server/src/dashboard</code>.</li> </ul>"},{"location":"ecosystem/","title":"Ecosystem","text":"<p>OpenML has a rich ecosystem of tools and projects that seamlessly integrate OpenML in various ways. </p> <p>Add your library</p> <p>Did you use OpenML in your work and want to share it with the community? We would love to have you! Simply create a pull request with the necessary information (click the  icon) and we will add it to this page.</p> <p>Integrate OpenML in your libraries</p> <p>If you want to integrate OpenML into machine learning and data science tools, it's easiest to build on one of the existing clients,  which often can be used as is or extended. For instance, see how to extend the Python API to integrate OpenML into Python tools. </p> automlbenchmark  402 stars <p>OpenML AutoML Benchmarking Framework</p> openml-python  280 stars <p>OpenML's Python API for a World of Data and More \ud83d\udcab</p> openml-r  95 stars <p>R package to interface with OpenML</p> openml-rust  11 stars <p>A rust interface to http://openml.org/</p> OpenML.jl  10 stars <p>Partial implementation of the OpenML API for Julia</p> continual-automl  5 stars <p>Adaptations of AutoML libraries H2O, Autosklearn and GAMA for stream learning</p> openml-pytorch  5 stars <p>Pytorch extension for openml-python</p> openml-dotnet  5 stars <p>.NET API</p> openml-rapidminer  4 stars <p>RapidMiner plugin</p> openml-tensorflow  2 stars <p>Tensorflow extension for openml-python</p> OpenmlCortana  0 stars <p>Openml Cortana connector</p> openml-keras  0 stars <p>Keras extension for openml-python</p> openml-croissant  0 stars <p>Converting dataset metadata from OpenML to Croissant format</p> openml-azure  0 stars <p>Tools for interfacing with Azure</p> openml-onnx  0 stars <p>onnx extension for openml</p> openml-mxnet  0 stars <p>MXNet extension for openml</p> flow-visualization  0 stars <p>Tool to convert openml flows to ONNX and visualize them via Netron</p>"},{"location":"ecosystem/Java/","title":"Java","text":"<p>The Java API allows you connect to OpenML from Java applications.</p>"},{"location":"ecosystem/Java/#java-docs","title":"Java Docs","text":"<p>Read the full Java Docs.</p>"},{"location":"ecosystem/Java/#download","title":"Download","text":"<p>Stable releases of the Java API are available from Maven Central Or, you can check out the developer version from GitHub</p> <p>Include the jar file in your projects as usual, or install via Maven.</p>"},{"location":"ecosystem/Java/#quick-start","title":"Quick Start","text":"<ul> <li>Create an <code>OpenmlConnector</code> instance with your authentication details. This will create a client with all OpenML functionalities. <p>OpenmlConnector client = new OpenmlConnector(\"api_key\")</p> </li> </ul> <p>All functions are described in the Java Docs.</p>"},{"location":"ecosystem/Java/#downloading","title":"Downloading","text":"<p>To download data, flows, tasks, runs, etc. you need the unique id of that resource. The id is shown on each item's webpage and in the corresponding url. For instance, let's download Data set 1. The following returns a DataSetDescription object that contains all information about that data set.</p> <pre><code>DataSetDescription data = client.dataGet(1);\n</code></pre> <p>You can also search for the items you need online, and click the icon to get all id's that match a search.</p>"},{"location":"ecosystem/Java/#uploading","title":"Uploading","text":"<p>To upload data, flows, runs, etc. you need to provide a description of the object. We provide wrapper classes to provide this information, e.g. <code>DataSetDescription</code>, as well as to capture the server response, e.g. <code>UploadDataSet</code>, which always includes the generated id for reference:</p> <pre><code>DataSetDescription description = new DataSetDescription( \"iris\", \"The famous iris dataset\", \"arff\", \"class\");\nUploadDataSet result = client.dataUpload( description, datasetFile );\nint data_id = result.getId();\n</code></pre> <p>More details are given in the corresponding functions below. Also see the Java Docs for all possible inputs and return values.</p>"},{"location":"ecosystem/Java/#data-download","title":"Data download","text":""},{"location":"ecosystem/Java/#datagetint-data_id","title":"<code>dataGet(int data_id)</code>","text":"<p>Retrieves the description of a specified data set.</p> <pre><code>DataSetDescription data = client.dataGet(1);\nString name = data.getName();\nString version = data.getVersion();\nString description = data.getDescription();\nString url = data.getUrl();\n</code></pre>"},{"location":"ecosystem/Java/#datafeaturesint-data_id","title":"<code>dataFeatures(int data_id)</code>","text":"<p>Retrieves the description of the features of a specified data set.</p> <pre><code>DataFeature reponse = client.dataFeatures(1);\nDataFeature.Feature[] features = reponse.getFeatures();\nString name = features[0].getName();\nString type = features[0].getDataType();\nboolean isTarget = features[0].getIs_target();\n</code></pre>"},{"location":"ecosystem/Java/#dataqualityint-data_id","title":"<code>dataQuality(int data_id)</code>","text":"<p>Retrieves the description of the qualities (meta-features) of a specified data set.</p> <pre><code>    DataQuality response = client.dataQuality(1);\n    DataQuality.Quality[] qualities = reponse.getQualities();\n    String name = qualities[0].getName();\n    String value = qualities[0].getValue();\n</code></pre>"},{"location":"ecosystem/Java/#dataqualityint-data_id-int-start-int-end-int-interval_size","title":"<code>dataQuality(int data_id, int start, int end, int interval_size)</code>","text":"<p>For data streams. Retrieves the description of the qualities (meta-features) of a specified portion of a data stream.</p> <pre><code>    DataQuality qualities = client.dataQuality(1,0,10000,null);\n</code></pre>"},{"location":"ecosystem/Java/#dataqualitylist","title":"<code>dataQualityList()</code>","text":"<p>Retrieves a list of all data qualities known to OpenML.</p> <pre><code>    DataQualityList response = client.dataQualityList();\n    String[] qualities = response.getQualities();\n</code></pre>"},{"location":"ecosystem/Java/#data-upload","title":"Data upload","text":""},{"location":"ecosystem/Java/#datauploaddatasetdescription-description-file-dataset","title":"<code>dataUpload(DataSetDescription description, File dataset)</code>","text":"<p>Uploads a data set file to OpenML given a description. Throws an exception if the upload failed, see openml.data.upload for error codes.</p> <pre><code>    DataSetDescription dataset = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\");\n    UploadDataSet data = client.dataUpload( dataset, new File(\"data/path\"));\n    int data_id = result.getId();\n</code></pre>"},{"location":"ecosystem/Java/#datauploaddatasetdescription-description","title":"<code>dataUpload(DataSetDescription description)</code>","text":"<p>Registers an existing dataset (hosted elsewhere). The description needs to include the url of the data set. Throws an exception if the upload failed, see openml.data.upload for error codes.</p> <pre><code>    DataSetDescription description = new DataSetDescription( \"iris\", \"The iris dataset\", \"arff\", \"class\");\n    description.setUrl(\"http://datarepository.org/mydataset\");\n    UploadDataSet data = client.dataUpload( description );\n    int data_id = result.getId();\n</code></pre>"},{"location":"ecosystem/Java/#flow-download","title":"Flow download","text":""},{"location":"ecosystem/Java/#flowgetint-flow_id","title":"<code>flowGet(int flow_id)</code>","text":"<p>Retrieves the description of the flow/implementation with the given id.</p> <pre><code>    Implementation flow = client.flowGet(100);\n    String name = flow.getName();\n    String version = flow.getVersion();\n    String description = flow.getDescription();\n    String binary_url = flow.getBinary_url();\n    String source_url = flow.getSource_url();\n    Parameter[] parameters = flow.getParameter();\n</code></pre>"},{"location":"ecosystem/Java/#flow-management","title":"Flow management","text":""},{"location":"ecosystem/Java/#flowowned","title":"<code>flowOwned()</code>","text":"<p>Retrieves an array of id's of all flows/implementations owned by you.</p> <pre><code>    ImplementationOwned response = client.flowOwned();\n    Integer[] ids = response.getIds();\n</code></pre>"},{"location":"ecosystem/Java/#flowexistsstring-name-string-version","title":"<code>flowExists(String name, String version)</code>","text":"<p>Checks whether an implementation with the given name and version is already registered on OpenML.</p> <pre><code>    ImplementationExists check = client.flowExists(\"weka.j48\", \"3.7.12\");\n    boolean exists = check.exists();\n    int flow_id = check.getId();\n</code></pre>"},{"location":"ecosystem/Java/#flowdeleteint-id","title":"<code>flowDelete(int id)</code>","text":"<p>Removes the flow with the given id (if you are its owner).</p> <pre><code>    ImplementationDelete response = client.openmlImplementationDelete(100);\n</code></pre>"},{"location":"ecosystem/Java/#flow-upload","title":"Flow upload","text":""},{"location":"ecosystem/Java/#flowuploadimplementation-description-file-binary-file-source","title":"<code>flowUpload(Implementation description, File binary, File source)</code>","text":"<p>Uploads implementation files (binary and/or source) to OpenML given a description.</p> <pre><code>    Implementation flow = new Implementation(\"weka.J48\", \"3.7.12\", \"description\", \"Java\", \"WEKA 3.7.12\")\n    UploadImplementation response = client.flowUpload( flow, new File(\"code.jar\"), new File(\"source.zip\"));\n    int flow_id = response.getId();\n</code></pre>"},{"location":"ecosystem/Java/#task-download","title":"Task download","text":""},{"location":"ecosystem/Java/#taskgetint-task_id","title":"<code>taskGet(int task_id)</code>","text":"<p>Retrieves the description of the task with the given id.</p> <pre><code>    Task task = client.taskGet(1);\n    String task_type = task.getTask_type();\n    Input[] inputs = task.getInputs();\n    Output[] outputs = task.getOutputs();\n</code></pre>"},{"location":"ecosystem/Java/#taskevaluationsint-task_id","title":"<code>taskEvaluations(int task_id)</code>","text":"<p>Retrieves all evaluations for the task with the given id.</p> <pre><code>    TaskEvaluations response = client.taskEvaluations(1);\n    Evaluation[] evaluations = response.getEvaluation();\n</code></pre>"},{"location":"ecosystem/Java/#taskevaluationsint-task_id-int-start-int-end-int-interval_size","title":"<code>taskEvaluations(int task_id, int start, int end, int interval_size)</code>","text":"<p>For data streams. Retrieves all evaluations for the task over the specified window of the stream.</p> <pre><code>    TaskEvaluations response = client.taskEvaluations(1);\n    Evaluation[] evaluations = response.getEvaluation();\n</code></pre>"},{"location":"ecosystem/Java/#run-download","title":"Run download","text":""},{"location":"ecosystem/Java/#rungetint-run_id","title":"<code>runGet(int run_id)</code>","text":"<p>Retrieves the description of the run with the given id.</p> <pre><code>    Run run = client.runGet(1);\n    int task_id = run.getTask_id();\n    int flow_id = run.getImplementation_id();\n    Parameter_setting[] settings = run.getParameter_settings()\n    EvaluationScore[] scores = run.getOutputEvaluation();\n</code></pre>"},{"location":"ecosystem/Java/#run-management","title":"Run management","text":""},{"location":"ecosystem/Java/#rundeleteint-run_id","title":"<code>runDelete(int run_id)</code>","text":"<p>Deletes the run with the given id (if you are its owner).</p> <pre><code>    RunDelete response = client.runDelete(1);\n</code></pre>"},{"location":"ecosystem/Java/#run-upload","title":"Run upload","text":""},{"location":"ecosystem/Java/#runuploadrun-description-mapstringfile-output_files","title":"<code>runUpload(Run description, Map&lt;String,File&gt; output_files)</code>","text":"<p>Uploads a run to OpenML, including a description and a set of output files depending on the task type.</p> <pre><code>    Run.Parameter_setting[] parameter_settings = new Run.Parameter_setting[1];\n    parameter_settings[0] = Run.Parameter_setting(null, \"M\", \"2\");\n    Run run = new Run(\"1\", null, \"100\", \"setup_string\", parameter_settings);\n    Map outputs = new HashMap&lt;String,File&gt;();\n    outputs.add(\"predictions\",new File(\"predictions.arff\"));\n    UploadRun response = client.runUpload( run, outputs);\n    int run_id = response.getRun_id();\n</code></pre>"},{"location":"ecosystem/MOA/","title":"MOA","text":"<p>OpenML features extensive support for MOA. However currently this is implemented as a stand alone MOA compilation, using the latest version (as of May, 2014).</p> <p>Download MOA for OpenML</p>"},{"location":"ecosystem/MOA/#quick-start","title":"Quick Start","text":"<ul> <li>Download the standalone MOA environment above.</li> <li>Find your API key in your profile (log in first). Create a config file called <code>openml.conf</code> in a <code>.openml</code> directory in your home dir. It should contain the following lines: <p>api_key = YOUR_KEY</p> </li> <li>Launch the JAR file by double clicking on it, or launch from command-line using the following command: <p>java -cp openmlmoa.beta.jar moa.gui.GUI</p> </li> <li>Select the task <code>moa.tasks.openml.OpenmlDataStreamClassification</code> to evaluate a classifier on an OpenML task, and send the results to OpenML.</li> <li>Optionally, you can generate new streams using the Bayesian Network Generator: select the <code>moa.tasks.WriteStreamToArff</code> task, with <code>moa.streams.generators.BayesianNetworkGenerator</code>.</li> </ul>"},{"location":"ecosystem/Python_extensions/","title":"Integrating your Python libraries","text":"<p>OpenML-Python provides an extension interface to connect other machine learning libraries than scikit-learn to OpenML. Please check the <code>api_extensions</code> and use the scikit-learn extension in <code>openml.extensions.sklearn.SklearnExtension</code>{.interpreted-text role=\"class\"} as a starting point.</p>"},{"location":"ecosystem/Python_extensions/#connecting-new-machine-learning-libraries","title":"Connecting new machine learning libraries","text":""},{"location":"ecosystem/Python_extensions/#content-of-the-library","title":"Content of the Library","text":"<p>To leverage support from the community and to tap in the potential of OpenML, interfacing with popular machine learning libraries is essential. The OpenML-Python package is capable of downloading meta-data and results (data, flows, runs), regardless of the library that was used to upload it. However, in order to simplify the process of uploading flows and runs from a specific library, an additional interface can be built. The OpenML-Python team does not have the capacity to develop and maintain such interfaces on its own. For this reason, we have built an extension interface to allows others to contribute back. Building a suitable extension for therefore requires an understanding of the current OpenML-Python support.</p> <p>The <code>sphx_glr_examples_20_basic_simple_flows_and_runs_tutorial.py</code>{.interpreted-text role=\"ref\"} tutorial shows how scikit-learn currently works with OpenML-Python as an extension. The sklearn extension packaged with the openml-python repository can be used as a template/benchmark to build the new extension.</p>"},{"location":"ecosystem/Python_extensions/#api","title":"API","text":"<ul> <li>The extension scripts must import the [openml]{.title-ref} package     and be able to interface with any function from the OpenML-Python     <code>api</code>.</li> <li>The extension has to be defined as a Python class and must inherit     from <code>openml.extensions.Extension</code>.</li> <li>This class needs to have all the functions from [class     Extension]{.title-ref} overloaded as required.</li> <li>The redefined functions should have adequate and appropriate     docstrings. The [Sklearn Extension API     :class:`openml.extensions.sklearn.SklearnExtension.html]{.title-ref}     is a good example to follow.</li> </ul>"},{"location":"ecosystem/Python_extensions/#interfacing-with-openml-python","title":"Interfacing with OpenML-Python","text":"<p>Once the new extension class has been defined, the openml-python module to <code>openml.extensions.register_extension</code> must be called to allow OpenML-Python to interface the new extension.</p> <p>The following methods should get implemented. Although the documentation in the [Extension]{.title-ref} interface should always be leading, here we list some additional information and best practices. The [Sklearn Extension API :class:`openml.extensions.sklearn.SklearnExtension.html]{.title-ref} is a good example to follow. Note that most methods are relatively simple and can be implemented in several lines of code.</p> <ul> <li>General setup (required)<ul> <li><code>can_handle_flow</code>: Takes as     argument an OpenML flow, and checks whether this can be handled     by the current extension. The OpenML database consists of many     flows, from various workbenches (e.g., scikit-learn, Weka, mlr).     This method is called before a model is being deserialized.     Typically, the flow-dependency field is used to check whether     the specific library is present, and no unknown libraries are     present there.</li> <li><code>can_handle_model</code>: Similar as     <code>can_handle_flow</code>, except that in     this case a Python object is given. As such, in many cases, this     method can be implemented by checking whether this adheres to a     certain base class.</li> </ul> </li> <li>Serialization and De-serialization (required)<ul> <li><code>flow_to_model</code>: deserializes the     OpenML Flow into a model (if the library can indeed handle the     flow). This method has an important interplay with     <code>model_to_flow</code>. Running these     two methods in succession should result in exactly the same     model (or flow). This property can be used for unit testing     (e.g., build a model with hyperparameters, make predictions on a     task, serialize it to a flow, deserialize it back, make it     predict on the same task, and check whether the predictions are     exactly the same.) The example in the scikit-learn interface     might seem daunting, but note that here some complicated design     choices were made, that allow for all sorts of interesting     research questions. It is probably good practice to start easy.</li> <li><code>model_to_flow</code>: The inverse of     <code>flow_to_model</code>. Serializes a     model into an OpenML Flow. The flow should preserve the class,     the library version, and the tunable hyperparameters.</li> <li><code>get_version_information</code>: Return     a tuple with the version information of the important libraries.</li> <li><code>create_setup_string</code>: No longer     used, and will be deprecated soon.</li> </ul> </li> <li>Performing runs (required)<ul> <li><code>is_estimator</code>: Gets as input a     class, and checks whether it has the status of estimator in the     library (typically, whether it has a train method and a predict     method).</li> <li><code>seed_model</code>: Sets a random seed     to the model.</li> <li><code>_run_model_on_fold</code>: One of the     main requirements for a library to generate run objects for the     OpenML server. Obtains a train split (with labels) and a test     split (without labels) and the goal is to train a model on the     train split and return the predictions on the test split. On top     of the actual predictions, also the class probabilities should     be determined. For classifiers that do not return class     probabilities, this can just be the hot-encoded predicted label.     The predictions will be evaluated on the OpenML server. Also,     additional information can be returned, for example,     user-defined measures (such as runtime information, as this can     not be inferred on the server). Additionally, information about     a hyperparameter optimization trace can be provided.</li> <li><code>obtain_parameter_values</code>:     Obtains the hyperparameters of a given model and the current     values. Please note that in the case of a hyperparameter     optimization procedure (e.g., random search), you only should     return the hyperparameters of this procedure (e.g., the     hyperparameter grid, budget, etc) and that the chosen model will     be inferred from the optimization trace.</li> <li><code>check_if_model_fitted</code>: Check     whether the train method of the model has been called (and as     such, whether the predict method can be used).</li> </ul> </li> <li>Hyperparameter optimization (optional)<ul> <li><code>instantiate_model_from_hpo_class</code>{.interpreted-text     role=\"meth\"}: If a given run has recorded the hyperparameter     optimization trace, then this method can be used to     reinstantiate the model with hyperparameters of a given     hyperparameter optimization iteration. Has some similarities     with <code>flow_to_model</code> (as this     method also sets the hyperparameters of a model). Note that     although this method is required, it is not necessary to     implement any logic if hyperparameter optimization is not     implemented. Simply raise a [NotImplementedError]{.title-ref}     then.</li> </ul> </li> </ul>"},{"location":"ecosystem/Python_extensions/#hosting-the-library","title":"Hosting the library","text":"<p>Each extension created should be a stand-alone repository, compatible with the OpenML-Python repository. The extension repository should work off-the-shelf with OpenML-Python installed.</p> <p>Create a public Github repo with the following directory structure:</p> <pre><code>| [repo name]\n|    |-- [extension name]\n|    |    |-- __init__.py\n|    |    |-- extension.py\n|    |    |-- config.py (optionally)\n</code></pre>"},{"location":"ecosystem/Python_extensions/#recommended","title":"Recommended","text":"<ul> <li>Test cases to keep the extension up to date with the     [openml-python]{.title-ref} upstream changes.</li> <li>Documentation of the extension API, especially if any new     functionality added to OpenML-Python\\'s extension design.</li> <li>Examples to show how the new extension interfaces and works with     OpenML-Python.</li> <li>Create a PR to add the new extension to the OpenML-Python API     documentation.</li> </ul> <p>Happy contributing!</p>"},{"location":"ecosystem/Rest/","title":"REST tutorial","text":"<p>OpenML offers a RESTful Web API, with predictive URLs, for uploading and downloading machine learning resources. Try the API Documentation to see examples of all calls, and test them right in your browser.</p>"},{"location":"ecosystem/Rest/#getting-started","title":"Getting started","text":"<ul> <li>REST services can be called using simple HTTP GET or POST actions.</li> <li>The REST Endpoint URL is <code>https://www.openml.org/api/v1/</code></li> <li>The default endpoint returns data in XML. If you prefer JSON, use the endpoint <code>https://www.openml.org/api/v1/json/</code>. Note that, to upload content, you still need to use XML (at least for now).</li> </ul>"},{"location":"ecosystem/Rest/#testing","title":"Testing","text":"<p>For continuous integration and testing purposes, we have a test server offering the same API, but which does not affect the production server.</p> <ul> <li>The test server REST Endpoint URL is <code>https://test.openml.org/api/v1/</code></li> </ul>"},{"location":"ecosystem/Rest/#error-messages","title":"Error messages","text":"<p>Error messages will look like this:</p> <pre><code>&lt;oml:error xmlns:oml=\"http://openml.org/error\"&gt;\n&lt;oml:code&gt;100&lt;/oml:code&gt;\n&lt;oml:message&gt;Please invoke legal function&lt;/oml:message&gt;\n&lt;oml:additional_information&gt;Additional information, not always available.&lt;/oml:additional_information&gt;\n&lt;/oml:error&gt;\n</code></pre> <p>All error messages are listed in the API documentation. E.g. try to get a non-existing dataset:</p> <ul> <li>in XML: https://www.openml.org/api_new/v1/data/99999</li> <li>in JSON: https://www.openml.org/api_new/v1/json/data/99999</li> </ul>"},{"location":"ecosystem/Rest/#examples","title":"Examples","text":"<p>You need to be logged in for these examples to work.</p>"},{"location":"ecosystem/Rest/#download-a-dataset","title":"Download a dataset","text":"<ul> <li>User asks for a dataset using the /data/{id} service. The <code>dataset id</code> is typically part of a task, or can be found on OpenML.org.</li> <li>OpenML returns a description of the dataset as an XML file (or JSON). Try it now</li> <li>The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.</li> <li>The dataset is returned by the server hosting the dataset. This can be OpenML, but also any other data repository. Try it now</li> </ul>"},{"location":"ecosystem/Rest/#download-a-flow","title":"Download a flow","text":"<ul> <li>User asks for a flow using the /flow/{id} service and a <code>flow id</code>. The <code>flow id</code> can be found on OpenML.org.</li> <li>OpenML returns a description of the flow as an XML file (or JSON). Try it now</li> <li>The flow description contains the URL where the flow can be downloaded (e.g. GitHub), either as source, binary or both, as well as additional information on history, dependencies and licence. The user calls the right URL to download it.</li> <li>The flow is returned by the server hosting it. This can be OpenML, but also any other code repository. Try it now</li> </ul>"},{"location":"ecosystem/Rest/#download-a-task","title":"Download a task","text":"<ul> <li>User asks for a task using the /task/{id} service and a <code>task id</code>. The <code>task id</code> is typically returned when searching for tasks.</li> <li>OpenML returns a description of the task as an XML file (or JSON). Try it now</li> <li>The task description contains the <code>dataset id</code>(s) of the datasets involved in this task. The user asks for the dataset using the /data/{id} service and the <code>dataset id</code>.</li> <li>OpenML returns a description of the dataset as an XML file (or JSON). Try it now</li> <li>The dataset description contains the URL where the dataset can be downloaded. The user calls that URL to download the dataset.</li> <li>The dataset is returned by the server hosting it. This can be OpenML, but also any other data repository. Try it now</li> <li>The task description may also contain links to other resources, such as the train-test splits to be used in cross-validation. The user calls that URL to download the train-test splits.</li> <li>The train-test splits are returned by OpenML. Try it now</li> </ul>"},{"location":"ecosystem/Weka/","title":"Weka","text":"<p>OpenML is integrated in the Weka (Waikato Environment for Knowledge Analysis) Experimenter and the Command Line Interface.</p>"},{"location":"ecosystem/Weka/#installation","title":"Installation","text":"<p>OpenML is available as a weka extension in the package manager:</p> <ul> <li>Download the latest version (3.7.13 or higher).</li> <li>Launch Weka, or start from commandline: <p>java -jar weka.jar</p> </li> <li>If you need more memory (e.g. 1GB), start as follows: <p>java -Xmx1G -jar weka.jar</p> </li> <li>Open the package manager (Under 'Tools')</li> <li>Select package OpenmlWeka and click install. Afterwards, restart WEKA.</li> <li>From the Tools menu, open the 'OpenML Experimenter'.</li> </ul>"},{"location":"ecosystem/Weka/#graphical-interface","title":"Graphical Interface","text":"<p>You can solve OpenML Tasks in the Weka Experimenter, and automatically upload your experiments to OpenML (or store them locally).  </p> <ul> <li>From the Tools menu, open the 'OpenML Experimenter'.</li> <li>Enter your API key in the top field (log in first). You can also store this in a config file (see below).</li> <li>In the 'Tasks' panel, click the 'Add New' button to add new tasks. Insert the task id's as comma-separated values (e.g., '1,2,3,4,5'). Use the search function on OpenML to find interesting tasks and click the ID icon to list the ID's. In the future this search will also be integrated in WEKA.</li> <li>Add algorithms in the \"Algorithm\" panel.</li> <li>Go to the \"Run\" tab, and click on the \"Start\" button.</li> <li>The experiment will be executed and sent to OpenML.org.</li> <li>The runs will now appear on OpenML.org. You can follow their progress and check for errors on your profile page under 'Runs'.</li> </ul>"},{"location":"ecosystem/Weka/#commandline-interface","title":"CommandLine Interface","text":"<p>The Command Line interface is useful for running experiments automatically on a server, without using a GUI.</p> <ul> <li>Create a config file called <code>openml.conf</code> in a new directory called <code>.openml</code> in your home dir. It should contain the following line: <p>api_key = YOUR_KEY</p> </li> <li>Execute the following command: <p>java -cp weka.jar openml.experiment.TaskBasedExperiment -T  -C  --  <li>For example, the following command will run Weka's J48 algorithm on Task 1: <p>java -cp OpenWeka.beta.jar openml.experiment.TaskBasedExperiment -T 1 -C weka.classifiers.trees.J48</p> </li> <li>The following suffix will set some parameters of this classifier: <p>-- -C 0.25 -M 2</p> </li>"},{"location":"ecosystem/Weka/#api-reference","title":"API reference","text":"<p>Check the Weka integration Java Docs for more details about the possibilities.</p>"},{"location":"ecosystem/Weka/#issues","title":"Issues","text":"<p>Please report any bugs that you may encounter in the issue tracker: https://github.com/openml/openml-weka Or email to j.n.van.rijn@liacs.leidenuniv.nl</p>"},{"location":"ecosystem/mlr/","title":"Machine Learning in R (mlr)","text":"<p>OpenML is readily integrated with mlr through the mlr3oml package.</p> <p>Example</p> <pre><code>library(mlr3oml)\nlibrary(mlr3)\n\n# Search for specific datasets\nodatasets = list_oml_data(\nnumber_features = c(10, 20),\nnumber_instances = c(45000, 50000),\nnumber_classes = 2\n)\n\n# Get dataset\nodata = odt(id = 1590)\n# Access the actual data\nodata$data\n\n# Convert to an mlr3::Task\ntsk_adult = as_task(odata, target = \"class\")\n</code></pre> <p>Key features:  </p> <ul> <li>Query and download OpenML datasets and use them however you like  </li> <li>Build any mlr learner, run it on any task and save the experiment as run objects  </li> <li>Upload your runs for collaboration or publishing  </li> <li>Query, download and reuse all shared runs  </li> </ul> <p>There is also an older (deprecated) OpenML R package.</p>"},{"location":"ecosystem/Scikit-learn/","title":"scikit-learn","text":"<p>OpenML is readily integrated with scikit-learn through the Python API. This page provides a brief overview of the key features and installation instructions. For more detailed API documentation, please refer to the official documentation.</p>"},{"location":"ecosystem/Scikit-learn/#key-features","title":"Key features:","text":"<ul> <li>Query and download OpenML datasets and use them however you like</li> <li>Build any sklearn estimator or pipeline and convert to OpenML flows</li> <li>Run any flow on any task and save the experiment as run objects</li> <li>Upload your runs for collaboration or publishing</li> <li>Query, download and reuse all shared runs</li> </ul>"},{"location":"ecosystem/Scikit-learn/#installation","title":"Installation","text":"<pre><code>pip install openml\n</code></pre>"},{"location":"ecosystem/Scikit-learn/#query-and-download-data","title":"Query and download data","text":"<pre><code>import openml\n\n# List all datasets and their properties\nopenml.datasets.list_datasets(output_format=\"dataframe\")\n\n# Get dataset by ID\ndataset = openml.datasets.get_dataset(61)\n\n# Get dataset by name\ndataset = openml.datasets.get_dataset('Fashion-MNIST')\n\n# Get the data itself as a dataframe (or otherwise)\nX, y, _, _ = dataset.get_data(dataset_format=\"dataframe\")\n</code></pre>"},{"location":"ecosystem/Scikit-learn/#download-tasks-run-models-locally-publish-results-with-scikit-learn","title":"Download tasks, run models locally, publish results (with scikit-learn)","text":"<pre><code>from sklearn import ensemble\nfrom openml import tasks, runs\n\n# Build any model you like\nclf = ensemble.RandomForestClassifier()\n\n# Download any OpenML task\ntask = tasks.get_task(3954)\n\n# Run and evaluate your model on the task\nrun = runs.run_model_on_task(clf, task)\n\n# Share the results on OpenML. Your API key can be found in your account.\n# openml.config.apikey = 'YOUR_KEY'\nrun.publish()\n</code></pre>"},{"location":"ecosystem/Scikit-learn/#openml-benchmarks","title":"OpenML Benchmarks","text":"<pre><code># List all tasks in a benchmark\nbenchmark = openml.study.get_suite('OpenML-CC18')\ntasks.list_tasks(output_format=\"dataframe\", task_id=benchmark.tasks)\n\n# Return benchmark results\nopenml.evaluations.list_evaluations(\n    function=\"area_under_roc_curve\",\n    tasks=benchmark.tasks,\n    output_format=\"dataframe\"\n)\n</code></pre>"},{"location":"ecosystem/Scikit-learn/basic_tutorial/","title":"Basic tutorial","text":"In\u00a0[12]: Copied! <pre>from IPython.display import display, HTML, Markdown\nimport os\nimport yaml\nwith open(\"../../../mkdocs.yml\", \"r\") as f:\n    load_config = yaml.safe_load(f)\nrepo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\")\nbinder_url = load_config[\"binder_url\"]\nrelative_file_path = \"integrations/Scikit-learn/basic_tutorial.ipynb\"\ndisplay(HTML(f\"\"\"&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/{repo_url}/{relative_file_path}\"&gt;\n  &lt;img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/&gt;\n&lt;/a&gt;\"\"\"))\ndisplay(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\"))\n</pre> from IPython.display import display, HTML, Markdown import os import yaml with open(\"../../../mkdocs.yml\", \"r\") as f:     load_config = yaml.safe_load(f) repo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\") binder_url = load_config[\"binder_url\"] relative_file_path = \"integrations/Scikit-learn/basic_tutorial.ipynb\" display(HTML(f\"\"\" \"\"\")) display(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\")) In\u00a0[\u00a0]: Copied! <pre>!pip install openml\n</pre> !pip install openml In\u00a0[2]: Copied! <pre>import openml\nfrom sklearn import impute, tree, pipeline\n</pre> import openml from sklearn import impute, tree, pipeline In\u00a0[7]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <pre>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/openml/config.py:184: UserWarning: Switching to the test server https://test.openml.org/api/v1/xml to not upload results to the live server. Using the test server may result in reduced performance of the API!\n  warnings.warn(\n</pre> In\u00a0[8]: Copied! <pre># Define a scikit-learn classifier or pipeline\nclf = pipeline.Pipeline(\n    steps=[\n        ('imputer', impute.SimpleImputer()),\n        ('estimator', tree.DecisionTreeClassifier())\n    ]\n)\n</pre>  # Define a scikit-learn classifier or pipeline clf = pipeline.Pipeline(     steps=[         ('imputer', impute.SimpleImputer()),         ('estimator', tree.DecisionTreeClassifier())     ] )  In\u00a0[9]: Copied! <pre># Download the OpenML task for the pendigits dataset with 10-fold\n# cross-validation.\ntask = openml.tasks.get_task(32)\ntask\n</pre>  # Download the OpenML task for the pendigits dataset with 10-fold # cross-validation. task = openml.tasks.get_task(32) task Out[9]: <pre>OpenML Classification Task\n==========================\nTask Type Description: https://test.openml.org/tt/TaskType.SUPERVISED_CLASSIFICATION\nTask ID..............: 32\nTask URL.............: https://test.openml.org/t/32\nEstimation Procedure.: crossvalidation\nTarget Feature.......: class\n# of Classes.........: 10\nCost Matrix..........: Available</pre> In\u00a0[11]: Copied! <pre># Run the scikit-learn model on the task.\nrun = openml.runs.run_model_on_task(clf, task)\n# Publish the experiment on OpenML (optional, requires an API key.\n# You can get your own API key by signing up to OpenML.org)\n</pre> # Run the scikit-learn model on the task. run = openml.runs.run_model_on_task(clf, task) # Publish the experiment on OpenML (optional, requires an API key. # You can get your own API key by signing up to OpenML.org)  In\u00a0[\u00a0]: Copied! <pre>run.publish()\nprint(f'View the run online: {run.openml_url}')\n</pre>  run.publish() print(f'View the run online: {run.openml_url}')"},{"location":"ecosystem/Scikit-learn/datasets_tutorial/","title":"Datasets","text":"In\u00a0[2]: Copied! <pre>from IPython.display import display, HTML, Markdown\nimport os\nimport yaml\nwith open(\"../../../mkdocs.yml\", \"r\") as f:\n    load_config = yaml.safe_load(f)\nrepo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\")\nbinder_url = load_config[\"binder_url\"]\nrelative_file_path = \"integrations/Scikit-learn/datasets_tutorial.ipynb\"\ndisplay(HTML(f\"\"\"&lt;a target=\"_blank\" href=\"https://colab.research.google.com/github/{repo_url}/{relative_file_path}\"&gt;\n  &lt;img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/&gt;\n&lt;/a&gt;\"\"\"))\ndisplay(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\"))\n</pre> from IPython.display import display, HTML, Markdown import os import yaml with open(\"../../../mkdocs.yml\", \"r\") as f:     load_config = yaml.safe_load(f) repo_url = load_config[\"repo_url\"].replace(\"https://github.com/\", \"\") binder_url = load_config[\"binder_url\"] relative_file_path = \"integrations/Scikit-learn/datasets_tutorial.ipynb\" display(HTML(f\"\"\" \"\"\")) display(Markdown(\"[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/SubhadityaMukherjee/openml_docs/HEAD?labpath=Scikit-learn%2Fdatasets_tutorial)\")) In\u00a0[9]: Copied! <pre>!pip install openml\n</pre> !pip install openml <pre>Requirement already satisfied: openml in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (0.14.2)\nRequirement already satisfied: scikit-learn&gt;=0.18 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (1.4.2)\nRequirement already satisfied: requests in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.31.0)\nRequirement already satisfied: liac-arff&gt;=2.4.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.5.0)\nRequirement already satisfied: numpy&gt;=1.6.2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (1.26.4)\nRequirement already satisfied: minio in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (7.2.7)\nRequirement already satisfied: pandas&gt;=1.0.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.2.2)\nRequirement already satisfied: scipy&gt;=0.13.3 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (1.13.0)\nRequirement already satisfied: pyarrow in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (16.0.0)\nRequirement already satisfied: xmltodict in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (0.13.0)\nRequirement already satisfied: python-dateutil in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from openml) (2.9.0.post0)\nRequirement already satisfied: tzdata&gt;=2022.7 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pandas&gt;=1.0.0-&gt;openml) (2024.1)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from pandas&gt;=1.0.0-&gt;openml) (2024.1)\nRequirement already satisfied: six&gt;=1.5 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from python-dateutil-&gt;openml) (1.16.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from scikit-learn&gt;=0.18-&gt;openml) (3.5.0)\nRequirement already satisfied: joblib&gt;=1.2.0 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from scikit-learn&gt;=0.18-&gt;openml) (1.4.0)\nRequirement already satisfied: urllib3 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (2.2.1)\nRequirement already satisfied: typing-extensions in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (4.11.0)\nRequirement already satisfied: pycryptodome in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (3.20.0)\nRequirement already satisfied: certifi in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (2024.2.2)\nRequirement already satisfied: argon2-cffi in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from minio-&gt;openml) (23.1.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests-&gt;openml) (3.7)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from requests-&gt;openml) (3.3.2)\nRequirement already satisfied: argon2-cffi-bindings in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from argon2-cffi-&gt;minio-&gt;openml) (21.2.0)\nRequirement already satisfied: cffi&gt;=1.0.1 in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from argon2-cffi-bindings-&gt;argon2-cffi-&gt;minio-&gt;openml) (1.16.0)\nRequirement already satisfied: pycparser in /Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages (from cffi&gt;=1.0.1-&gt;argon2-cffi-bindings-&gt;argon2-cffi-&gt;minio-&gt;openml) (2.22)\n\n[notice] A new release of pip is available: 23.0.1 -&gt; 24.0\n[notice] To update, run: pip install --upgrade pip\n</pre> In\u00a0[2]: Copied! <pre># License: BSD 3-Clauses\n\nimport openml\nimport pandas as pd\nfrom openml.datasets import edit_dataset, fork_dataset, get_dataset\n</pre> # License: BSD 3-Clauses  import openml import pandas as pd from openml.datasets import edit_dataset, fork_dataset, get_dataset In\u00a0[3]: Copied! <pre>datalist = openml.datasets.list_datasets(output_format=\"dataframe\")\ndatalist = datalist[[\"did\", \"name\", \"NumberOfInstances\", \"NumberOfFeatures\", \"NumberOfClasses\"]]\n\nprint(f\"First 10 of {len(datalist)} datasets...\")\ndatalist.head(n=10)\n\n# The same can be done with lesser lines of code\nopenml_df = openml.datasets.list_datasets(output_format=\"dataframe\")\nopenml_df.head(n=10)\n</pre> datalist = openml.datasets.list_datasets(output_format=\"dataframe\") datalist = datalist[[\"did\", \"name\", \"NumberOfInstances\", \"NumberOfFeatures\", \"NumberOfClasses\"]]  print(f\"First 10 of {len(datalist)} datasets...\") datalist.head(n=10)  # The same can be done with lesser lines of code openml_df = openml.datasets.list_datasets(output_format=\"dataframe\") openml_df.head(n=10) <pre>First 10 of 5466 datasets...\n</pre> Out[3]: did name version uploader status format MajorityClassSize MaxNominalAttDistinctValues MinorityClassSize NumberOfClasses NumberOfFeatures NumberOfInstances NumberOfInstancesWithMissingValues NumberOfMissingValues NumberOfNumericFeatures NumberOfSymbolicFeatures 2 2 anneal 1 1 active ARFF 684.0 7.0 8.0 5.0 39.0 898.0 898.0 22175.0 6.0 33.0 3 3 kr-vs-kp 1 1 active ARFF 1669.0 3.0 1527.0 2.0 37.0 3196.0 0.0 0.0 0.0 37.0 4 4 labor 1 1 active ARFF 37.0 3.0 20.0 2.0 17.0 57.0 56.0 326.0 8.0 9.0 5 5 arrhythmia 1 1 active ARFF 245.0 13.0 2.0 13.0 280.0 452.0 384.0 408.0 206.0 74.0 6 6 letter 1 1 active ARFF 813.0 26.0 734.0 26.0 17.0 20000.0 0.0 0.0 16.0 1.0 7 7 audiology 1 1 active ARFF 57.0 24.0 1.0 24.0 70.0 226.0 222.0 317.0 0.0 70.0 8 8 liver-disorders 1 1 active ARFF NaN NaN NaN 0.0 6.0 345.0 0.0 0.0 6.0 0.0 9 9 autos 1 1 active ARFF 67.0 22.0 3.0 6.0 26.0 205.0 46.0 59.0 15.0 11.0 10 10 lymph 1 1 active ARFF 81.0 8.0 2.0 4.0 19.0 148.0 0.0 0.0 3.0 16.0 11 11 balance-scale 1 1 active ARFF 288.0 3.0 49.0 3.0 5.0 625.0 0.0 0.0 4.0 1.0 In\u00a0[4]: Copied! <pre>datalist[datalist.NumberOfInstances &gt; 10000].sort_values([\"NumberOfInstances\"]).head(n=20)\n\"\"\ndatalist.query('name == \"eeg-eye-state\"')\n\"\"\ndatalist.query(\"NumberOfClasses &gt; 50\")\n</pre> datalist[datalist.NumberOfInstances &gt; 10000].sort_values([\"NumberOfInstances\"]).head(n=20) \"\" datalist.query('name == \"eeg-eye-state\"') \"\" datalist.query(\"NumberOfClasses &gt; 50\") Out[4]: did name NumberOfInstances NumberOfFeatures NumberOfClasses 1491 1491 one-hundred-plants-margin 1600.0 65.0 100.0 1492 1492 one-hundred-plants-shape 1600.0 65.0 100.0 1493 1493 one-hundred-plants-texture 1599.0 65.0 100.0 4552 4552 BachChoralHarmony 5665.0 17.0 102.0 41167 41167 dionis 416188.0 61.0 355.0 41169 41169 helena 65196.0 28.0 100.0 41960 41960 seattlecrime6 523590.0 8.0 144.0 41983 41983 CIFAR-100 60000.0 3073.0 100.0 42078 42078 beer_reviews 1586614.0 13.0 104.0 42087 42087 beer_reviews 1586614.0 13.0 104.0 42088 42088 beer_reviews 1586614.0 13.0 104.0 42089 42089 vancouver_employee 1586614.0 13.0 104.0 42123 42123 article_influence 3615.0 7.0 3169.0 42223 42223 dataset-autoHorse_fixed 201.0 69.0 186.0 42396 42396 aloi 108000.0 129.0 1000.0 43723 43723 Toronto-Apartment-Rental-Price 1124.0 7.0 188.0 44282 44282 Meta_Album_PLK_Mini 3440.0 3.0 86.0 44283 44283 Meta_Album_FLW_Mini 4080.0 3.0 102.0 44284 44284 Meta_Album_SPT_Mini 2920.0 3.0 73.0 44285 44285 Meta_Album_BRD_Mini 12600.0 3.0 315.0 44288 44288 Meta_Album_TEX_Mini 2560.0 3.0 64.0 44289 44289 Meta_Album_CRS_Mini 7840.0 3.0 196.0 44292 44292 Meta_Album_INS_2_Mini 4080.0 3.0 102.0 44298 44298 Meta_Album_DOG_Mini 4800.0 3.0 120.0 44304 44304 Meta_Album_TEX_ALOT_Mini 10000.0 3.0 250.0 44306 44306 Meta_Album_INS_Mini 4160.0 3.0 104.0 44317 44317 Meta_Album_PLK_Extended 473273.0 3.0 102.0 44318 44318 Meta_Album_FLW_Extended 8189.0 3.0 102.0 44319 44319 Meta_Album_SPT_Extended 10416.0 3.0 73.0 44320 44320 Meta_Album_BRD_Extended 49054.0 3.0 315.0 44322 44322 Meta_Album_TEX_Extended 8675.0 3.0 64.0 44323 44323 Meta_Album_CRS_Extended 16185.0 3.0 196.0 44326 44326 Meta_Album_INS_2_Extended 75222.0 3.0 102.0 44331 44331 Meta_Album_DOG_Extended 20480.0 3.0 120.0 44337 44337 Meta_Album_TEX_ALOT_Extended 25000.0 3.0 250.0 44340 44340 Meta_Album_INS_Extended 170506.0 3.0 117.0 44533 44533 dionis_seed_0_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44534 44534 dionis_seed_1_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44535 44535 dionis_seed_2_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44536 44536 dionis_seed_3_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44537 44537 dionis_seed_4_nrows_2000_nclasses_10_ncols_100... 2000.0 61.0 355.0 44728 44728 helena_seed_0_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 44729 44729 helena_seed_1_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 44730 44730 helena_seed_2_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 44731 44731 helena_seed_3_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 44732 44732 helena_seed_4_nrows_2000_nclasses_10_ncols_100... 2000.0 28.0 100.0 45049 45049 MD_MIX_Mini_Copy 28240.0 69.0 706.0 45102 45102 dailybike 731.0 13.0 606.0 45103 45103 dailybike 731.0 13.0 606.0 45104 45104 PLK_Mini_Copy 3440.0 3.0 86.0 45274 45274 PASS 1439588.0 7.0 94137.0 45569 45569 DBLP-QuAD 10000.0 10.0 9999.0 45923 45923 IndoorScenes 15620.0 3.0 67.0 45936 45936 IndoorScenes 15620.0 3.0 67.0 In\u00a0[5]: Copied! <pre># This is done based on the dataset ID.\ndataset = openml.datasets.get_dataset(1471)\n\n# Print a summary\nprint(\n    f\"This is dataset '{dataset.name}', the target feature is \"\n    f\"'{dataset.default_target_attribute}'\"\n)\nprint(f\"URL: {dataset.url}\")\nprint(dataset.description[:500])\n</pre> # This is done based on the dataset ID. dataset = openml.datasets.get_dataset(1471)  # Print a summary print(     f\"This is dataset '{dataset.name}', the target feature is \"     f\"'{dataset.default_target_attribute}'\" ) print(f\"URL: {dataset.url}\") print(dataset.description[:500]) <pre>This is dataset 'eeg-eye-state', the target feature is 'Class'\nURL: https://api.openml.org/data/v1/download/1587924/eeg-eye-state.arff\n**Author**: Oliver Roesler  \n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State), Baden-Wuerttemberg, Cooperative State University (DHBW), Stuttgart, Germany  \n**Please cite**: [UCI](https://archive.ics.uci.edu/ml/citation_policy.html)  \n\nAll data is from one continuous EEG measurement with the Emotiv EEG Neuroheadset. The duration of the measurement was 117 seconds. The eye state was detected via a camera during the EEG measurement and added later manually to the file after\n</pre> <p>Get the actual data.</p> <p>openml-python returns data as pandas dataframes (stored in the <code>eeg</code> variable below), and also some additional metadata that we don't care about right now.</p> In\u00a0[6]: Copied! <pre>eeg, *_ = dataset.get_data()\n</pre> eeg, *_ = dataset.get_data() <p>You can optionally choose to have openml separate out a column from the dataset. In particular, many datasets for supervised problems have a set <code>default_target_attribute</code> which may help identify the target variable.</p> In\u00a0[7]: Copied! <pre>X, y, categorical_indicator, attribute_names = dataset.get_data(\n    target=dataset.default_target_attribute\n)\nprint(X.head())\nprint(X.info())\n</pre> X, y, categorical_indicator, attribute_names = dataset.get_data(     target=dataset.default_target_attribute ) print(X.head()) print(X.info()) <pre>        V1       V2       V3       V4       V5       V6       V7       V8  \\\n0  4329.23  4009.23  4289.23  4148.21  4350.26  4586.15  4096.92  4641.03   \n1  4324.62  4004.62  4293.85  4148.72  4342.05  4586.67  4097.44  4638.97   \n2  4327.69  4006.67  4295.38  4156.41  4336.92  4583.59  4096.92  4630.26   \n3  4328.72  4011.79  4296.41  4155.90  4343.59  4582.56  4097.44  4630.77   \n4  4326.15  4011.79  4292.31  4151.28  4347.69  4586.67  4095.90  4627.69   \n\n        V9      V10      V11      V12      V13      V14  \n0  4222.05  4238.46  4211.28  4280.51  4635.90  4393.85  \n1  4210.77  4226.67  4207.69  4279.49  4632.82  4384.10  \n2  4207.69  4222.05  4206.67  4282.05  4628.72  4389.23  \n3  4217.44  4235.38  4210.77  4287.69  4632.31  4396.41  \n4  4210.77  4244.10  4212.82  4288.21  4632.82  4398.46  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 14980 entries, 0 to 14979\nData columns (total 14 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   V1      14980 non-null  float64\n 1   V2      14980 non-null  float64\n 2   V3      14980 non-null  float64\n 3   V4      14980 non-null  float64\n 4   V5      14980 non-null  float64\n 5   V6      14980 non-null  float64\n 6   V7      14980 non-null  float64\n 7   V8      14980 non-null  float64\n 8   V9      14980 non-null  float64\n 9   V10     14980 non-null  float64\n 10  V11     14980 non-null  float64\n 11  V12     14980 non-null  float64\n 12  V13     14980 non-null  float64\n 13  V14     14980 non-null  float64\ndtypes: float64(14)\nmemory usage: 1.6 MB\nNone\n</pre> <p>Sometimes you only need access to a dataset's metadata. In those cases, you can download the dataset without downloading the data file. The dataset object can be used as normal. Whenever you use any functionality that requires the data, such as <code>get_data</code>, the data will be downloaded. Starting from 0.15, not downloading data will be the default behavior instead. The data will be downloading automatically when you try to access it through openml objects, e.g., using <code>dataset.features</code>.</p> In\u00a0[8]: Copied! <pre>dataset = openml.datasets.get_dataset(1471, download_data=False)\n</pre> dataset = openml.datasets.get_dataset(1471, download_data=False) In\u00a0[9]: Copied! <pre>eegs = eeg.sample(n=1000)\n_ = pd.plotting.scatter_matrix(\n    X.iloc[:100, :4],\n    c=y[:100],\n    figsize=(10, 10),\n    marker=\"o\",\n    hist_kwds={\"bins\": 20},\n    alpha=0.8,\n    cmap=\"plasma\",\n)\n</pre> eegs = eeg.sample(n=1000) _ = pd.plotting.scatter_matrix(     X.iloc[:100, :4],     c=y[:100],     figsize=(10, 10),     marker=\"o\",     hist_kwds={\"bins\": 20},     alpha=0.8,     cmap=\"plasma\", ) <pre>/Users/eragon/.pyenv/versions/3.9.19/envs/openml/lib/python3.9/site-packages/pandas/plotting/_matplotlib/misc.py:97: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  ax.scatter(\n</pre> In\u00a0[\u00a0]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <p>Edit non-critical fields, allowed for all authorized users: description, creator, contributor, collection_date, language, citation, original_data_url, paper_url</p> In\u00a0[\u00a0]: Copied! <pre>desc = (\n    \"This data sets consists of 3 different types of irises' \"\n    \"(Setosa, Versicolour, and Virginica) petal and sepal length,\"\n    \" stored in a 150x4 numpy.ndarray\"\n)\ndid = 128\ndata_id = edit_dataset(\n    did,\n    description=desc,\n    creator=\"R.A.Fisher\",\n    collection_date=\"1937\",\n    citation=\"The use of multiple measurements in taxonomic problems\",\n    language=\"English\",\n)\nedited_dataset = get_dataset(data_id)\nprint(f\"Edited dataset ID: {data_id}\")\n</pre> desc = (     \"This data sets consists of 3 different types of irises' \"     \"(Setosa, Versicolour, and Virginica) petal and sepal length,\"     \" stored in a 150x4 numpy.ndarray\" ) did = 128 data_id = edit_dataset(     did,     description=desc,     creator=\"R.A.Fisher\",     collection_date=\"1937\",     citation=\"The use of multiple measurements in taxonomic problems\",     language=\"English\", ) edited_dataset = get_dataset(data_id) print(f\"Edited dataset ID: {data_id}\") <p>Editing critical fields (default_target_attribute, row_id_attribute, ignore_attribute) is allowed only for the dataset owner. Further, critical fields cannot be edited if the dataset has any tasks associated with it. To edit critical fields of a dataset (without tasks) owned by you, configure the API key: openml.config.apikey = 'FILL_IN_OPENML_API_KEY' This example here only shows a failure when trying to work on a dataset not owned by you:</p> In\u00a0[\u00a0]: Copied! <pre>try:\n    data_id = edit_dataset(1, default_target_attribute=\"shape\")\nexcept openml.exceptions.OpenMLServerException as e:\n    print(e)\n</pre> try:     data_id = edit_dataset(1, default_target_attribute=\"shape\") except openml.exceptions.OpenMLServerException as e:     print(e) In\u00a0[\u00a0]: Copied! <pre>data_id = fork_dataset(1)\nprint(data_id)\ndata_id = edit_dataset(data_id, default_target_attribute=\"shape\")\nprint(f\"Forked dataset ID: {data_id}\")\n\nopenml.config.stop_using_configuration_for_example()\n</pre> data_id = fork_dataset(1) print(data_id) data_id = edit_dataset(data_id, default_target_attribute=\"shape\") print(f\"Forked dataset ID: {data_id}\")  openml.config.stop_using_configuration_for_example()"},{"location":"ecosystem/Scikit-learn/datasets_tutorial/#datasets","title":"Datasets\u00b6","text":"<p>How to list and download datasets.</p>"},{"location":"ecosystem/Scikit-learn/datasets_tutorial/#exercise-0","title":"Exercise 0\u00b6","text":"<ul> <li><p>List datasets</p> <ul> <li>Use the output_format parameter to select output type</li> <li>Default gives 'dict' (other option: 'dataframe', see below)</li> </ul> </li> </ul> <p>Note: list_datasets will return a pandas dataframe by default from 0.15. When using openml-python 0.14, <code>list_datasets</code> will warn you to use output_format='dataframe'.</p>"},{"location":"ecosystem/Scikit-learn/datasets_tutorial/#exercise-1","title":"Exercise 1\u00b6","text":"<ul> <li>Find datasets with more than 10000 examples.</li> <li>Find a dataset called 'eeg_eye_state'.</li> <li>Find all datasets with more than 50 classes.</li> </ul>"},{"location":"ecosystem/Scikit-learn/datasets_tutorial/#download-datasets","title":"Download datasets\u00b6","text":""},{"location":"ecosystem/Scikit-learn/datasets_tutorial/#exercise-2","title":"Exercise 2\u00b6","text":"<ul> <li>Explore the data visually.</li> </ul>"},{"location":"ecosystem/Scikit-learn/datasets_tutorial/#edit-a-created-dataset","title":"Edit a created dataset\u00b6","text":"<p>This example uses the test server, to avoid editing a dataset on the main server.</p> Warning<p>.. include:: ../../test_server_usage_warning.txt</p>"},{"location":"ecosystem/Scikit-learn/datasets_tutorial/#fork-dataset","title":"Fork dataset\u00b6","text":"<p>Used to create a copy of the dataset with you as the owner. Use this API only if you are unable to edit the critical fields (default_target_attribute, ignore_attribute, row_id_attribute) of a dataset through the edit_dataset API. After the dataset is forked, you can edit the new version of the dataset using edit_dataset.</p>"},{"location":"intro/Governance/","title":"Governance","text":"<p>The purpose of this document is to formalize the governance process used by the OpenML project (the OpenML GitHub organization which contains all code and projects related to OpenML.org), to clarify how decisions are made and how the various elements of our community interact. This document establishes a decision-making structure that takes into account feedback from all members of the community and strives to find consensus, while avoiding any deadlocks.</p> <p>The OpenML project is an independent open source project that is legally represented by the Open Machine Learning Foundation. The Open Machine Learning Foundation is a not-for-profit organization supporting, but not controlling, the OpenML project. The Foundation is open to engage with universities, companies, or anyone sharing the same goals. The OpenML project has a separate governance model described in this document.</p> <p>This is a meritocratic, consensus-based community project. Anyone with an interest in the project can join the community, contribute to the project design, and participate in the decision making process. This document describes how that participation takes place and how to set about earning merit within the project community.</p>"},{"location":"intro/Governance/#roles-and-responsibilities","title":"Roles And Responsibilities","text":""},{"location":"intro/Governance/#contributors","title":"Contributors","text":"<p>Contributors are community members who contribute in concrete ways to the project. Anyone can become a contributor, and contributions can take many forms \u2013 not only code \u2013 as detailed in the contributors guide. Contributors need to create pull requests to contribute to the code or documentation.</p>"},{"location":"intro/Governance/#core-contributors","title":"Core contributors","text":"<p>Core contributors are community members who have shown that they are dedicated to the continued development of the project through ongoing engagement with the community. They have shown they can be trusted to maintain OpenML with care. Being a core contributor allows contributors to more easily carry on with their project related activities by giving them write access to the project\u2019s repository (abiding by the decision making process described below, e.g. merging pull requests that obey the decision making procedure described below) and is represented as being an organization member on the OpenML GitHub organization. Core contributors are expected to review code contributions, can merge approved pull requests, can cast votes for and against merging a pull-request, and can be involved in deciding major changes to the API.</p> <p>New core contributors can be nominated by any existing core contributors. Once they have been nominated, there will be a vote in the private OpenML core email list by the current core contributors. While it is expected that most votes will be unanimous, a two-thirds majority of the cast votes is enough. The vote needs to be open for at least 1 week.</p> <p>Core contributors that have not contributed to the project (commits or GitHub comments) in the past 12 months will become emeritus core contributors and recant their commit and voting rights until they become active again. The list of core contributors, active and emeritus (with dates at which they became active) is public on the OpenML website.</p>"},{"location":"intro/Governance/#steering-committee","title":"Steering Committee","text":"<p>The Steering Committee (SC) members are core contributors who have additional responsibilities to ensure the smooth running of the project. SC members are expected to participate in strategic planning, join monthly meetings, and approve changes to the governance model. The purpose of the SC is to ensure a smooth progress from the big-picture perspective. Indeed, changes that impact the full project require a synthetic analysis and a consensus that is both explicit and informed. In cases that the core contributor community (which includes the SC members) fails to reach such a consensus in the required time frame, the SC is the entity to resolve the issue.</p> <p>The SC consists of community representatives and partner representatives. Community representatives of the SC are nominated by a core contributor. A nomination will result in a discussion that cannot take more than a month and then a vote by the core contributors which will stay open for a week. SC membership votes are subject to a two-third majority of all cast votes as well as a simple majority approval of all the current SC members.</p> <p>Partner institutions who enter a collaboration agreement or sponsorship agreement with the OpenML Foundation can nominate a representative on the Steering Committee, if so agreed in the agreement. Such a collaboration should in principle include one full-time developer to work on OpenML (in cash or in kind) for the duration of the agreement. New partner representatives have to be confirmed by the SC following the same voting rules above.</p> <p>The OpenML community must have at least equal footing in the steering committee. Additional SC members may be nominated to ensure this, following the membership voting rules described above.</p> <p>When decisions are escalated to the steering committee (see the decision making process below), and no consensus can be found within a month, the SC can meet and decide by consensus or with a simple majority of all cast votes.</p> <p>SC members who do not actively engage with the SC duties are expected to resign.</p> <p>The initial Steering Committee of OpenML consists of Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Heidi Seibold, Jan van Rijn, and Joaquin Vanschoren. They all represent the OpenML community.</p>"},{"location":"intro/Governance/#decision-making-process","title":"Decision Making Process","text":"<p>Decisions about the future of the project are made through discussion with all members of the community. All non-sensitive project management discussion takes place on the project contributors\u2019 mailing list and the issue trackers of the sub-projects. Occasionally, sensitive discussion occurs on the private core developer email list (see below). This includes voting on core/SC membership or discussion of internal disputes. All discussions must follow the OpenML honor code.</p> <p>OpenML uses a \u201cconsensus seeking\u201d process for making decisions. The group tries to find a resolution that has no open objections among core contributors. At any point during the discussion, any core contributors can call for a vote, which will conclude one month from the call for the vote, or when two thirds of all votes are in favor.</p> <p>If no option can gather two thirds of the votes cast (ignoring abstentions), the decision is escalated to the SC, which in turn will use consensus seeking with the fallback option of a simple majority vote if no consensus can be found within a month. This is what we hereafter may refer to as \u201cthe decision making process\u201d. It applies to all core OpenML repositories.</p> <p>Decisions (in addition to adding core contributors and SC membership as above) are made according to the following rules:</p> <p>Normal changes:  </p> <ul> <li>Minor Documentation changes, such as typo fixes, or addition / correction of a sentence: requires one approved review by a core contributor, and no objections in the comments (lazy consensus). Core contributors are expected to give \u201creasonable time\u201d to others to give their opinion on the pull request if they\u2019re not confident others would agree.  </li> <li>Non-server packages that only have one core contributor are not subject to the ruling in the bullet point above (i.e. a sole core developer can make decisions on their own).</li> </ul> <p>Major changes:  </p> <ul> <li>Major changes to the API principles and metadata schema require a concrete proposal outlined in an OpenML Request for Comments (RfC), which has to be opened for public consultation for at least 1 month. The final version has to be approved using the decision-making process outlined above (two-third of the cast vote by core contributors or simple majority if escalated to the SC). Voting is typically done as a comment in the pull request (+1, -1, or 0 to abstain).  </li> <li>RfCs must be announced and shared via the public mailing list and may link additional content (such as blog posts or google docs etc. detailing the changes).  </li> <li>Changes to the governance model use the same decision process outlined above.  </li> </ul> <p>If a veto -1 vote is cast on a lazy consensus, the proposer can appeal to the community and core contributors and the change can be approved or rejected using the decision making procedure outlined above.</p>"},{"location":"intro/Governance/#communication-channels","title":"Communication channels","text":"<p>OpenML uses the following communication channels:  </p> <ul> <li>An open contributor mailing list and the GitHub issue trackers.  </li> <li>A chat application for daily interaction with the community (currently Slack).  </li> <li>Private email lists (without archive) for the core developers (core@openml.org) and steering committee (steering@openml.org), for membership voting and sensitive discussions.  </li> <li>Biyearly Steering Committee meeting at predefined times, listed on the website, and asynchronous discussions on a discussion board. They are open to all steering committee members and core contributors, and they can all request discussion on a topic. Closed meetings for SC members only can be called in if there are sensitive discussions or other valid reasons.  </li> <li>A monthly Engineering meeting at predefined times, listed on the website. The meeting is open to all. Discussion points are put on the [project roadmap]  (https://github.com/orgs/openml/projects/2).</li> </ul>"},{"location":"intro/showcase/","title":"Research using OpenML","text":"<p>This page will have a list of interesting research papers that have used OpenML. If you have used OpenML in your research and would like to have your paper listed here, please drop a PR with the relevant information (click the  icon above).</p>"},{"location":"intro/terms/","title":"Terms","text":""},{"location":"intro/terms/#honor-code","title":"Honor Code","text":"<p>By joining OpenML, you join a special worldwide community of data scientists building on each other's results and connecting their minds as efficiently as possible. This community depends on your motivation to share data, tools and ideas, and to do so with honesty. In return, you will gain trust, visibility and reputation, igniting online collaborations and studies that otherwise may not have happened.</p> <p>By using any part of OpenML, you agree to:</p> <ul> <li>Give credit where credit is due. Cite the authors whose work you are building on, or build collaborations where appropriate.</li> <li>Give back to the community by sharing your own data as openly and as soon as possible, or by helping the community in other ways. In doing so, you gain visibility and impact (citations).</li> <li>Share data according to your best efforts. Everybody make mistakes, but we trust you to correct them as soon as possible. Remove or flag data that cannot be trusted.</li> <li>Be polite and constructive in all discussions. Criticism of methods is welcomed, but personal criticisms should be avoided.</li> <li>Respect circles of trust. OpenML allows you to collaborate in 'circles' of trusted people to share unpublished results. Be considerate in sharing data with people outside this circle.</li> <li>Do not steal the work of people who openly share it. OpenML makes it easy to find all shared data (and when it was shared), thus everybody will know if you do this.</li> </ul>"},{"location":"intro/terms/#terms-of-use","title":"Terms of Use","text":"<p>You agree that you are responsible for your own use of OpenML.org and all content submitted by you, in accordance with the Honor Code and all applicable local, state, national and international laws.</p> <p>By submitting or distributing content from OpenML.org, you affirm that you have the necessary rights, licenses, consents and/or permissions to reproduce and publish this content. You cannot upload sensitive or confidential data. You, and not the developers of OpenML.org, are solely responsible for your submissions.</p> <p>By submitting content to OpenML.org, you grant OpenML.org the right to host, transfer, display and use this content, in accordance with your sharing settings and any licences granted by you. You also grant to each user a non-exclusive license to access and use this content for their own research purposes, in accordance with any licences granted by you.</p> <p>You may maintain one user account and not let anyone else use your username and/or password. You may not impersonate other persons.</p> <p>You will not intend to damage, disable, or impair any OpenML server or interfere with any other party's use and enjoyment of the service. You may not attempt to gain unauthorized access to the Site, other accounts, computer systems or networks connected to any OpenML server. You may not obtain or attempt to obtain any materials or information not intentionally made available through OpenML.</p> <p>Strictly prohibited are content that defames, harasses or threatens others, that infringes another's intellectual property, as well as indecent or unlawful content, advertising, or intentionally inaccurate information posted with the intent of misleading others. It is also prohibited to post code containing viruses, malware, spyware or any other similar software that may damage the operation of another's computer or property.</p>"},{"location":"notebooks/getting_started/","title":"Getting Started","text":"In\u00a0[2]: Copied! <pre>!pip install -q openml\n</pre> !pip install -q openml In\u00a0[3]: Copied! <pre># License: BSD 3-Clause\n\nimport openml\nfrom sklearn import neighbors\n</pre> # License: BSD 3-Clause  import openml from sklearn import neighbors In\u00a0[4]: Copied! <pre>openml.config.start_using_configuration_for_example()\n</pre> openml.config.start_using_configuration_for_example() <pre>/var/folders/0t/5d8ttqzd773fy0wq3h5db0xr0000gn/T/ipykernel_60921/256497051.py:1: UserWarning: Switching to the test server https://test.openml.org/api/v1/xml to not upload results to the live server. Using the test server may result in reduced performance of the API!\n  openml.config.start_using_configuration_for_example()\n</pre> <p>When using the main server instead, make sure your apikey is configured. This can be done with the following line of code (uncomment it!). Never share your apikey with others.</p> In\u00a0[5]: Copied! <pre># openml.config.apikey = 'YOURKEY'\n</pre> # openml.config.apikey = 'YOURKEY' In\u00a0[6]: Copied! <pre># Uncomment and set your OpenML cache directory\n# import os\n# openml.config.cache_directory = os.path.expanduser('YOURDIR')\n</pre> # Uncomment and set your OpenML cache directory # import os # openml.config.cache_directory = os.path.expanduser('YOURDIR') In\u00a0[7]: Copied! <pre>task = openml.tasks.get_task(403)\ndata = openml.datasets.get_dataset(task.dataset_id)\nclf = neighbors.KNeighborsClassifier(n_neighbors=5)\nrun = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False)\n# Publish the experiment on OpenML (optional, requires an API key).\n# For this tutorial, our configuration publishes to the test server\n# as to not crowd the main server with runs created by examples.\nmyrun = run.publish()\nprint(f\"kNN on {data.name}: {myrun.openml_url}\")\n</pre> task = openml.tasks.get_task(403) data = openml.datasets.get_dataset(task.dataset_id) clf = neighbors.KNeighborsClassifier(n_neighbors=5) run = openml.runs.run_model_on_task(clf, task, avoid_duplicate_runs=False) # Publish the experiment on OpenML (optional, requires an API key). # For this tutorial, our configuration publishes to the test server # as to not crowd the main server with runs created by examples. myrun = run.publish() print(f\"kNN on {data.name}: {myrun.openml_url}\") <pre>kNN on eeg-eye-state: https://test.openml.org/r/32906\n</pre> In\u00a0[8]: Copied! <pre>openml.config.stop_using_configuration_for_example()\n</pre> openml.config.stop_using_configuration_for_example()"},{"location":"notebooks/getting_started/#getting-started","title":"Getting Started\u00b6","text":"<p>This page will guide you through the process of getting started with OpenML. While this page is a good starting point, for more detailed information, please refer to the integrations section and the rest of the documentation.</p>"},{"location":"notebooks/getting_started/#authentication","title":"Authentication\u00b6","text":"<ul> <li>If you are using the OpenML API to download datasets, upload results, or create tasks, you will need to authenticate. You can do this by creating an account on the OpenML website and using your API key. - You can find detailed instructions on how to authenticate in the authentication section</li> </ul>"},{"location":"notebooks/getting_started/#eeg-eye-state-example","title":"EEG Eye State example\u00b6","text":"<p>Download the OpenML task for the eeg-eye-state.</p>"},{"location":"notebooks/getting_started/#caching","title":"Caching\u00b6","text":"<p>When downloading datasets, tasks, runs and flows, they will be cached to retrieve them without calling the server later. As with the API key, the cache directory can be either specified through the config file or through the API:</p> <ul> <li>Add the  line cachedir = 'MYDIR' to the config file, replacing 'MYDIR' with the path to the cache directory. By default, OpenML will use ~/.openml/cache as the cache directory.</li> <li>Run the code below, replacing 'YOURDIR' with the path to the cache directory.</li> </ul>"},{"location":"pytorch/","title":"Pytorch extension for OpenML python","text":"<p>Pytorch extension for openml-python API.</p>"},{"location":"pytorch/#installation-instructions","title":"Installation Instructions:","text":"<p><code>pip install openml-pytorch</code></p> <p>PyPi link https://pypi.org/project/openml-pytorch/</p>"},{"location":"pytorch/#usage","title":"Usage","text":"<p>To use this extension, you need to have a task from OpenML. You can either browse the OpenML website to find a task (and get it's ID), or follow the example to create a task from a custom dataset.</p> <p>Then, follow one of the examples in the Examples folder to see how to use this extension for your type of data.</p> <p>Import openML libraries <pre><code>import torch.nn\nimport torch.optim\n\nimport openml_pytorch.config\nimport openml\nimport logging\n\nfrom openml_pytorch.trainer import OpenMLTrainerModule\nfrom openml_pytorch.trainer import OpenMLDataModule\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\nfrom openml_pytorch.trainer import convert_to_rgb\n</code></pre> Create a pytorch model and get a task from openML <pre><code>model = torchvision.models.efficientnet_b0(num_classes=200)\n# Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(362128)\n</code></pre> Download the task from openML and define Data and Trainer configuration <pre><code>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(\n            convert_to_rgb\n        ),  # Convert PIL Image to RGB if it's not already.\n        Resize(\n            (64, 64)\n        ),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\ndata_module = OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size = 64,\n    transform=transform\n)\ntrainer = OpenMLTrainerModule(\n    data_module=data_module,\n    verbose = True,\n    epoch_count = 1,\n)\nopenml_pytorch.config.trainer = trainer\n</code></pre> Run the model on the task <pre><code>run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\nrun.publish()\nprint('URL for run: %s/run/%d' % (openml.config.server, run.run_id))\n</code></pre> Note: The input layer of the network should be compatible with OpenML data output shape. Please check examples for more information.</p> <p>Additionally, if you want to publish the run with onnx file, then you must call <code>openml_pytorch.add_onnx_to_run()</code> immediately before <code>run.publish()</code>. </p> <pre><code>run = openml_pytorch.add_onnx_to_run(run)\n</code></pre>"},{"location":"pytorch/Limitations%20of%20the%20API/","title":"Limitations","text":"<ul> <li>Image datasets are supported as a workaround by using a CSV file with image paths. This is not ideal and might eventually be replaced by something else. At the moment, the focus is on tabular data.</li> <li>Many features (like custom metrics, models etc) are still dependant on the OpenML Python API, which is in the middle of a major rewrite. Until that is complete, this package will not be able to provide all the features it aims to.</li> </ul>"},{"location":"pytorch/Philosophy%20behind%20the%20API%20Design/","title":"Philosophy behind the API design","text":"<p>This API is designed to make it easier to use PyTorch with OpenML and has been heavily inspired by the current state of the art Deep Learning frameworks like FastAI and PyTorch Lightning. </p> <p>To make the library as modular as possible, callbacks are used throughout the training loop. This allows for easy customization of the training loop without having to modify the core code.</p>"},{"location":"pytorch/Philosophy%20behind%20the%20API%20Design/#separation-of-concerns","title":"Separation of Concerns","text":"<p>Here, we focus on the data, model and training as separate blocks that can be strung together in a pipeline. This makes it easier to experiment with different models, data and training strategies.</p> <p>That being the case, the OpenMLDataModule and OpenMLTrainerModule are designed to handle the data and training respectively. This might seem a bit verbose at first, but it makes it easier to understand what is happening at each step of the process and allows for easier customization.</p>"},{"location":"pytorch/API%20reference/Callbacks/","title":"Callbacks","text":"<p>Callbacks module contains classes and functions for handling callback functions during an event-driven process. This makes it easier to customize the behavior of the training loop and add additional functionality to the training process without modifying the core code.</p> <p>To use a callback, create a class that inherits from the Callback class and implement the necessary methods. Callbacks can be used to perform actions at different stages of the training process, such as at the beginning or end of an epoch, batch, or fitting process. Then pass the callback object to the Trainer.</p>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.AvgStats","title":"<code>AvgStats</code>","text":"<p>AvgStats class is used to track and accumulate average statistics (like loss and other metrics) during training and validation phases.</p> <p>Attributes:     metrics (list): A list of metric functions to be tracked.     in_train (bool): A flag to indicate if the statistics are for the training phase.</p> <p>Methods:     init(metrics, in_train):         Initializes the AvgStats with metrics and in_train flag.</p> <pre><code>reset():\n    Resets the accumulated statistics.\n\nall_stats:\n    Property that returns all accumulated statistics including loss and metrics.\n\navg_stats:\n    Property that returns the average of the accumulated statistics.\n\naccumulate(run):\n    Accumulates the statistics using the data from the given run.\n\n__repr__():\n    Returns a string representation of the average statistics.\n</code></pre> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>class AvgStats:\n    \"\"\"\n    AvgStats class is used to track and accumulate average statistics (like loss and other metrics) during training and validation phases.\n\n    Attributes:\n        metrics (list): A list of metric functions to be tracked.\n        in_train (bool): A flag to indicate if the statistics are for the training phase.\n\n    Methods:\n        __init__(metrics, in_train):\n            Initializes the AvgStats with metrics and in_train flag.\n\n        reset():\n            Resets the accumulated statistics.\n\n        all_stats:\n            Property that returns all accumulated statistics including loss and metrics.\n\n        avg_stats:\n            Property that returns the average of the accumulated statistics.\n\n        accumulate(run):\n            Accumulates the statistics using the data from the given run.\n\n        __repr__():\n            Returns a string representation of the average statistics.\n    \"\"\"\n    def __init__(self, metrics, in_train):\n        self.metrics, self.in_train = listify(metrics), in_train\n\n    def reset(self):\n        self.tot_loss, self.count = 0.0, 0\n        self.tot_mets = [0.0] * len(self.metrics)\n\n    @property\n    def all_stats(self):\n        return [self.tot_loss.item()] + self.tot_mets\n\n    @property\n    def avg_stats(self):\n        return [o / self.count for o in self.all_stats]\n\n    def accumulate(self, run):\n        bn = run.xb.shape[0]\n        self.tot_loss += run.loss * bn\n        self.count += bn\n        for i, m in enumerate(self.metrics):\n            self.tot_mets[i] += m(run.pred, run.yb) * bn\n\n    def __repr__(self):\n        if not self.count:\n            return \"\"\n        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.AvgStatsCallBack","title":"<code>AvgStatsCallBack</code>","text":"<p>               Bases: <code>Callback</code></p> <p>AvgStatsCallBack class is a custom callback used to track and print average statistics for training and validation phases during the training loop.</p> <p>Arguments:     metrics: A list of metric functions to evaluate during training and validation.</p> <p>Methods:     init: Initializes the callback with given metrics and sets up AvgStats objects for both training and validation phases.     begin_epoch: Resets the statistics at the beginning of each epoch.     after_loss: Accumulates the metrics after computing the loss, differentiating between training and validation phases.     after_epoch: Prints the accumulated statistics for both training and validation phases after each epoch.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>class AvgStatsCallBack(Callback):\n    \"\"\"\n    AvgStatsCallBack class is a custom callback used to track and print average statistics for training and validation phases during the training loop.\n\n    Arguments:\n        metrics: A list of metric functions to evaluate during training and validation.\n\n    Methods:\n        __init__: Initializes the callback with given metrics and sets up AvgStats objects for both training and validation phases.\n        begin_epoch: Resets the statistics at the beginning of each epoch.\n        after_loss: Accumulates the metrics after computing the loss, differentiating between training and validation phases.\n        after_epoch: Prints the accumulated statistics for both training and validation phases after each epoch.\n    \"\"\"\n    def __init__(self, metrics):\n        self.train_stats, self.valid_stats = AvgStats(metrics, True), AvgStats(\n            metrics, False\n        )\n\n    def begin_epoch(self):\n        self.train_stats.reset()\n        self.valid_stats.reset()\n\n    def after_loss(self):\n        stats = self.train_stats if self.in_train else self.valid_stats\n        with torch.no_grad():\n            stats.accumulate(self.run)\n\n    def after_epoch(self):\n        print(self.train_stats)\n        print(self.valid_stats)\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.Callback","title":"<code>Callback</code>","text":"<p>Callback class is a base class designed for handling different callback functions during an event-driven process. It provides functionality to set a runner, retrieve the class name in snake_case format, directly call callback methods, and delegate attribute access to the runner if the attribute does not exist in the Callback class.</p> <p>The _order is used to decide the order of Callbacks.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>class Callback:\n    \"\"\"\n\n        Callback class is a base class designed for handling different callback functions during\n        an event-driven process. It provides functionality to set a runner, retrieve the class\n        name in snake_case format, directly call callback methods, and delegate attribute access\n        to the runner if the attribute does not exist in the Callback class.\n\n        The _order is used to decide the order of Callbacks.\n\n    \"\"\"\n    _order = 0\n\n    def set_runner(self, run) -&gt; None:\n        self.run = run\n\n    @property\n    def name(self):\n        name = re.sub(r\"Callback$\", \"\", self.__class__.__name__)\n        return camel2snake(name or \"callback\")\n\n    def __call__(self, cb_name):\n        f = getattr(self, cb_name, None)\n        if f and f():\n            return True\n        return False\n\n    def __getattr__(self, k):\n        return getattr(self.run, k)\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.ParamScheduler","title":"<code>ParamScheduler</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Manages scheduling of parameter adjustments over the course of training.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>class ParamScheduler(Callback):\n    \"\"\"\n    Manages scheduling of parameter adjustments over the course of training.\n    \"\"\"\n    _order = 1\n\n    def __init__(self, pname, sched_funcs):\n        self.pname, self.sched_funcs = pname, sched_funcs\n\n    def begin_fit(self):\n        \"\"\"\n        Prepare the scheduler at the start of the fitting process.\n        This method ensures that sched_funcs is a list with one function per parameter group.\n        \"\"\"\n        if not isinstance(self.sched_funcs, (list, tuple)):\n            self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n\n    def set_param(self):\n        \"\"\"\n        Adjust the parameter value for each parameter group based on the scheduling function.\n        Ensures the number of scheduling functions matches the number of parameter groups.\n        \"\"\"\n        assert len(self.opt.param_groups) == len(self.sched_funcs)\n        for pg, f in zip(self.opt.param_groups, self.sched_funcs):\n            pg[self.pname] = f(self.n_epochs / self.epochs)\n\n    def begin_batch(self):\n        \"\"\"\n        Apply parameter adjustments at the beginning of each batch if in training mode.\n        \"\"\"\n        if self.in_train:\n            self.set_param()\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.ParamScheduler.begin_batch","title":"<code>begin_batch()</code>","text":"<p>Apply parameter adjustments at the beginning of each batch if in training mode.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def begin_batch(self):\n    \"\"\"\n    Apply parameter adjustments at the beginning of each batch if in training mode.\n    \"\"\"\n    if self.in_train:\n        self.set_param()\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.ParamScheduler.begin_fit","title":"<code>begin_fit()</code>","text":"<p>Prepare the scheduler at the start of the fitting process. This method ensures that sched_funcs is a list with one function per parameter group.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def begin_fit(self):\n    \"\"\"\n    Prepare the scheduler at the start of the fitting process.\n    This method ensures that sched_funcs is a list with one function per parameter group.\n    \"\"\"\n    if not isinstance(self.sched_funcs, (list, tuple)):\n        self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.ParamScheduler.set_param","title":"<code>set_param()</code>","text":"<p>Adjust the parameter value for each parameter group based on the scheduling function. Ensures the number of scheduling functions matches the number of parameter groups.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def set_param(self):\n    \"\"\"\n    Adjust the parameter value for each parameter group based on the scheduling function.\n    Ensures the number of scheduling functions matches the number of parameter groups.\n    \"\"\"\n    assert len(self.opt.param_groups) == len(self.sched_funcs)\n    for pg, f in zip(self.opt.param_groups, self.sched_funcs):\n        pg[self.pname] = f(self.n_epochs / self.epochs)\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.Recorder","title":"<code>Recorder</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Recorder is a callback class used to record learning rates and losses during the training process.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>class Recorder(Callback):\n    \"\"\"\n        Recorder is a callback class used to record learning rates and losses during the training process.\n    \"\"\"\n    def begin_fit(self):\n        \"\"\"\n        Initializes attributes necessary for the fitting process.\n\n        Sets up learning rates and losses storage.\n\n        Attributes:\n            self.lrs (list): A list of lists, where each inner list will hold learning rates for a parameter group.\n            self.losses (list): An empty list to store loss values during the fitting process.\n        \"\"\"\n        self.lrs = [[] for _ in self.opt.param_groups]\n        self.losses = []\n\n    def after_batch(self):\n        \"\"\"\n        Handles operations to execute after each training batch.\n\n        Modifies the learning rate for each parameter group in the optimizer \n        and appends the current learning rate and loss to the corresponding lists.\n\n        \"\"\"\n        if not self.in_train:\n            return\n        for pg, lr in zip(self.opt.param_groups, self.lrs):\n            lr.append(pg[\"lr\"])\n        self.losses.append(self.loss.detach().cpu())\n\n    def plot_lr(self, pgid=-1):\n        \"\"\"\n        Plots the learning rate for a given parameter group.\n        \"\"\"\n        plt.plot(self.lrs[pgid])\n\n    def plot_loss(self, skip_last=0):\n        \"\"\"\n        Plots the loss for a given parameter group.\n        \"\"\"\n        plt.plot(self.losses[: len(self.losses) - skip_last])\n\n    def plot(self, skip_last=0, pgid=-1):\n        \"\"\"\n        Generates a plot of the loss values against the learning rates.\n        \"\"\"\n        losses = [o.item() for o in self.losses]\n        lrs = self.lrs[pgid]\n        n = len(losses) - skip_last\n        plt.xscale(\"log\")\n        plt.plot(lrs[:n], losses[:n])\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.Recorder.after_batch","title":"<code>after_batch()</code>","text":"<p>Handles operations to execute after each training batch.</p> <p>Modifies the learning rate for each parameter group in the optimizer  and appends the current learning rate and loss to the corresponding lists.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def after_batch(self):\n    \"\"\"\n    Handles operations to execute after each training batch.\n\n    Modifies the learning rate for each parameter group in the optimizer \n    and appends the current learning rate and loss to the corresponding lists.\n\n    \"\"\"\n    if not self.in_train:\n        return\n    for pg, lr in zip(self.opt.param_groups, self.lrs):\n        lr.append(pg[\"lr\"])\n    self.losses.append(self.loss.detach().cpu())\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.Recorder.begin_fit","title":"<code>begin_fit()</code>","text":"<p>Initializes attributes necessary for the fitting process.</p> <p>Sets up learning rates and losses storage.</p> <p>Attributes:     self.lrs (list): A list of lists, where each inner list will hold learning rates for a parameter group.     self.losses (list): An empty list to store loss values during the fitting process.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def begin_fit(self):\n    \"\"\"\n    Initializes attributes necessary for the fitting process.\n\n    Sets up learning rates and losses storage.\n\n    Attributes:\n        self.lrs (list): A list of lists, where each inner list will hold learning rates for a parameter group.\n        self.losses (list): An empty list to store loss values during the fitting process.\n    \"\"\"\n    self.lrs = [[] for _ in self.opt.param_groups]\n    self.losses = []\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.Recorder.plot","title":"<code>plot(skip_last=0, pgid=-1)</code>","text":"<p>Generates a plot of the loss values against the learning rates.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def plot(self, skip_last=0, pgid=-1):\n    \"\"\"\n    Generates a plot of the loss values against the learning rates.\n    \"\"\"\n    losses = [o.item() for o in self.losses]\n    lrs = self.lrs[pgid]\n    n = len(losses) - skip_last\n    plt.xscale(\"log\")\n    plt.plot(lrs[:n], losses[:n])\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.Recorder.plot_loss","title":"<code>plot_loss(skip_last=0)</code>","text":"<p>Plots the loss for a given parameter group.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def plot_loss(self, skip_last=0):\n    \"\"\"\n    Plots the loss for a given parameter group.\n    \"\"\"\n    plt.plot(self.losses[: len(self.losses) - skip_last])\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.Recorder.plot_lr","title":"<code>plot_lr(pgid=-1)</code>","text":"<p>Plots the learning rate for a given parameter group.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def plot_lr(self, pgid=-1):\n    \"\"\"\n    Plots the learning rate for a given parameter group.\n    \"\"\"\n    plt.plot(self.lrs[pgid])\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.TrainEvalCallback","title":"<code>TrainEvalCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>TrainEvalCallback class is a custom callback used during the training and validation phases of a machine learning model to perform specific actions at the beginning and after certain events.</p> <p>Methods:</p> <p>begin_fit():     Initialize the number of epochs and iteration counts at the start     of the fitting process.</p> <p>after_batch():     Update the epoch and iteration counts after each batch during     training.</p> <p>begin_epoch():     Set the current epoch, switch the model to training mode, and     indicate that the model is in training.</p> <p>begin_validate():     Switch the model to evaluation mode and indicate that the model     is in validation.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>class TrainEvalCallback(Callback):\n    \"\"\"\n        TrainEvalCallback class is a custom callback used during the training\n        and validation phases of a machine learning model to perform specific\n        actions at the beginning and after certain events.\n\n        Methods:\n\n        begin_fit():\n            Initialize the number of epochs and iteration counts at the start\n            of the fitting process.\n\n        after_batch():\n            Update the epoch and iteration counts after each batch during\n            training.\n\n        begin_epoch():\n            Set the current epoch, switch the model to training mode, and\n            indicate that the model is in training.\n\n        begin_validate():\n            Switch the model to evaluation mode and indicate that the model\n            is in validation.\n    \"\"\"\n    def begin_fit(self):\n        self.run.n_epochs = 0\n        self.run.n_iter = 0\n\n    def after_batch(self):\n        if not self.in_train:\n            return\n        self.run.n_epochs += 1.0 / self.iters\n        self.run.n_iter += 1\n\n    def begin_epoch(self):\n        self.run.n_epochs = self.epoch\n        self.model.train()\n        self.run.in_train = True\n\n    def begin_validate(self):\n        self.model.eval()\n        self.run.in_train = False\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.annealer","title":"<code>annealer(f)</code>","text":"<p>A decorator function for creating a partially applied function with predefined start and end arguments. The inner function <code>_inner</code> captures the <code>start</code> and <code>end</code> parameters and returns a <code>partial</code> object that fixes these parameters for the decorated function <code>f</code>.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def annealer(f) -&gt; callable:\n    \"\"\"\n    A decorator function for creating a partially applied function with predefined start and end arguments.\n    The inner function `_inner` captures the `start` and `end` parameters and returns a `partial` object that fixes these parameters for the decorated function `f`.\n    \"\"\"\n    def _inner(start, end):\n        return partial(f, start, end)\n\n    return _inner\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.camel2snake","title":"<code>camel2snake(name)</code>","text":"<p>Convert <code>name</code> from camel case to snake case.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def camel2snake(name : str) -&gt; str:\n    \"\"\"\n    Convert `name` from camel case to snake case.\n    \"\"\"\n    s1 = re.sub(_camel_re1, r\"\\1_\\2\", name)\n    return re.sub(_camel_re2, r\"\\1_\\2\", s1).lower()\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.combine_scheds","title":"<code>combine_scheds(pcts, scheds)</code>","text":"<p>Combine multiple scheduling functions.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def combine_scheds(pcts: Iterable[float], scheds: Iterable[callable]) -&gt; callable:\n    \"\"\"\n    Combine multiple scheduling functions.\n    \"\"\"\n    assert sum(pcts) == 1.0\n    pcts = torch.tensor([0] + listify(pcts))\n    assert torch.all(pcts &gt;= 0)\n    pcts = torch.cumsum(pcts, 0)\n\n    def _inner(pos):\n        idx = (pos &gt;= pcts).nonzero().max()\n        actual_pos = (pos - pcts[idx]) / (pcts[idx + 1] - pcts[idx])\n        return scheds[idx](actual_pos)\n\n    return _inner\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.listify","title":"<code>listify(o=None)</code>","text":"<p>Convert <code>o</code> to list. If <code>o</code> is None, return empty list.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>def listify(o = None) -&gt; list:\n    \"\"\"\n    Convert `o` to list. If `o` is None, return empty list.\n    \"\"\"\n    if o is None:\n        return []\n    if isinstance(o, list):\n        return o\n    if isinstance(o, str):\n        return [o]\n    if isinstance(o, Iterable):\n        return list(o)\n    return [o]\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.sched_cos","title":"<code>sched_cos(start, end, pos)</code>","text":"<p>A cosine schedule function.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>@annealer\ndef sched_cos(start: float, end: float, pos: float) -&gt; float:\n    \"\"\"\n    A cosine schedule function.\n    \"\"\"\n    return start + (1 + math.cos(math.pi * (1 - pos))) * (end - start) / 2\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.sched_exp","title":"<code>sched_exp(start, end, pos)</code>","text":"<p>Exponential schedule function.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>@annealer\ndef sched_exp(start: float, end: float, pos: float) -&gt; float:\n    \"\"\"\n    Exponential schedule function.\n    \"\"\"\n    return start * (end / start) ** pos\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.sched_lin","title":"<code>sched_lin(start, end, pos)</code>","text":"<p>A linear schedule function.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>@annealer\ndef sched_lin(start: float, end: float, pos: float) -&gt; float:\n    \"\"\"\n    A linear schedule function.\n    \"\"\"\n    return start + pos * (end - start)\n</code></pre>"},{"location":"pytorch/API%20reference/Callbacks/#callbacks.sched_no","title":"<code>sched_no(start, end, pos)</code>","text":"<p>Disabled scheduling.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/callbacks.py</code> <pre><code>@annealer\ndef sched_no(start: float, end: float, pos: float) -&gt; float:\n    \"\"\"\n    Disabled scheduling.\n    \"\"\"\n    return start\n</code></pre>"},{"location":"pytorch/API%20reference/Custom%20Datasets/","title":"Custom Datasets","text":"<p>This module contains custom dataset classes for handling image and tabular data from OpenML in PyTorch. To add support for new data types, new classes can be added to this module.</p>"},{"location":"pytorch/API%20reference/Custom%20Datasets/#custom_datasets.OpenMLImageDataset","title":"<code>OpenMLImageDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Class representing an image dataset from OpenML for use in PyTorch.</p> <p>Methods:</p> <pre><code>__init__(self, X, y, image_size, image_dir, transform_x=None, transform_y=None)\n    Initializes the dataset with given data, image size, directory, and optional transformations.\n\n__getitem__(self, idx)\n    Retrieves an image and its corresponding label (if available) from the dataset at the specified index. Applies transformations if provided.\n\n__len__(self)\n    Returns the total number of images in the dataset.\n</code></pre> Source code in <code>temp_dir/pytorch/openml_pytorch/custom_datasets.py</code> <pre><code>class OpenMLImageDataset(Dataset):\n    \"\"\"\n        Class representing an image dataset from OpenML for use in PyTorch.\n\n        Methods:\n\n            __init__(self, X, y, image_size, image_dir, transform_x=None, transform_y=None)\n                Initializes the dataset with given data, image size, directory, and optional transformations.\n\n            __getitem__(self, idx)\n                Retrieves an image and its corresponding label (if available) from the dataset at the specified index. Applies transformations if provided.\n\n            __len__(self)\n                Returns the total number of images in the dataset.\n    \"\"\"\n    def __init__(self, X, y, image_size, image_dir, transform_x = None, transform_y = None):\n        self.X = X\n        self.y = y\n        self.image_size = image_size\n        self.image_dir = image_dir\n        self.transform_x = transform_x\n        self.transform_y = transform_y\n\n    def __getitem__(self, idx):\n        img_name = str(os.path.join(self.image_dir, self.X.iloc[idx, 0]))\n        image = read_image(img_name)\n        image = image.float()\n        image = T.Resize((self.image_size, self.image_size))(image)\n        if self.transform_x is not None:\n            image = self.transform_x(image)\n        if self.y is not None:\n            label = self.y.iloc[idx]\n            if label is not None:\n                if self.transform_y is not None:\n                    label = self.transform_y(label)\n                return image, label\n        else:\n            return image\n\n    def __len__(self):\n        return len(self.X)\n</code></pre>"},{"location":"pytorch/API%20reference/Custom%20Datasets/#custom_datasets.OpenMLTabularDataset","title":"<code>OpenMLTabularDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>OpenMLTabularDataset</p> <p>A custom dataset class to handle tabular data from OpenML (or any similar tabular dataset). It encodes categorical features and the target column using LabelEncoder from sklearn.</p> <p>Methods:     init(X, y) : Initializes the dataset with the data and the target column.                      Encodes the categorical features and target if provided.</p> <pre><code>__getitem__(idx): Retrieves the input data and target value at the specified index.\n                  Converts the data to tensors and returns them.\n\n__len__(): Returns the length of the dataset.\n</code></pre> Source code in <code>temp_dir/pytorch/openml_pytorch/custom_datasets.py</code> <pre><code>class OpenMLTabularDataset(Dataset):\n    \"\"\"\n    OpenMLTabularDataset\n\n    A custom dataset class to handle tabular data from OpenML (or any similar tabular dataset).\n    It encodes categorical features and the target column using LabelEncoder from sklearn.\n\n    Methods:\n        __init__(X, y) : Initializes the dataset with the data and the target column.\n                         Encodes the categorical features and target if provided.\n\n        __getitem__(idx): Retrieves the input data and target value at the specified index.\n                          Converts the data to tensors and returns them.\n\n        __len__(): Returns the length of the dataset.\n    \"\"\"\n    def __init__(self, X, y):\n        self.data = X\n        # self.target_col_name = target_col\n        for col in self.data.select_dtypes(include=['object', 'category']):\n            # convert to float\n            self.data[col] = self.data[col].astype('category').cat.codes\n        self.label_mapping = None\n\n        # self.label_mapping = preprocessing.LabelEncoder()\n        # try:\n        #     self.data = self.data.apply(self.label_mapping.fit_transform)\n        # except ValueError:\n        #     pass\n\n        # try:\n        #     self.y = self.label_mapping.fit_transform(y)\n        # except ValueError:\n        #     self.y = None\n        self.y = y\n\n    def __getitem__(self, idx):\n        # x is the input data, y is the target value from the target column\n        x = self.data.iloc[idx, :]\n        x = torch.tensor(x.values.astype('float32'))\n        if self.y is not None:\n            y = self.y[idx]\n            y = torch.tensor(y)\n            return x, y\n        else:\n            return x\n\n\n    def __len__(self):\n        return len(self.data)\n</code></pre>"},{"location":"pytorch/API%20reference/Metrics/","title":"Metrics","text":"<p>This module provides utility functions for evaluating model performance and activation functions. It includes functions to compute the accuracy, top-k accuracy of model predictions, and the sigmoid function.</p>"},{"location":"pytorch/API%20reference/Metrics/#metrics.accuracy","title":"<code>accuracy(out, yb)</code>","text":"<p>Computes the accuracy of model predictions.</p> <p>Parameters: out (Tensor): The output tensor from the model, containing predicted class scores. yb (Tensor): The ground truth labels tensor.</p> <p>Returns: Tensor: The mean accuracy of the predictions, computed as a float tensor.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/metrics.py</code> <pre><code>def accuracy(out, yb):\n    \"\"\"\n\n    Computes the accuracy of model predictions.\n\n    Parameters:\n    out (Tensor): The output tensor from the model, containing predicted class scores.\n    yb (Tensor): The ground truth labels tensor.\n\n    Returns:\n    Tensor: The mean accuracy of the predictions, computed as a float tensor.\n    \"\"\"\n    return (torch.argmax(out, dim=1) == yb.long()).float().mean()\n</code></pre>"},{"location":"pytorch/API%20reference/Metrics/#metrics.accuracy_topk","title":"<code>accuracy_topk(out, yb, k=5)</code>","text":"<p>Computes the top-k accuracy of the given model outputs.</p> <p>Args:     out (torch.Tensor): The output predictions of the model, of shape (batch_size, num_classes).     yb (torch.Tensor): The ground truth labels, of shape (batch_size,).     k (int, optional): The number of top predictions to consider. Default is 5.</p> <p>Returns:     float: The top-k accuracy as a float value.</p> <p>The function calculates how often the true label is among the top-k predicted labels.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/metrics.py</code> <pre><code>def accuracy_topk(out, yb, k=5):\n    \"\"\"\n\n    Computes the top-k accuracy of the given model outputs.\n\n    Args:\n        out (torch.Tensor): The output predictions of the model, of shape (batch_size, num_classes).\n        yb (torch.Tensor): The ground truth labels, of shape (batch_size,).\n        k (int, optional): The number of top predictions to consider. Default is 5.\n\n    Returns:\n        float: The top-k accuracy as a float value.\n\n    The function calculates how often the true label is among the top-k predicted labels.\n    \"\"\"\n    return (torch.topk(out, k, dim=1)[1] == yb.long().unsqueeze(1)).float().mean()\n</code></pre>"},{"location":"pytorch/API%20reference/Metrics/#metrics.sigmoid","title":"<code>sigmoid(x)</code>","text":"<p>Computes the sigmoid function</p> <p>The sigmoid function is defined as 1 / (1 + exp(-x)). This function is used to map any real-valued number into the range (0, 1). It is widely used in machine learning, especially in logistic regression and neural networks.</p> <p>Args:     x (numpy.ndarray or float): The input value or array over which the     sigmoid function should be applied.</p> <p>Returns:     numpy.ndarray or float: The sigmoid of the input value or array.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/metrics.py</code> <pre><code>def sigmoid(x):\n    \"\"\"\n    Computes the sigmoid function\n\n    The sigmoid function is defined as 1 / (1 + exp(-x)). This function is used\n    to map any real-valued number into the range (0, 1). It is widely used in\n    machine learning, especially in logistic regression and neural networks.\n\n    Args:\n        x (numpy.ndarray or float): The input value or array over which the\n        sigmoid function should be applied.\n\n    Returns:\n        numpy.ndarray or float: The sigmoid of the input value or array.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/","title":"OpenML Connection","text":"<p>This module defines the Pytorch extension for OpenML-python.</p>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension","title":"<code>PytorchExtension</code>","text":"<p>               Bases: <code>Extension</code></p> <p>Connect Pytorch to OpenML-Python.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>class PytorchExtension(Extension):\n    \"\"\"Connect Pytorch to OpenML-Python.\"\"\"\n\n    ################################################################################################\n    # General setup\n\n    @classmethod\n    def can_handle_flow(cls, flow: 'OpenMLFlow') -&gt; bool:\n        \"\"\"Check whether a given describes a Pytorch estimator.\n\n        This is done by parsing the ``external_version`` field.\n\n        Parameters\n        ----------\n        flow : OpenMLFlow\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return cls._is_pytorch_flow(flow)\n\n    @classmethod\n    def can_handle_model(cls, model: Any) -&gt; bool:\n        \"\"\"Check whether a model is an instance of ``torch.nn.Module``.\n\n        Parameters\n        ----------\n        model : Any\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        from torch.nn import Module\n        return isinstance(model, Module)\n\n    ################################################################################################\n    # Method for dataloader \n\n\n\n    ################################################################################################\n    # Methods for flow serialization and de-serialization\n\n    def flow_to_model(self, flow: 'OpenMLFlow', initialize_with_defaults: bool = False) -&gt; Any:\n        \"\"\"Initializes a Pytorch model based on a flow.\n\n        Parameters\n        ----------\n        flow : mixed\n            the object to deserialize (can be flow object, or any serialized\n            parameter value that is accepted by)\n\n        initialize_with_defaults : bool, optional (default=False)\n            If this flag is set, the hyperparameter values of flows will be\n            ignored and a flow with its defaults is returned.\n\n        Returns\n        -------\n        mixed\n        \"\"\"\n        return self._deserialize_pytorch(flow, initialize_with_defaults=initialize_with_defaults)\n\n    def _deserialize_pytorch(\n        self,\n        o: Any,\n        components: Optional[Dict] = None,\n        initialize_with_defaults: bool = False,\n        recursion_depth: int = 0,\n    ) -&gt; Any:\n        \"\"\"Recursive function to deserialize a Pytorch flow.\n\n        This function delegates all work to the respective functions to deserialize special data\n        structures etc.\n\n        Parameters\n        ----------\n        o : mixed\n            the object to deserialize (can be flow object, or any serialized\n            parameter value that is accepted by)\n\n        components : dict\n\n\n        initialize_with_defaults : bool, optional (default=False)\n            If this flag is set, the hyperparameter values of flows will be\n            ignored and a flow with its defaults is returned.\n\n        recursion_depth : int\n            The depth at which this flow is called, mostly for debugging\n            purposes\n\n        Returns\n        -------\n        mixed\n        \"\"\"\n\n        logging.info('-%s flow_to_pytorch START o=%s, components=%s, '\n                     'init_defaults=%s' % ('-' * recursion_depth, o, components,\n                                           initialize_with_defaults))\n        depth_pp = recursion_depth + 1  # shortcut var, depth plus plus\n\n        # First, we need to check whether the presented object is a json string.\n        # JSON strings are used to encoder parameter values. By passing around\n        # json strings for parameters, we make sure that we can flow_to_pytorch\n        # the parameter values to the correct type.\n\n        if isinstance(o, str):\n            try:\n                o = json.loads(o)\n            except JSONDecodeError:\n                pass\n\n        if isinstance(o, dict):\n            # Check if the dict encodes a 'special' object, which could not\n            # easily converted into a string, but rather the information to\n            # re-create the object were stored in a dictionary.\n            if 'oml-python:serialized_object' in o:\n                serialized_type = o['oml-python:serialized_object']\n                value = o['value']\n                if serialized_type == 'type':\n                    rval = self._deserialize_type(value)\n                elif serialized_type == 'function':\n                    rval = self._deserialize_function(value)\n                elif serialized_type == 'methoddescriptor':\n                    rval = self._deserialize_methoddescriptor(value)\n                elif serialized_type == 'component_reference':\n                    assert components is not None  # Necessary for mypy\n                    value = self._deserialize_pytorch(value, recursion_depth=depth_pp)\n                    step_name = value['step_name']\n                    key = value['key']\n                    if key not in components:\n                        key = str(key)\n                    component = self._deserialize_pytorch(\n                        components[key],\n                        initialize_with_defaults=initialize_with_defaults,\n                        recursion_depth=depth_pp\n                    )\n                    # The component is now added to where it should be used\n                    # later. It should not be passed to the constructor of the\n                    # main flow object.\n                    del components[key]\n                    if step_name is None:\n                        rval = component\n                    elif 'argument_1' not in value:\n                        rval = (step_name, component)\n                    else:\n                        rval = (step_name, component, value['argument_1'])\n                else:\n                    raise ValueError('Cannot flow_to_pytorch %s' % serialized_type)\n\n            else:\n                rval = OrderedDict(\n                    (\n                        self._deserialize_pytorch(\n                            o=key,\n                            components=components,\n                            initialize_with_defaults=initialize_with_defaults,\n                            recursion_depth=depth_pp,\n                        ),\n                        self._deserialize_pytorch(\n                            o=value,\n                            components=components,\n                            initialize_with_defaults=initialize_with_defaults,\n                            recursion_depth=depth_pp,\n                        )\n                    )\n                    for key, value in sorted(o.items())\n                )\n        elif isinstance(o, (list, tuple)):\n            rval = [\n                self._deserialize_pytorch(\n                    o=element,\n                    components=components,\n                    initialize_with_defaults=initialize_with_defaults,\n                    recursion_depth=depth_pp,\n                )\n                for element in o\n            ]\n            if isinstance(o, tuple):\n                rval = tuple(rval)\n        elif isinstance(o, (bool, int, float, str)) or o is None:\n            rval = o\n        elif isinstance(o, OpenMLFlow):\n            if not self._is_pytorch_flow(o):\n                raise ValueError('Only pytorch flows can be reinstantiated')\n            rval = self._deserialize_model(\n                flow=o,\n                keep_defaults=initialize_with_defaults,\n                recursion_depth=recursion_depth,\n            )\n        else:\n            raise TypeError(o)\n        logging.info('-%s flow_to_pytorch END   o=%s, rval=%s'\n                     % ('-' * recursion_depth, o, rval))\n        return rval\n\n    def model_to_flow(self, model: Any, custom_name: Optional[str] = None) -&gt; 'OpenMLFlow':\n        \"\"\"Transform a Pytorch model to a flow for uploading it to OpenML.\n\n        Parameters\n        ----------\n        model : Any\n\n        Returns\n        -------\n        OpenMLFlow\n        \"\"\"\n        # Necessary to make pypy not complain about all the different possible return types\n        return self._serialize_pytorch(model, custom_name)\n\n    def _serialize_pytorch(self, o: Any, parent_model: Optional[Any] = None, custom_name: Optional[str] = None) -&gt; Any:\n        rval = None  # type: Any\n        if self.is_estimator(o):\n            # is the main model or a submodel\n            rval = self._serialize_model(o, custom_name)\n        elif isinstance(o, (list, tuple)):\n            rval = [self._serialize_pytorch(element, parent_model) for element in o]\n            if isinstance(o, tuple):\n                rval = tuple(rval)\n        elif isinstance(o, SIMPLE_TYPES) or o is None:\n            if isinstance(o, tuple(SIMPLE_NUMPY_TYPES)):\n                o = o.item()\n            # base parameter values\n            rval = o\n        elif isinstance(o, dict):\n            if not isinstance(o, OrderedDict):\n                o = OrderedDict([(key, value) for key, value in sorted(o.items())])\n\n            rval = OrderedDict()\n            for key, value in o.items():\n                if not isinstance(key, str):\n                    raise TypeError('Can only use string as keys, you passed '\n                                    'type %s for value %s.' %\n                                    (type(key), str(key)))\n                key = self._serialize_pytorch(key, parent_model)\n                value = self._serialize_pytorch(value, parent_model)\n                rval[key] = value\n            rval = rval\n        elif isinstance(o, type):\n            rval = self._serialize_type(o)\n        # This only works for user-defined functions (and not even partial).\n        # I think this is exactly what we want here as there shouldn't be any\n        # built-in or functool.partials in a pipeline\n        elif inspect.isfunction(o):\n            rval = self._serialize_function(o)\n        elif inspect.ismethoddescriptor(o):\n            rval = self._serialize_methoddescriptor(o)\n        else:\n            raise TypeError(o, type(o))\n        return rval\n\n    def get_version_information(self) -&gt; List[str]:\n        \"\"\"List versions of libraries required by the flow.\n\n        Libraries listed are ``Python``, ``pytorch``, ``numpy`` and ``scipy``.\n\n        Returns\n        -------\n        List\n        \"\"\"\n\n        # This can possibly be done by a package such as pyxb, but I could not get\n        # it to work properly.\n        import scipy\n        import numpy\n\n        major, minor, micro, _, _ = sys.version_info\n        python_version = 'Python_{}.'.format(\n            \".\".join([str(major), str(minor), str(micro)]))\n        pytorch_version = 'Torch_{}.'.format(torch.__version__)\n        numpy_version = 'NumPy_{}.'.format(numpy.__version__)\n        scipy_version = 'SciPy_{}.'.format(scipy.__version__)\n        pytorch_version_formatted = pytorch_version.replace('+','_')\n        return [python_version, pytorch_version_formatted, numpy_version, scipy_version]\n\n    def create_setup_string(self, model: Any) -&gt; str:\n        \"\"\"Create a string which can be used to reinstantiate the given model.\n\n        Parameters\n        ----------\n        model : Any\n\n        Returns\n        -------\n        str\n        \"\"\"\n        run_environment = \" \".join(self.get_version_information())\n        return run_environment + \" \" + str(model)\n\n    @classmethod\n    def _is_pytorch_flow(cls, flow: OpenMLFlow) -&gt; bool:\n        return (\n            flow.external_version.startswith('torch==')\n            or ',torch==' in flow.external_version\n        )\n\n    def _serialize_model(self, model: Any, custom_name: Optional[str] = None) -&gt; OpenMLFlow:\n        \"\"\"Create an OpenMLFlow.\n\n        Calls `pytorch_to_flow` recursively to properly serialize the\n        parameters to strings and the components (other models) to OpenMLFlows.\n\n        Parameters\n        ----------\n        model : pytorch estimator\n\n        Returns\n        -------\n        OpenMLFlow\n\n        \"\"\"\n\n        # Get all necessary information about the model objects itself\n        parameters, parameters_meta_info, subcomponents, subcomponents_explicit = \\\n            self._extract_information_from_model(model)\n\n        # Check that a component does not occur multiple times in a flow as this\n        # is not supported by OpenML\n        self._check_multiple_occurence_of_component_in_flow(model, subcomponents)\n\n        import zlib\n        import os\n\n        # class_name = model.__module__ + \".\" + model.__class__.__name__\n        class_name = 'torch.nn' + \".\" + model.__class__.__name__\n        class_name += '.'\n        class_name += format(zlib.crc32(bytearray(os.urandom(32))), 'x')\n        class_name += format(zlib.crc32(bytearray(os.urandom(32))), 'x')\n\n        name = class_name\n\n        # Get the external versions of all sub-components\n        external_version = self._get_external_version_string(model, subcomponents)\n\n        dependencies = '\\n'.join([\n            self._format_external_version(\n                'torch',\n                torch.__version__,\n            ),\n            'numpy&gt;=1.6.1',\n            'scipy&gt;=0.9',\n        ])\n\n        torch_version = self._format_external_version('torch', torch.__version__)\n        torch_version_formatted = torch_version.replace('==', '_')\n        torch_version_formatted = torch_version_formatted.replace('+', '_')\n\n        flow = OpenMLFlow(name=name,\n                          class_name=class_name,\n                          description='Automatically created pytorch flow.',\n                          model=model,\n                          components=subcomponents,\n                          parameters=parameters,\n                          parameters_meta_info=parameters_meta_info,\n                          external_version=external_version,\n                          tags=['openml-python', 'pytorch',\n                                'python', torch_version_formatted],\n                          language='English',\n                          dependencies=dependencies, \n                          custom_name=custom_name)\n\n        return flow\n\n    def _get_external_version_string(\n        self,\n        model: Any,\n        sub_components: Dict[str, OpenMLFlow],\n    ) -&gt; str:\n        # Create external version string for a flow, given the model and the\n        # already parsed dictionary of sub_components. Retrieves the external\n        # version of all subcomponents, which themselves already contain all\n        # requirements for their subcomponents. The external version string is a\n        # sorted concatenation of all modules which are present in this run.\n        model_package_name = model.__module__.split('.')[0]\n        module = importlib.import_module(model_package_name)\n        model_package_version_number = 'module.__version__'  # type: ignore\n        external_version = self._format_external_version(\n            model_package_name, model_package_version_number,\n        )\n        openml_version = self._format_external_version('openml', openml.__version__)\n        torch_version = self._format_external_version('torch', torch.__version__)\n        external_versions = set()\n        external_versions.add(external_version)\n        external_versions.add(openml_version)\n        external_versions.add(torch_version)\n        for visitee in sub_components.values():\n            for external_version in visitee.external_version.split(','):\n                external_versions.add(external_version)\n        return ','.join(list(sorted(external_versions)))\n\n    def _check_multiple_occurence_of_component_in_flow(\n        self,\n        model: Any,\n        sub_components: Dict[str, OpenMLFlow],\n    ) -&gt; None:\n        to_visit_stack = []  # type: List[OpenMLFlow]\n        to_visit_stack.extend(sub_components.values())\n        known_sub_components = set()  # type: Set[str]\n        while len(to_visit_stack) &gt; 0:\n            visitee = to_visit_stack.pop()\n            if visitee.name in known_sub_components:\n                raise ValueError('Found a second occurence of component %s when '\n                                 'trying to serialize %s.' % (visitee.name, model))\n            else:\n                known_sub_components.add(visitee.name)\n                to_visit_stack.extend(visitee.components.values())\n\n    def _is_container_module(self, module: torch.nn.Module) -&gt; bool:\n        if isinstance(module,\n                      (torch.nn.Sequential,\n                       torch.nn.ModuleDict,\n                       torch.nn.ModuleList)):\n            return True\n        if module in (torch.nn.modules.container.Sequential,\n                      torch.nn.modules.container.ModuleDict,\n                      torch.nn.modules.container.ModuleList):\n            return True\n        return False\n\n    def _get_module_hyperparameters(self, module: torch.nn.Module,\n                                    parameters: Dict[str, torch.nn.Parameter]) -&gt; Dict[str, Any]:\n        # Extract the signature of the module constructor\n        main_signature = inspect.signature(module.__init__)\n        params = dict()  # type: Dict[str, Any]\n\n        check_bases = False  # type: bool\n        for param_name, param in main_signature.parameters.items():\n            # Skip hyper-parameters which are actually parameters.\n            if param_name in parameters.keys():\n                continue\n\n            # Skip *args and **kwargs, and check the base classes instead.\n            if param.kind in (inspect.Parameter.VAR_POSITIONAL,\n                              inspect.Parameter.VAR_KEYWORD):\n                check_bases = True\n                continue\n\n            # Extract the hyperparameter from the module.\n            if hasattr(module, param_name):\n                params[param_name] = getattr(module, param_name)\n\n        if check_bases:\n            for base in module.__class__.__bases__:\n                # Extract the signature  of the base constructor\n                base_signature = inspect.signature(base.__init__)\n\n                for param_name, param in base_signature.parameters.items():\n                    # Skip hyper-parameters which are actually parameters.\n                    if param_name in parameters.keys():\n                        continue\n\n                    # Skip *args and **kwargs since they are not relevant.\n                    if param.kind in (inspect.Parameter.VAR_POSITIONAL,\n                                      inspect.Parameter.VAR_KEYWORD):\n                        continue\n\n                    # Extract the hyperparameter from the module.\n                    if hasattr(module, param_name):\n                        params[param_name] = getattr(module, param_name)\n\n        from .layers import Functional\n        if isinstance(module, Functional):\n            params['args'] = getattr(module, 'args')\n            params['kwargs'] = getattr(module, 'kwargs')\n\n        return params\n\n    def _get_module_descriptors(self, model: torch.nn.Module, deep=True) -&gt; Dict[str, Any]:\n        # The named children (modules) of the given module.\n        named_children = list((k, v) for (k, v) in model.named_children())\n        # The parameters of the given module and its submodules.\n        model_parameters = dict((k, v) for (k, v) in model.named_parameters())\n\n        parameters = dict()  # type: Dict[str, Any]\n\n        if not self._is_container_module(model):\n            # For non-containers, we simply extract the hyperparameters.\n            parameters = self._get_module_hyperparameters(model, model_parameters)\n        else:\n            # Otherwise we serialize their children as lists of pairs in order\n            # to maintain the order of the sub modules.\n            parameters['children'] = named_children\n\n        # If a deep description is required, append the children to the dictionary of\n        # returned parameters.\n        if deep:\n            named_children_dict = dict(named_children)\n            parameters = {**parameters, **named_children_dict}\n\n        return parameters\n\n    def _extract_information_from_model(\n        self,\n        model: Any,\n    ) -&gt; Tuple[\n        'OrderedDict[str, Optional[str]]',\n        'OrderedDict[str, Optional[Dict]]',\n        'OrderedDict[str, OpenMLFlow]',\n        Set,\n    ]:\n        # This function contains four \"global\" states and is quite long and\n        # complicated. If it gets to complicated to ensure it's correctness,\n        # it would be best to make it a class with the four \"global\" states being\n        # the class attributes and the if/elif/else in the for-loop calls to\n        # separate class methods\n\n        # stores all entities that should become subcomponents\n        sub_components = OrderedDict()  # type: OrderedDict[str, OpenMLFlow]\n        # stores the keys of all subcomponents that should become\n        sub_components_explicit = set()\n        parameters = OrderedDict()  # type: OrderedDict[str, Optional[str]]\n        parameters_meta_info = OrderedDict()  # type: OrderedDict[str, Optional[Dict]]\n\n        model_parameters = self._get_module_descriptors(model, deep=True)\n        for k, v in sorted(model_parameters.items(), key=lambda t: t[0]):\n            rval = self._serialize_pytorch(v, model)\n\n            def flatten_all(list_):\n                \"\"\" Flattens arbitrary depth lists of lists (e.g. [[1,2],[3,[1]]] -&gt; [1,2,3,1]). \"\"\"\n                for el in list_:\n                    if isinstance(el, (list, tuple)):\n                        yield from flatten_all(el)\n                    else:\n                        yield el\n\n            is_non_empty_list_of_lists_with_same_type = (\n                isinstance(rval, (list, tuple))\n                and len(rval) &gt; 0\n                and isinstance(rval[0], (list, tuple))\n                and all([isinstance(rval_i, type(rval[0])) for rval_i in rval])\n            )\n\n            # Check that all list elements are of simple types.\n            nested_list_of_simple_types = (\n                is_non_empty_list_of_lists_with_same_type\n                and all([isinstance(el, SIMPLE_TYPES) for el in flatten_all(rval)])\n            )\n\n            if is_non_empty_list_of_lists_with_same_type and not nested_list_of_simple_types:\n                # If a list of lists is identified that include 'non-simple' types (e.g. objects),\n                # we assume they are steps in a pipeline, feature union, or base classifiers in\n                # a voting classifier.\n                parameter_value = list()  # type: List\n                reserved_keywords = set(self._get_module_descriptors(model, deep=False).keys())\n\n                for sub_component_tuple in rval:\n                    identifier = sub_component_tuple[0]\n                    sub_component = sub_component_tuple[1]\n                    sub_component_type = type(sub_component_tuple)\n                    if not 2 &lt;= len(sub_component_tuple) &lt;= 3:\n                        msg = 'Length of tuple does not match assumptions'\n                        raise ValueError(msg)\n                    if not isinstance(sub_component, (OpenMLFlow, type(None))):\n                        msg = 'Second item of tuple does not match assumptions. ' \\\n                              'Expected OpenMLFlow, got %s' % type(sub_component)\n                        raise TypeError(msg)\n\n                    if identifier in reserved_keywords:\n                        parent_model = \"{}.{}\".format(model.__module__,\n                                                      model.__class__.__name__)\n                        msg = 'Found element shadowing official ' \\\n                              'parameter for %s: %s' % (parent_model,\n                                                        identifier)\n                        raise PyOpenMLError(msg)\n\n                    if sub_component is None:\n                        # In a FeatureUnion it is legal to have a None step\n\n                        pv = [identifier, None]\n                        if sub_component_type is tuple:\n                            parameter_value.append(tuple(pv))\n                        else:\n                            parameter_value.append(pv)\n\n                    else:\n                        # Add the component to the list of components, add a\n                        # component reference as a placeholder to the list of\n                        # parameters, which will be replaced by the real component\n                        # when deserializing the parameter\n                        sub_components_explicit.add(identifier)\n                        sub_components[identifier] = sub_component\n                        component_reference = OrderedDict()  # type: Dict[str, Union[str, Dict]]\n                        component_reference['oml-python:serialized_object'] = 'component_reference'\n                        cr_value = OrderedDict()  # type: Dict[str, Any]\n                        cr_value['key'] = identifier\n                        cr_value['step_name'] = identifier\n                        if len(sub_component_tuple) == 3:\n                            cr_value['argument_1'] = sub_component_tuple[2]\n                        component_reference['value'] = cr_value\n                        parameter_value.append(component_reference)\n\n                # Here (and in the elif and else branch below) are the only\n                # places where we encode a value as json to make sure that all\n                # parameter values still have the same type after\n                # deserialization\n\n                if isinstance(rval, tuple):\n                    parameter_json = json.dumps(tuple(parameter_value))\n                else:\n                    parameter_json = json.dumps(parameter_value)\n                parameters[k] = parameter_json\n\n            elif isinstance(rval, OpenMLFlow):\n\n                # A subcomponent, for example the layers in a sequential model\n                sub_components[k] = rval\n                sub_components_explicit.add(k)\n                component_reference = OrderedDict()\n                component_reference['oml-python:serialized_object'] = 'component_reference'\n                cr_value = OrderedDict()\n                cr_value['key'] = k\n                cr_value['step_name'] = None\n                component_reference['value'] = cr_value\n                cr = self._serialize_pytorch(component_reference, model)\n                parameters[k] = json.dumps(cr)\n\n            else:\n                # a regular hyperparameter\n                rval = json.dumps(rval)\n                parameters[k] = rval\n\n            parameters_meta_info[k] = OrderedDict((('description', None), ('data_type', None)))\n\n        return parameters, parameters_meta_info, sub_components, sub_components_explicit\n\n    def _get_fn_arguments_with_defaults(self, fn_name: Callable) -&gt; Tuple[Dict, Set]:\n        \"\"\"\n        Returns:\n            i) a dict with all parameter names that have a default value, and\n            ii) a set with all parameter names that do not have a default\n\n        Parameters\n        ----------\n        fn_name : callable\n            The function of which we want to obtain the defaults\n\n        Returns\n        -------\n        params_with_defaults: dict\n            a dict mapping parameter name to the default value\n        params_without_defaults: set\n            a set with all parameters that do not have a default value\n        \"\"\"\n        # parameters with defaults are optional, all others are required.\n        signature = inspect.getfullargspec(fn_name)\n        if signature.defaults:\n            optional_params = dict(zip(reversed(signature.args), reversed(signature.defaults)))\n        else:\n            optional_params = dict()\n        required_params = {arg for arg in signature.args if arg not in optional_params}\n        return optional_params, required_params\n\n    def _deserialize_model(\n        self,\n        flow: OpenMLFlow,\n        keep_defaults: bool,\n        recursion_depth: int,\n    ) -&gt; Any:\n        logging.info('-%s deserialize %s' % ('-' * recursion_depth, flow.name))\n        model_name = flow.class_name\n        self._check_dependencies(flow.dependencies)\n\n        parameters = flow.parameters\n        components = flow.components\n        parameter_dict = OrderedDict()  # type: Dict[str, Any]\n\n        # Do a shallow copy of the components dictionary so we can remove the\n        # components from this copy once we added them into the pipeline. This\n        # allows us to not consider them any more when looping over the\n        # components, but keeping the dictionary of components untouched in the\n        # original components dictionary.\n        components_ = copy.copy(components)\n\n        for name in parameters:\n            value = parameters.get(name)\n            logging.info('--%s flow_parameter=%s, value=%s' %\n                         ('-' * recursion_depth, name, value))\n            rval = self._deserialize_pytorch(\n                value,\n                components=components_,\n                initialize_with_defaults=keep_defaults,\n                recursion_depth=recursion_depth + 1,\n            )\n            parameter_dict[name] = rval\n\n        for name in components:\n            if name in parameter_dict:\n                continue\n            if name not in components_:\n                continue\n            value = components[name]\n            logging.info('--%s flow_component=%s, value=%s'\n                         % ('-' * recursion_depth, name, value))\n            rval = self._deserialize_pytorch(\n                value,\n                recursion_depth=recursion_depth + 1,\n            )\n            parameter_dict[name] = rval\n\n        # Remove the unique identifier\n        model_name = model_name.rsplit('.', 1)[0]\n\n        module_name = model_name.rsplit('.', 1)\n        model_class = getattr(importlib.import_module(module_name[0]),\n                              module_name[1])\n\n        if keep_defaults:\n            # obtain all params with a default\n            param_defaults, _ = \\\n                self._get_fn_arguments_with_defaults(model_class.__init__)\n\n            # delete the params that have a default from the dict,\n            # so they get initialized with their default value\n            # except [...]\n            for param in param_defaults:\n                # [...] the ones that also have a key in the components dict.\n                # As OpenML stores different flows for ensembles with different\n                # (base-)components, in OpenML terms, these are not considered\n                # hyperparameters but rather constants (i.e., changing them would\n                # result in a different flow)\n                if param not in components.keys() and param in parameter_dict:\n                    del parameter_dict[param]\n\n        if self._is_container_module(model_class):\n            children = parameter_dict['children']\n            children = list((str(k), v) for (k, v) in children)\n            children = OrderedDict(children)\n            return model_class(children)\n\n        from .layers import Functional\n        if model_class is Functional:\n            return model_class(function=parameter_dict['function'],\n                               *parameter_dict['args'],\n                               **parameter_dict['kwargs'])\n\n        return model_class(**parameter_dict)\n\n    def _check_dependencies(self, dependencies: str) -&gt; None:\n        if not dependencies:\n            return\n\n        dependencies_list = dependencies.split('\\n')\n        for dependency_string in dependencies_list:\n            match = DEPENDENCIES_PATTERN.match(dependency_string)\n            if not match:\n                raise ValueError('Cannot parse dependency %s' % dependency_string)\n\n            dependency_name = match.group('name')\n            operation = match.group('operation')\n            version = match.group('version')\n\n            module = importlib.import_module(dependency_name)\n            required_version = LooseVersion(version)\n            installed_version = LooseVersion(module.__version__)  # type: ignore\n\n            if operation == '==':\n                check = required_version == installed_version\n            elif operation == '&gt;':\n                check = installed_version &gt; required_version\n            elif operation == '&gt;=':\n                check = (installed_version &gt; required_version\n                         or installed_version == required_version)\n            else:\n                raise NotImplementedError(\n                    'operation \\'%s\\' is not supported' % operation)\n            if not check:\n                raise ValueError('Trying to deserialize a model with dependency '\n                                 '%s not satisfied.' % dependency_string)\n\n    def _serialize_type(self, o: Any) -&gt; 'OrderedDict[str, str]':\n        mapping = {float: 'float',\n                   np.float: 'np.float',\n                   np.float32: 'np.float32',\n                   np.float64: 'np.float64',\n                   int: 'int',\n                   np.int: 'np.int',\n                   np.int32: 'np.int32',\n                   np.int64: 'np.int64'}\n        ret = OrderedDict()  # type: 'OrderedDict[str, str]'\n        ret['oml-python:serialized_object'] = 'type'\n        ret['value'] = mapping[o]\n        return ret\n\n    def _deserialize_type(self, o: str) -&gt; Any:\n        mapping = {'float': float,\n                   'np.float': np.float,\n                   'np.float32': np.float32,\n                   'np.float64': np.float64,\n                   'int': int,\n                   'np.int': np.int,\n                   'np.int32': np.int32,\n                   'np.int64': np.int64}\n        return mapping[o]\n\n    def _serialize_function(self, o: Callable) -&gt; 'OrderedDict[str, str]':\n        name = o.__module__ + '.' + o.__name__\n        ret = OrderedDict()  # type: 'OrderedDict[str, str]'\n        ret['oml-python:serialized_object'] = 'function'\n        ret['value'] = name\n        return ret\n\n    def _deserialize_function(self, name: str) -&gt; Callable:\n        module_name = name.rsplit('.', 1)\n        function_handle = getattr(importlib.import_module(module_name[0]), module_name[1])\n        return function_handle\n\n    def _serialize_methoddescriptor(self, o: Any) -&gt; 'OrderedDict[str, str]':\n        name = o.__objclass__.__module__ \\\n            + '.' + o.__objclass__.__name__ \\\n            + '.' + o.__name__\n        ret = OrderedDict()  # type: 'OrderedDict[str, str]'\n        ret['oml-python:serialized_object'] = 'methoddescriptor'\n        ret['value'] = name\n        return ret\n\n    def _deserialize_methoddescriptor(self, name: str) -&gt; Any:\n        module_name = name.rsplit('.', 2)\n        object_handle = getattr(importlib.import_module(module_name[0]), module_name[1])\n        function_handle = getattr(object_handle, module_name[2])\n        return function_handle\n\n    def _format_external_version(\n        self,\n        model_package_name: str,\n        model_package_version_number: str,\n    ) -&gt; str:\n        return '%s==%s' % (model_package_name, model_package_version_number)\n\n    @staticmethod\n    def _get_parameter_values_recursive(param_grid: Union[Dict, List[Dict]],\n                                        parameter_name: str) -&gt; List[Any]:\n        \"\"\"\n        Returns a list of values for a given hyperparameter, encountered\n        recursively throughout the flow. (e.g., n_jobs can be defined\n        for various flows)\n\n        Parameters\n        ----------\n        param_grid: Union[Dict, List[Dict]]\n            Dict mapping from hyperparameter list to value, to a list of\n            such dicts\n\n        parameter_name: str\n            The hyperparameter that needs to be inspected\n\n        Returns\n        -------\n        List\n            A list of all values of hyperparameters with this name\n        \"\"\"\n        if isinstance(param_grid, dict):\n            result = list()\n            for param, value in param_grid.items():\n                if param.split('__')[-1] == parameter_name:\n                    result.append(value)\n            return result\n        elif isinstance(param_grid, list):\n            result = list()\n            for sub_grid in param_grid:\n                result.extend(PytorchExtension._get_parameter_values_recursive(sub_grid,\n                                                                               parameter_name))\n            return result\n        else:\n            raise ValueError('Param_grid should either be a dict or list of dicts')\n\n    ################################################################################################\n    # Methods for performing runs with extension modules\n\n    def is_estimator(self, model: Any) -&gt; bool:\n        \"\"\"Check whether the given model is a pytorch estimator.\n\n        This function is only required for backwards compatibility and will be removed in the\n        near future.\n\n        Parameters\n        ----------\n        model : Any\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        return isinstance(model, torch.nn.Module)\n\n    def seed_model(self, model: Any, seed: Optional[int] = None) -&gt; Any:\n        \"\"\"Set the random state of all the unseeded components of a model and return the seeded\n        model.\n\n        Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n        Models that are already seeded will maintain the seed. In this case,\n        only integer seeds are allowed (An exception is raised when a RandomState was used as\n        seed).\n\n        Parameters\n        ----------\n        model : pytorch model\n            The model to be seeded\n        seed : int\n            The seed to initialize the RandomState with. Unseeded subcomponents\n            will be seeded with a random number from the RandomState.\n\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return model\n\n    def _run_model_on_fold(\n        self,\n        model: Any,\n        task: 'OpenMLTask',\n        X_train: Union[np.ndarray, scipy.sparse.spmatrix, pd.DataFrame],\n        rep_no: int,\n        fold_no: int,\n        y_train: Optional[np.ndarray] = None,\n        X_test: Optional[Union[np.ndarray, scipy.sparse.spmatrix, pd.DataFrame]] = None,\n    ) -&gt; Tuple[\n        np.ndarray,\n        np.ndarray,\n        'OrderedDict[str, float]',\n        Optional[OpenMLRunTrace],\n        Optional[Any]\n    ]:\n        \"\"\"Run a model on a repeat,fold,subsample triplet of the task and return prediction\n        information.\n\n        Furthermore, it will measure run time measures in case multi-core behaviour allows this.\n        * exact user cpu time will be measured if the number of cores is set (recursive throughout\n        the model) exactly to 1\n        * wall clock time will be measured if the number of cores is set (recursive throughout the\n        model) to any given number (but not when it is set to -1)\n\n        Returns the data that is necessary to construct the OpenML Run object. Is used by\n        run_task_get_arff_content. Do not use this function unless you know what you are doing.\n\n        Parameters\n        ----------\n        model : Any\n            The UNTRAINED model to run. The model instance will be copied and not altered.\n        task : OpenMLTask\n            The task to run the model on.\n        X_train : array-like\n            Training data for the given repetition and fold.\n        rep_no : int\n            The repeat of the experiment (0-based; in case of 1 time CV, always 0)\n        fold_no : int\n            The fold nr of the experiment (0-based; in case of holdout, always 0)\n        y_train : Optional[np.ndarray] (default=None)\n            Target attributes for supervised tasks. In case of classification, these are integer\n            indices to the potential classes specified by dataset.\n        X_test : Optional, array-like (default=None)\n            Test attributes to test for generalization in supervised tasks.\n\n        Returns\n        -------\n        predictions : np.ndarray\n            Model predictions.\n        probabilities :  Optional, np.ndarray\n            Predicted probabilities (only applicable for supervised classification tasks).\n        user_defined_measures : OrderedDict[str, float]\n            User defined measures that were generated on this fold\n        trace : Optional, OpenMLRunTrace\n            Hyperparameter optimization trace (only applicable for supervised tasks with\n            hyperparameter optimization).\n        additional_information: Optional, Any\n            Additional information provided by the extension to be converted into additional files.\n        \"\"\"\n\n        try:\n            trainer:OpenMLTrainerModule = config.trainer\n            trainer.logger = config.logger\n        except AttributeError:\n            raise ValueError('Trainer not set to config. Please use openml_pytorch.config.trainer = trainer to set the trainer.')\n        return trainer.run_model_on_fold(model, task, X_train, rep_no, fold_no, y_train, X_test)\n\n\n    def compile_additional_information(\n            self,\n            task: 'OpenMLTask',\n            additional_information: List[Tuple[int, int, Any]]\n    ) -&gt; Dict[str, Tuple[str, str]]:\n        \"\"\"Compiles additional information provided by the extension during the runs into a final\n        set of files.\n\n        Parameters\n        ----------\n        task : OpenMLTask\n            The task the model was run on.\n        additional_information: List[Tuple[int, int, Any]]\n            A list of (fold, repetition, additional information) tuples obtained during training.\n\n        Returns\n        -------\n        files : Dict[str, Tuple[str, str]]\n            A dictionary of files with their file name and contents.\n        \"\"\"\n        return dict()\n\n    def obtain_parameter_values(\n        self,\n        flow: 'OpenMLFlow',\n        model: Any = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Extracts all parameter settings required for the flow from the model.\n\n        If no explicit model is provided, the parameters will be extracted from `flow.model`\n        instead.\n\n        Parameters\n        ----------\n        flow : OpenMLFlow\n            OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n        model: Any, optional (default=None)\n            The model from which to obtain the parameter values. Must match the flow signature.\n            If None, use the model specified in ``OpenMLFlow.model``.\n\n        Returns\n        -------\n        list\n            A list of dicts, where each dict has the following entries:\n            - ``oml:name`` : str: The OpenML parameter name\n            - ``oml:value`` : mixed: A representation of the parameter value\n            - ``oml:component`` : int: flow id to which the parameter belongs\n        \"\"\"\n        openml.flows.functions._check_flow_for_server_id(flow)\n\n        def get_flow_dict(_flow):\n            flow_map = {_flow.name: _flow.flow_id}\n            for subflow in _flow.components:\n                flow_map.update(get_flow_dict(_flow.components[subflow]))\n            return flow_map\n\n        def extract_parameters(_flow, _flow_dict, component_model,\n                               _main_call=False, main_id=None):\n            def is_subcomponent_specification(values):\n                # checks whether the current value can be a specification of\n                # subcomponents, as for example the value for steps parameter\n                # (in Pipeline) or transformers parameter (in\n                # ColumnTransformer). These are always lists/tuples of lists/\n                # tuples, size bigger than 2 and an OpenMLFlow item involved.\n                if not isinstance(values, (tuple, list)):\n                    return False\n                for item in values:\n                    if not isinstance(item, (tuple, list)):\n                        return False\n                    if len(item) &lt; 2:\n                        return False\n                    if not isinstance(item[1], openml.flows.OpenMLFlow):\n                        return False\n                return True\n\n            # _flow is openml flow object, _param dict maps from flow name to flow\n            # id for the main call, the param dict can be overridden (useful for\n            # unit tests / sentinels) this way, for flows without subflows we do\n            # not have to rely on _flow_dict\n            exp_parameters = set(_flow.parameters)\n            exp_components = set(_flow.components)\n            model_parameters = set([mp for mp in self._get_module_descriptors(component_model)\n                                    if '__' not in mp])\n            if len((exp_parameters | exp_components) ^ model_parameters) != 0:\n                flow_params = sorted(exp_parameters | exp_components)\n                model_params = sorted(model_parameters)\n                raise ValueError('Parameters of the model do not match the '\n                                 'parameters expected by the '\n                                 'flow:\\nexpected flow parameters: '\n                                 '%s\\nmodel parameters: %s' % (flow_params,\n                                                               model_params))\n\n            _params = []\n            for _param_name in _flow.parameters:\n                _current = OrderedDict()\n                _current['oml:name'] = _param_name\n\n                current_param_values = self.model_to_flow(\n                    self._get_module_descriptors(component_model)[_param_name])\n\n                # Try to filter out components (a.k.a. subflows) which are\n                # handled further down in the code (by recursively calling\n                # this function)!\n                if isinstance(current_param_values, openml.flows.OpenMLFlow):\n                    continue\n\n                if is_subcomponent_specification(current_param_values):\n                    # complex parameter value, with subcomponents\n                    parsed_values = list()\n                    for subcomponent in current_param_values:\n                        if len(subcomponent) &lt; 2 or len(subcomponent) &gt; 3:\n                            raise ValueError('Component reference should be '\n                                             'size {2,3}. ')\n\n                        subcomponent_identifier = subcomponent[0]\n                        subcomponent_flow = subcomponent[1]\n                        if not isinstance(subcomponent_identifier, str):\n                            raise TypeError('Subcomponent identifier should be '\n                                            'string')\n                        if not isinstance(subcomponent_flow,\n                                          openml.flows.OpenMLFlow):\n                            raise TypeError('Subcomponent flow should be string')\n\n                        current = {\n                            \"oml-python:serialized_object\": \"component_reference\",\n                            \"value\": {\n                                \"key\": subcomponent_identifier,\n                                \"step_name\": subcomponent_identifier\n                            }\n                        }\n                        if len(subcomponent) == 3:\n                            if not isinstance(subcomponent[2], list):\n                                raise TypeError('Subcomponent argument should be'\n                                                'list')\n                            current['value']['argument_1'] = subcomponent[2]\n                        parsed_values.append(current)\n                    parsed_values = json.dumps(parsed_values)\n                else:\n                    # vanilla parameter value\n                    parsed_values = json.dumps(current_param_values)\n\n                _current['oml:value'] = parsed_values\n                if _main_call:\n                    _current['oml:component'] = main_id\n                else:\n                    _current['oml:component'] = _flow_dict[_flow.name]\n                _params.append(_current)\n\n            for _identifier in _flow.components:\n                subcomponent_model = self._get_module_descriptors(component_model)[_identifier]\n                _params.extend(extract_parameters(_flow.components[_identifier],\n                                                  _flow_dict, subcomponent_model))\n            return _params\n\n        flow_dict = get_flow_dict(flow)\n        model = model if model is not None else flow.model\n        parameters = extract_parameters(flow, flow_dict, model, True, flow.flow_id)\n\n        return parameters\n\n    def _openml_param_name_to_pytorch(\n        self,\n        openml_parameter: openml.setups.OpenMLParameter,\n        flow: OpenMLFlow,\n    ) -&gt; str:\n        \"\"\"\n        Converts the name of an OpenMLParameter into the pytorch name, given a flow.\n\n        Parameters\n        ----------\n        openml_parameter: OpenMLParameter\n            The parameter under consideration\n\n        flow: OpenMLFlow\n            The flow that provides context.\n\n        Returns\n        -------\n        pytorch_parameter_name: str\n            The name the parameter will have once used in pytorch\n        \"\"\"\n        if not isinstance(openml_parameter, openml.setups.OpenMLParameter):\n            raise ValueError('openml_parameter should be an instance of OpenMLParameter')\n        if not isinstance(flow, OpenMLFlow):\n            raise ValueError('flow should be an instance of OpenMLFlow')\n\n        flow_structure = flow.get_structure('name')\n        if openml_parameter.flow_name not in flow_structure:\n            raise ValueError('Obtained OpenMLParameter and OpenMLFlow do not correspond. ')\n        name = openml_parameter.flow_name  # for PEP8\n        return '__'.join(flow_structure[name] + [openml_parameter.parameter_name])\n\n    ################################################################################################\n    # Methods for hyperparameter optimization\n\n    def instantiate_model_from_hpo_class(\n        self,\n        model: Any,\n        trace_iteration: OpenMLTraceIteration,\n    ) -&gt; Any:\n        \"\"\"Instantiate a ``base_estimator`` which can be searched over by the hyperparameter\n        optimization model (UNUSED)\n\n        Parameters\n        ----------\n        model : Any\n            A hyperparameter optimization model which defines the model to be instantiated.\n        trace_iteration : OpenMLTraceIteration\n            Describing the hyperparameter settings to instantiate.\n\n        Returns\n        -------\n        Any\n        \"\"\"\n\n        return model\n\n\n    def check_if_model_fitted(self, model: Any) -&gt; bool:\n        \"\"\"Returns True/False denoting if the model has already been fitted/trained\n        Parameters\n        ----------\n        model : Any\n        Returns\n        -------\n        bool\n        \"\"\"\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.can_handle_flow","title":"<code>can_handle_flow(flow)</code>  <code>classmethod</code>","text":"<p>Check whether a given describes a Pytorch estimator.</p> <p>This is done by parsing the <code>external_version</code> field.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>OpenMLFlow</code> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>@classmethod\ndef can_handle_flow(cls, flow: 'OpenMLFlow') -&gt; bool:\n    \"\"\"Check whether a given describes a Pytorch estimator.\n\n    This is done by parsing the ``external_version`` field.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return cls._is_pytorch_flow(flow)\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.can_handle_model","title":"<code>can_handle_model(model)</code>  <code>classmethod</code>","text":"<p>Check whether a model is an instance of <code>torch.nn.Module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>@classmethod\ndef can_handle_model(cls, model: Any) -&gt; bool:\n    \"\"\"Check whether a model is an instance of ``torch.nn.Module``.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    from torch.nn import Module\n    return isinstance(model, Module)\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.check_if_model_fitted","title":"<code>check_if_model_fitted(model)</code>","text":"<p>Returns True/False denoting if the model has already been fitted/trained</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def check_if_model_fitted(self, model: Any) -&gt; bool:\n    \"\"\"Returns True/False denoting if the model has already been fitted/trained\n    Parameters\n    ----------\n    model : Any\n    Returns\n    -------\n    bool\n    \"\"\"\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.compile_additional_information","title":"<code>compile_additional_information(task, additional_information)</code>","text":"<p>Compiles additional information provided by the extension during the runs into a final set of files.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>OpenMLTask</code> <p>The task the model was run on.</p> required <code>additional_information</code> <code>List[Tuple[int, int, Any]]</code> <p>A list of (fold, repetition, additional information) tuples obtained during training.</p> required <p>Returns:</p> Name Type Description <code>files</code> <code>Dict[str, Tuple[str, str]]</code> <p>A dictionary of files with their file name and contents.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def compile_additional_information(\n        self,\n        task: 'OpenMLTask',\n        additional_information: List[Tuple[int, int, Any]]\n) -&gt; Dict[str, Tuple[str, str]]:\n    \"\"\"Compiles additional information provided by the extension during the runs into a final\n    set of files.\n\n    Parameters\n    ----------\n    task : OpenMLTask\n        The task the model was run on.\n    additional_information: List[Tuple[int, int, Any]]\n        A list of (fold, repetition, additional information) tuples obtained during training.\n\n    Returns\n    -------\n    files : Dict[str, Tuple[str, str]]\n        A dictionary of files with their file name and contents.\n    \"\"\"\n    return dict()\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.create_setup_string","title":"<code>create_setup_string(model)</code>","text":"<p>Create a string which can be used to reinstantiate the given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> required <p>Returns:</p> Type Description <code>str</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def create_setup_string(self, model: Any) -&gt; str:\n    \"\"\"Create a string which can be used to reinstantiate the given model.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    str\n    \"\"\"\n    run_environment = \" \".join(self.get_version_information())\n    return run_environment + \" \" + str(model)\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.flow_to_model","title":"<code>flow_to_model(flow, initialize_with_defaults=False)</code>","text":"<p>Initializes a Pytorch model based on a flow.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>mixed</code> <p>the object to deserialize (can be flow object, or any serialized parameter value that is accepted by)</p> required <code>initialize_with_defaults</code> <code>(bool, optional(default=False))</code> <p>If this flag is set, the hyperparameter values of flows will be ignored and a flow with its defaults is returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>mixed</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def flow_to_model(self, flow: 'OpenMLFlow', initialize_with_defaults: bool = False) -&gt; Any:\n    \"\"\"Initializes a Pytorch model based on a flow.\n\n    Parameters\n    ----------\n    flow : mixed\n        the object to deserialize (can be flow object, or any serialized\n        parameter value that is accepted by)\n\n    initialize_with_defaults : bool, optional (default=False)\n        If this flag is set, the hyperparameter values of flows will be\n        ignored and a flow with its defaults is returned.\n\n    Returns\n    -------\n    mixed\n    \"\"\"\n    return self._deserialize_pytorch(flow, initialize_with_defaults=initialize_with_defaults)\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.get_version_information","title":"<code>get_version_information()</code>","text":"<p>List versions of libraries required by the flow.</p> <p>Libraries listed are <code>Python</code>, <code>pytorch</code>, <code>numpy</code> and <code>scipy</code>.</p> <p>Returns:</p> Type Description <code>List</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def get_version_information(self) -&gt; List[str]:\n    \"\"\"List versions of libraries required by the flow.\n\n    Libraries listed are ``Python``, ``pytorch``, ``numpy`` and ``scipy``.\n\n    Returns\n    -------\n    List\n    \"\"\"\n\n    # This can possibly be done by a package such as pyxb, but I could not get\n    # it to work properly.\n    import scipy\n    import numpy\n\n    major, minor, micro, _, _ = sys.version_info\n    python_version = 'Python_{}.'.format(\n        \".\".join([str(major), str(minor), str(micro)]))\n    pytorch_version = 'Torch_{}.'.format(torch.__version__)\n    numpy_version = 'NumPy_{}.'.format(numpy.__version__)\n    scipy_version = 'SciPy_{}.'.format(scipy.__version__)\n    pytorch_version_formatted = pytorch_version.replace('+','_')\n    return [python_version, pytorch_version_formatted, numpy_version, scipy_version]\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.instantiate_model_from_hpo_class","title":"<code>instantiate_model_from_hpo_class(model, trace_iteration)</code>","text":"<p>Instantiate a <code>base_estimator</code> which can be searched over by the hyperparameter optimization model (UNUSED)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>A hyperparameter optimization model which defines the model to be instantiated.</p> required <code>trace_iteration</code> <code>OpenMLTraceIteration</code> <p>Describing the hyperparameter settings to instantiate.</p> required <p>Returns:</p> Type Description <code>Any</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def instantiate_model_from_hpo_class(\n    self,\n    model: Any,\n    trace_iteration: OpenMLTraceIteration,\n) -&gt; Any:\n    \"\"\"Instantiate a ``base_estimator`` which can be searched over by the hyperparameter\n    optimization model (UNUSED)\n\n    Parameters\n    ----------\n    model : Any\n        A hyperparameter optimization model which defines the model to be instantiated.\n    trace_iteration : OpenMLTraceIteration\n        Describing the hyperparameter settings to instantiate.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return model\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.is_estimator","title":"<code>is_estimator(model)</code>","text":"<p>Check whether the given model is a pytorch estimator.</p> <p>This function is only required for backwards compatibility and will be removed in the near future.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def is_estimator(self, model: Any) -&gt; bool:\n    \"\"\"Check whether the given model is a pytorch estimator.\n\n    This function is only required for backwards compatibility and will be removed in the\n    near future.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    bool\n    \"\"\"\n    return isinstance(model, torch.nn.Module)\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.model_to_flow","title":"<code>model_to_flow(model, custom_name=None)</code>","text":"<p>Transform a Pytorch model to a flow for uploading it to OpenML.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> required <p>Returns:</p> Type Description <code>OpenMLFlow</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def model_to_flow(self, model: Any, custom_name: Optional[str] = None) -&gt; 'OpenMLFlow':\n    \"\"\"Transform a Pytorch model to a flow for uploading it to OpenML.\n\n    Parameters\n    ----------\n    model : Any\n\n    Returns\n    -------\n    OpenMLFlow\n    \"\"\"\n    # Necessary to make pypy not complain about all the different possible return types\n    return self._serialize_pytorch(model, custom_name)\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.obtain_parameter_values","title":"<code>obtain_parameter_values(flow, model=None)</code>","text":"<p>Extracts all parameter settings required for the flow from the model.</p> <p>If no explicit model is provided, the parameters will be extracted from <code>flow.model</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>OpenMLFlow</code> <p>OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)</p> required <code>model</code> <code>Any</code> <p>The model from which to obtain the parameter values. Must match the flow signature. If None, use the model specified in <code>OpenMLFlow.model</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of dicts, where each dict has the following entries: - <code>oml:name</code> : str: The OpenML parameter name - <code>oml:value</code> : mixed: A representation of the parameter value - <code>oml:component</code> : int: flow id to which the parameter belongs</p> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def obtain_parameter_values(\n    self,\n    flow: 'OpenMLFlow',\n    model: Any = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Extracts all parameter settings required for the flow from the model.\n\n    If no explicit model is provided, the parameters will be extracted from `flow.model`\n    instead.\n\n    Parameters\n    ----------\n    flow : OpenMLFlow\n        OpenMLFlow object (containing flow ids, i.e., it has to be downloaded from the server)\n\n    model: Any, optional (default=None)\n        The model from which to obtain the parameter values. Must match the flow signature.\n        If None, use the model specified in ``OpenMLFlow.model``.\n\n    Returns\n    -------\n    list\n        A list of dicts, where each dict has the following entries:\n        - ``oml:name`` : str: The OpenML parameter name\n        - ``oml:value`` : mixed: A representation of the parameter value\n        - ``oml:component`` : int: flow id to which the parameter belongs\n    \"\"\"\n    openml.flows.functions._check_flow_for_server_id(flow)\n\n    def get_flow_dict(_flow):\n        flow_map = {_flow.name: _flow.flow_id}\n        for subflow in _flow.components:\n            flow_map.update(get_flow_dict(_flow.components[subflow]))\n        return flow_map\n\n    def extract_parameters(_flow, _flow_dict, component_model,\n                           _main_call=False, main_id=None):\n        def is_subcomponent_specification(values):\n            # checks whether the current value can be a specification of\n            # subcomponents, as for example the value for steps parameter\n            # (in Pipeline) or transformers parameter (in\n            # ColumnTransformer). These are always lists/tuples of lists/\n            # tuples, size bigger than 2 and an OpenMLFlow item involved.\n            if not isinstance(values, (tuple, list)):\n                return False\n            for item in values:\n                if not isinstance(item, (tuple, list)):\n                    return False\n                if len(item) &lt; 2:\n                    return False\n                if not isinstance(item[1], openml.flows.OpenMLFlow):\n                    return False\n            return True\n\n        # _flow is openml flow object, _param dict maps from flow name to flow\n        # id for the main call, the param dict can be overridden (useful for\n        # unit tests / sentinels) this way, for flows without subflows we do\n        # not have to rely on _flow_dict\n        exp_parameters = set(_flow.parameters)\n        exp_components = set(_flow.components)\n        model_parameters = set([mp for mp in self._get_module_descriptors(component_model)\n                                if '__' not in mp])\n        if len((exp_parameters | exp_components) ^ model_parameters) != 0:\n            flow_params = sorted(exp_parameters | exp_components)\n            model_params = sorted(model_parameters)\n            raise ValueError('Parameters of the model do not match the '\n                             'parameters expected by the '\n                             'flow:\\nexpected flow parameters: '\n                             '%s\\nmodel parameters: %s' % (flow_params,\n                                                           model_params))\n\n        _params = []\n        for _param_name in _flow.parameters:\n            _current = OrderedDict()\n            _current['oml:name'] = _param_name\n\n            current_param_values = self.model_to_flow(\n                self._get_module_descriptors(component_model)[_param_name])\n\n            # Try to filter out components (a.k.a. subflows) which are\n            # handled further down in the code (by recursively calling\n            # this function)!\n            if isinstance(current_param_values, openml.flows.OpenMLFlow):\n                continue\n\n            if is_subcomponent_specification(current_param_values):\n                # complex parameter value, with subcomponents\n                parsed_values = list()\n                for subcomponent in current_param_values:\n                    if len(subcomponent) &lt; 2 or len(subcomponent) &gt; 3:\n                        raise ValueError('Component reference should be '\n                                         'size {2,3}. ')\n\n                    subcomponent_identifier = subcomponent[0]\n                    subcomponent_flow = subcomponent[1]\n                    if not isinstance(subcomponent_identifier, str):\n                        raise TypeError('Subcomponent identifier should be '\n                                        'string')\n                    if not isinstance(subcomponent_flow,\n                                      openml.flows.OpenMLFlow):\n                        raise TypeError('Subcomponent flow should be string')\n\n                    current = {\n                        \"oml-python:serialized_object\": \"component_reference\",\n                        \"value\": {\n                            \"key\": subcomponent_identifier,\n                            \"step_name\": subcomponent_identifier\n                        }\n                    }\n                    if len(subcomponent) == 3:\n                        if not isinstance(subcomponent[2], list):\n                            raise TypeError('Subcomponent argument should be'\n                                            'list')\n                        current['value']['argument_1'] = subcomponent[2]\n                    parsed_values.append(current)\n                parsed_values = json.dumps(parsed_values)\n            else:\n                # vanilla parameter value\n                parsed_values = json.dumps(current_param_values)\n\n            _current['oml:value'] = parsed_values\n            if _main_call:\n                _current['oml:component'] = main_id\n            else:\n                _current['oml:component'] = _flow_dict[_flow.name]\n            _params.append(_current)\n\n        for _identifier in _flow.components:\n            subcomponent_model = self._get_module_descriptors(component_model)[_identifier]\n            _params.extend(extract_parameters(_flow.components[_identifier],\n                                              _flow_dict, subcomponent_model))\n        return _params\n\n    flow_dict = get_flow_dict(flow)\n    model = model if model is not None else flow.model\n    parameters = extract_parameters(flow, flow_dict, model, True, flow.flow_id)\n\n    return parameters\n</code></pre>"},{"location":"pytorch/API%20reference/OpenML%20Connection/#extension.PytorchExtension.seed_model","title":"<code>seed_model(model, seed=None)</code>","text":"<p>Set the random state of all the unseeded components of a model and return the seeded model.</p> <p>Required so that all seed information can be uploaded to OpenML for reproducible results.</p> <p>Models that are already seeded will maintain the seed. In this case, only integer seeds are allowed (An exception is raised when a RandomState was used as seed).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>pytorch model</code> <p>The model to be seeded</p> required <code>seed</code> <code>int</code> <p>The seed to initialize the RandomState with. Unseeded subcomponents will be seeded with a random number from the RandomState.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> Source code in <code>temp_dir/pytorch/openml_pytorch/extension.py</code> <pre><code>def seed_model(self, model: Any, seed: Optional[int] = None) -&gt; Any:\n    \"\"\"Set the random state of all the unseeded components of a model and return the seeded\n    model.\n\n    Required so that all seed information can be uploaded to OpenML for reproducible results.\n\n    Models that are already seeded will maintain the seed. In this case,\n    only integer seeds are allowed (An exception is raised when a RandomState was used as\n    seed).\n\n    Parameters\n    ----------\n    model : pytorch model\n        The model to be seeded\n    seed : int\n        The seed to initialize the RandomState with. Unseeded subcomponents\n        will be seeded with a random number from the RandomState.\n\n    Returns\n    -------\n    Any\n    \"\"\"\n\n    return model\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/","title":"Trainer","text":"<p>This module provides classes and methods to facilitate the configuration, data handling, training, and evaluation of machine learning models using PyTorch and OpenML datasets. The functionalities include: - Generation of default configurations for models. - Handling of image and tabular data. - Training and evaluating machine learning models. - Exporting trained models to ONNX format. - Managing data transformations and loaders.</p> <p>This module provides classes and methods to facilitate the configuration, data handling, training, and evaluation of machine learning models using PyTorch and OpenML datasets. The functionalities include: - Generation of default configurations for models. - Handling of image and tabular data. - Training and evaluating machine learning models. - Exporting trained models to ONNX format. - Managing data transformations and loaders.</p>"},{"location":"pytorch/API%20reference/Trainer/#trainer.BaseDataHandler","title":"<code>BaseDataHandler</code>","text":"<p>BaseDataHandler class is an abstract base class for data handling operations.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>class BaseDataHandler:\n    \"\"\"\n        BaseDataHandler class is an abstract base class for data handling operations.\n    \"\"\"\n    def prepare_data(self, X_train, y_train, X_val, y_val, data_config=None):\n        raise NotImplementedError\n\n    def prepare_test_data(self, X_test, data_config=None):\n        raise NotImplementedError\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/#trainer.DataContainer","title":"<code>DataContainer</code>","text":"<p>class DataContainer:     A class to contain the training, validation, and test data loaders. This just makes it easier to access them when required.</p> <pre><code>Attributes:\ntrain_dl: DataLoader object for the training data.\nvalid_dl: DataLoader object for the validation data.\ntest_dl: Optional DataLoader object for the test data.\n</code></pre> Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>class DataContainer:\n    \"\"\"\n    class DataContainer:\n        A class to contain the training, validation, and test data loaders. This just makes it easier to access them when required.\n\n        Attributes:\n        train_dl: DataLoader object for the training data.\n        valid_dl: DataLoader object for the validation data.\n        test_dl: Optional DataLoader object for the test data.\n    \"\"\"\n    def __init__(self, train_dl, valid_dl, test_dl=None):\n        self.train_dl, self.valid_dl = train_dl, valid_dl\n        self.test_dl = test_dl\n\n    @property\n    def train_ds(self):\n        return self.train_dl.dataset\n\n    @property\n    def valid_ds(self):\n        return self.valid_dl.dataset\n\n    @property\n    def test_ds(self):\n        return self.test_dl.dataset\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/#trainer.DefaultConfigGenerator","title":"<code>DefaultConfigGenerator</code>","text":"<p>DefaultConfigGenerator class provides various methods to generate default configurations.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>class DefaultConfigGenerator:\n    \"\"\"\n    DefaultConfigGenerator class provides various methods to generate default configurations.\n    \"\"\"\n\n    @staticmethod\n    def _default_criterion_gen(task: OpenMLTask) -&gt; torch.nn.Module:\n        \"\"\"\n        _default_criterion_gen returns a criterion based on the task type - regressions use\n        torch.nn.SmoothL1Loss while classifications use torch.nn.CrossEntropyLoss\n        \"\"\"\n        if isinstance(task, OpenMLRegressionTask):\n            return torch.nn.SmoothL1Loss()\n        elif isinstance(task, OpenMLClassificationTask):\n            return torch.nn.CrossEntropyLoss()\n        else:\n            raise ValueError(task)\n\n    @staticmethod\n    def _default_optimizer_gen(model: torch.nn.Module, _: OpenMLTask):\n        \"\"\"\n        _default_optimizer_gen returns the torch.optim.Adam optimizer for the given model\n        \"\"\"\n        return torch.optim.Adam\n\n    @staticmethod\n    def _default_scheduler_gen(optim, _: OpenMLTask) -&gt; Any:\n        \"\"\"\n        _default_scheduler_gen returns the torch.optim.lr_scheduler.ReduceLROnPlateau scheduler for the given optimizer\n        \"\"\"\n        return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optim)\n\n    @staticmethod\n    def _default_predict(output: torch.Tensor, task: OpenMLTask) -&gt; torch.Tensor:\n        \"\"\"\n        _default_predict turns the outputs into predictions by returning the argmax of the output tensor for classification tasks, and by flattening the prediction in case of the regression\n        \"\"\"\n        output_axis = output.dim() - 1\n        if isinstance(task, OpenMLClassificationTask):\n            output = torch.argmax(output, dim=output_axis)\n        elif isinstance(task, OpenMLRegressionTask):\n            output = output.view(-1)\n        else:\n            raise ValueError(task)\n        return output\n\n    @staticmethod\n    def _default_predict_proba(\n        output: torch.Tensor, task: OpenMLTask\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        _default_predict_proba turns the outputs into probabilities using softmax\n        \"\"\"\n        output_axis = output.dim() - 1\n        output = output.softmax(dim=output_axis)\n        return output\n\n    @staticmethod\n    def _default_sanitize(tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        _default sanitizer replaces NaNs with 1e-6\n        \"\"\"\n        tensor = torch.where(\n            torch.isnan(tensor), torch.ones_like(tensor) * torch.tensor(1e-6), tensor\n        )\n        return tensor\n\n    @staticmethod\n    def _default_retype_labels(\n         tensor: torch.Tensor, task: OpenMLTask\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        _default_retype_labels changes the type of the tensor to long for classification tasks and to float for regression tasks\n        \"\"\"\n        if isinstance(task, OpenMLClassificationTask):\n            return tensor.long()\n        elif isinstance(task, OpenMLRegressionTask):\n            return tensor.float()\n        else:\n            raise ValueError(task)\n\n    def get_device(\n        self,\n    ):\n        \"\"\"\n        Checks if a GPU is available and returns the device to be used for training (cuda, mps or cpu)\n        \"\"\"\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n        elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n            device = torch.device(\"mps\")\n        else:\n            device = torch.device(\"cpu\")\n\n        return device\n\n    def default_image_transform(self):\n        return Compose(\n            [\n                ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n                Lambda(convert_to_rgb),  # Convert PIL Image to RGB if it's not already.\n                Resize((128, 128)),  # Resize the image.\n                ToTensor(),  # Convert the PIL Image back to a tensor.\n            ]\n        )\n\n    def return_model_config(self):\n        \"\"\"\n        Returns a configuration object for the model\n        \"\"\"\n\n        return SimpleNamespace(\n            device=self.get_device(),\n            criterion=self._default_criterion_gen,\n            optimizer_gen=self._default_optimizer_gen,\n            scheduler_gen=self._default_scheduler_gen,\n            # predict turns the outputs of the model into actual predictions\n            predict=self._default_predict,  # type: Callable[[torch.Tensor, OpenMLTask], torch.Tensor]\n            # predict_proba turns the outputs of the model into probabilities for each class\n            predict_proba=self._default_predict_proba,  # type: Callable[[torch.Tensor], torch.Tensor]\n            # epoch_count represents the number of epochs the model should be trained for\n            epoch_count=3,  # type: int,\n            # progress_callback=(\n            #     self._default_progress_callback\n            # ),  # type: Callable[[int, int, int, int, float, float], None]\n            # enable progress bar\n            verbose=True,\n        )\n\n    def return_data_config(self):\n        \"\"\"\n        Returns a configuration object for the data\n        \"\"\"\n        return SimpleNamespace(\n            type_of_data=\"image\",\n            perform_validation=False,\n            # progress_callback is called when a training step is finished, in order to report the current progress\n            # sanitize sanitizes the input data in order to ensure that models can be trained safely\n            sanitize=self._default_sanitize,  # type: Callable[[torch.Tensor], torch.Tensor]\n            # retype_labels changes the types of the labels in order to ensure type compatibility\n            retype_labels=(\n                self._default_retype_labels\n            ),  # type: Callable[[torch.Tensor, OpenMLTask], torch.Tensor]\n            # image_size is the size of the images that are fed into the model\n            image_size=128,\n            # batch_size represents the processing batch size for training\n            batch_size=64,  # type: int\n            data_augmentation=None,\n            validation_split=0.1,\n            transform=self.default_image_transform(),\n        )\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/#trainer.DefaultConfigGenerator.get_device","title":"<code>get_device()</code>","text":"<p>Checks if a GPU is available and returns the device to be used for training (cuda, mps or cpu)</p> Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>def get_device(\n    self,\n):\n    \"\"\"\n    Checks if a GPU is available and returns the device to be used for training (cuda, mps or cpu)\n    \"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    return device\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/#trainer.DefaultConfigGenerator.return_data_config","title":"<code>return_data_config()</code>","text":"<p>Returns a configuration object for the data</p> Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>def return_data_config(self):\n    \"\"\"\n    Returns a configuration object for the data\n    \"\"\"\n    return SimpleNamespace(\n        type_of_data=\"image\",\n        perform_validation=False,\n        # progress_callback is called when a training step is finished, in order to report the current progress\n        # sanitize sanitizes the input data in order to ensure that models can be trained safely\n        sanitize=self._default_sanitize,  # type: Callable[[torch.Tensor], torch.Tensor]\n        # retype_labels changes the types of the labels in order to ensure type compatibility\n        retype_labels=(\n            self._default_retype_labels\n        ),  # type: Callable[[torch.Tensor, OpenMLTask], torch.Tensor]\n        # image_size is the size of the images that are fed into the model\n        image_size=128,\n        # batch_size represents the processing batch size for training\n        batch_size=64,  # type: int\n        data_augmentation=None,\n        validation_split=0.1,\n        transform=self.default_image_transform(),\n    )\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/#trainer.DefaultConfigGenerator.return_model_config","title":"<code>return_model_config()</code>","text":"<p>Returns a configuration object for the model</p> Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>def return_model_config(self):\n    \"\"\"\n    Returns a configuration object for the model\n    \"\"\"\n\n    return SimpleNamespace(\n        device=self.get_device(),\n        criterion=self._default_criterion_gen,\n        optimizer_gen=self._default_optimizer_gen,\n        scheduler_gen=self._default_scheduler_gen,\n        # predict turns the outputs of the model into actual predictions\n        predict=self._default_predict,  # type: Callable[[torch.Tensor, OpenMLTask], torch.Tensor]\n        # predict_proba turns the outputs of the model into probabilities for each class\n        predict_proba=self._default_predict_proba,  # type: Callable[[torch.Tensor], torch.Tensor]\n        # epoch_count represents the number of epochs the model should be trained for\n        epoch_count=3,  # type: int,\n        # progress_callback=(\n        #     self._default_progress_callback\n        # ),  # type: Callable[[int, int, int, int, float, float], None]\n        # enable progress bar\n        verbose=True,\n    )\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/#trainer.OpenMLImageHandler","title":"<code>OpenMLImageHandler</code>","text":"<p>               Bases: <code>BaseDataHandler</code></p> <p>OpenMLImageHandler is a class that extends BaseDataHandler to handle image data from OpenML datasets.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>class OpenMLImageHandler(BaseDataHandler):\n    \"\"\"\n        OpenMLImageHandler is a class that extends BaseDataHandler to handle image data from OpenML datasets.\n    \"\"\"\n    def prepare_data(self, X_train, y_train, X_val, y_val, data_config = None):\n        train = OpenMLImageDataset(\n            image_dir=data_config.file_dir,\n            X=X_train,\n            y=y_train,\n            transform_x=data_config.transform,\n            image_size=data_config.image_size,\n        )\n        val = OpenMLImageDataset(\n            image_dir=data_config.file_dir,\n            X=X_val,\n            y=y_val,\n            transform_x=data_config.transform,\n            image_size=data_config.image_size,\n        )\n        return train, val\n\n    def prepare_test_data(self, X_test, data_config = None):\n        test = OpenMLImageDataset(\n            image_dir=data_config.file_dir,\n            X=X_test,\n            y=None,\n            transform_x=data_config.transform,\n            image_size=data_config.image_size,\n        )\n        return test\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/#trainer.OpenMLTabularHandler","title":"<code>OpenMLTabularHandler</code>","text":"<p>               Bases: <code>BaseDataHandler</code></p> <p>OpenMLTabularHandler is a class that extends BaseDataHandler to handle tabular data from OpenML datasets.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>class OpenMLTabularHandler(BaseDataHandler):\n    \"\"\"\n    OpenMLTabularHandler is a class that extends BaseDataHandler to handle tabular data from OpenML datasets.\n    \"\"\"\n    def prepare_data(self, X_train, y_train, X_val, y_val, data_config = None):\n        train = OpenMLTabularDataset(X=X_train, y=y_train)\n        val = OpenMLTabularDataset(X=X_val, y=y_val)\n        return train, val\n\n    def prepare_test_data(self, X_test, data_config = None):\n        test = OpenMLTabularDataset(X=X_test, y=None)\n        return test\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/#trainer.OpenMLTrainerModule","title":"<code>OpenMLTrainerModule</code>","text":"Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>class OpenMLTrainerModule:\n\n    def _default_progress_callback(\n        self, fold: int, rep: int, epoch: int, step: int, loss: float, accuracy: float\n    ):\n        # todo : move this into callback\n        \"\"\"\n                _default_progress_callback reports the current fold, rep, epoch, step and loss for every\n        training iteration to the default logger\n        \"\"\"\n        self.logger.info(\n            \"[%d, %d, %d, %d] loss: %.4f, accuracy: %.4f\"\n            % (fold, rep, epoch, step, loss, accuracy)\n        )\n    def __init__(\n        self,\n        data_module: OpenMLDataModule,\n        callbacks: List[Callback] = [],\n        **kwargs,\n    ):\n        self.config_gen = DefaultConfigGenerator()\n        self.model_config = self.config_gen.return_model_config()\n        self.data_module = data_module\n        self.callbacks = callbacks\n\n        self.config = SimpleNamespace(\n            **{**self.model_config.__dict__, **self.data_module.data_config.__dict__}\n        )\n        # update the config with the user defined values\n        self.config.__dict__.update(kwargs)\n        self.config.progress_callback = self._default_progress_callback\n        self.logger: logging.Logger = logging.getLogger(__name__)\n\n        self.user_defined_measures = OrderedDict()\n        # self.callbacks.append(LoggingCallback(self.logger, print_output=False))\n        self.loss = 0\n        self.training_state = True\n\n        self.phases = [0.2, 0.8]\n        self.scheds = combine_scheds(\n            self.phases, [sched_cos(1e-4, 5e-3), sched_cos(5e-3, 1e-3)]\n        )\n\n        self.cbfs = [\n            Recorder,\n            partial(AvgStatsCallBack, [accuracy]),\n            partial(ParamScheduler, \"lr\", self.scheds),\n            # TensorBoardCallback(),\n        ]\n\n    def _onnx_export(self, model_copy):\n        f = io.BytesIO()\n        torch.onnx.export(model_copy, sample_input, f)\n        onnx_model = onnx.load_model_from_string(f.getvalue())\n        onnx_ = onnx_model.SerializeToString()\n        return onnx_\n\n    def run_model_on_fold(\n        self,\n        model: torch.nn.Module,\n        task: OpenMLTask,\n        X_train: pd.DataFrame,\n        rep_no: int,\n        fold_no: int,\n        y_train: Optional[pd.Series],\n        X_test: pd.DataFrame,\n    ) -&gt; Tuple[np.ndarray, Optional[np.ndarray], OrderedDict, Optional[Any]]:\n\n        # if task has no class labels, we assign the class labels to be the unique values in the training set\n        if task.class_labels is None:\n            task.class_labels = y_train.unique()\n\n        self.add_callbacks()\n\n        self.model = copy.deepcopy(model)\n\n        try:\n            data, model_classes = self.run_training(task, X_train, y_train, X_test)\n\n        except AttributeError as e:\n            # typically happens when training a regressor8 on classification task\n            raise PyOpenMLError(str(e))\n\n        # In supervised learning this returns the predictions for Y\n        pred_y, proba_y = self.run_evaluation(task, data, model_classes)\n\n        # Convert predictions to class labels\n        if task.class_labels is not None:\n            pred_y = [task.class_labels[i] for i in pred_y]\n\n        # Convert model to onnx\n        onnx_ = self._onnx_export(self.model)\n\n        global last_models\n        last_models = onnx_\n\n        return pred_y, proba_y, self.user_defined_measures, None\n\n    def check_config(self):\n        raise NotImplementedError\n\n    def _prediction_to_probabilities(\n        self, y: np.ndarray, classes: List[Any]\n    ) -&gt; np.ndarray:\n        \"\"\"Transforms predicted probabilities to match with OpenML class indices.\n\n        Parameters\n        ----------\n        y : np.ndarray\n            Predicted probabilities (possibly omitting classes if they were not present in the\n            training data).\n        model_classes : list\n            List of classes known_predicted by the model, ordered by their index.\n\n        Returns\n        -------\n        np.ndarray\n        \"\"\"\n        # y: list or numpy array of predictions\n        # model_classes: mapping from original array id to\n        # prediction index id\n        if not isinstance(classes, list):\n            raise ValueError(\n                \"please convert model classes to list prior to \" \"calling this fn\"\n            )\n        result = np.zeros((len(y), len(classes)), dtype=np.float32)\n        for obs, prediction_idx in enumerate(y):\n            result[obs][prediction_idx] = 1.0\n        return result\n\n    def run_evaluation(self, task, data, model_classes):\n        if isinstance(task, OpenMLSupervisedTask):\n            self.model.eval()\n            pred_y = self.pred_test(task, self.model, data.test_dl, self.config.predict)\n        else:\n            raise ValueError(task)\n\n        if isinstance(task, OpenMLClassificationTask):\n            try:\n                self.model.eval()\n                proba_y = self.pred_test(\n                    task, self.model, data.test_dl, self.config.predict_proba\n                )\n\n            except AttributeError:\n                if task.class_labels is not None:\n                    proba_y = self._prediction_to_probabilities(\n                        pred_y, list(task.class_labels)\n                    )\n                else:\n                    raise ValueError(\"The task has no class labels\")\n\n            if task.class_labels is not None:\n                if proba_y.shape[1] != len(task.class_labels):\n                    # Remap the probabilities in case there was a class missing\n                    # at training time. By default, the classification targets\n                    # are mapped to be zero-based indices to the actual classes.\n                    # Therefore, the model_classes contain the correct indices to\n                    # the correct probability array. Example:\n                    # classes in the dataset: 0, 1, 2, 3, 4, 5\n                    # classes in the training set: 0, 1, 2, 4, 5\n                    # then we need to add a column full of zeros into the probabilities\n                    # for class 3 because the rest of the library expects that the\n                    # probabilities are ordered the same way as the classes are ordered).\n                    proba_y_new = np.zeros((proba_y.shape[0], len(task.class_labels)))\n                    for idx, model_class in enumerate(model_classes):\n                        proba_y_new[:, model_class] = proba_y[:, idx]\n                    proba_y = proba_y_new\n\n                if proba_y.shape[1] != len(task.class_labels):\n                    message = \"Estimator only predicted for {}/{} classes!\".format(\n                        proba_y.shape[1],\n                        len(task.class_labels),\n                    )\n                    warnings.warn(message)\n                    self.logger.warning(message)\n            else:\n                raise ValueError(\"The task has no class labels\")\n\n        elif isinstance(task, OpenMLRegressionTask):\n            proba_y = None\n\n        else:\n            raise TypeError(type(task))\n        return pred_y, proba_y\n\n    def run_training(self, task, X_train, y_train, X_test):\n        if isinstance(task, OpenMLSupervisedTask) or isinstance(\n            task, OpenMLClassificationTask\n        ):\n            self.opt = self.config.optimizer_gen(self.model, task)(\n                self.model.parameters()\n            )\n\n            self.criterion = self.config.criterion(task)\n            self.device = self.config.device\n\n            if self.config.device != \"cpu\":\n                self.criterion = self.criterion.to(self.config.device)\n\n            data, model_classes = self.data_module.get_data(\n                X_train, y_train, X_test, task\n            )\n            self.learn = Learner(\n                self.model,\n                self.opt,\n                self.criterion,\n                data,\n                model_classes,\n            )\n            self.learn.device = self.device\n            self.learn.model.to(self.device)\n            gc.collect()\n\n            self.runner = ModelRunner(cb_funcs=self.cbfs)\n            self.learn.model.train()\n            self.runner.fit(epochs=self.config.epoch_count, learn=self.learn)\n            self.learn.model.eval()\n\n            print(\"Loss\", self.runner.loss)\n        return data, model_classes\n\n    def add_callbacks(self):\n        if self.callbacks is not None and len(self.callbacks) &gt; 0:\n            for callback in self.callbacks:\n                if callback not in self.cbfs:\n                    self.cbfs.append(callback)\n\n    def pred_test(self, task, model_copy, test_loader, predict_func):\n        probabilities = []\n        for batch_idx, inputs in enumerate(test_loader):\n            inputs = self.config.sanitize(inputs)\n            # if torch.cuda.is_available():\n            inputs = inputs.to(self.config.device)\n\n            # Perform inference on the batch\n            pred_y_batch = model_copy(inputs)\n            pred_y_batch = predict_func(pred_y_batch, task)\n            pred_y_batch = pred_y_batch.cpu().detach().numpy()\n\n            probabilities.append(pred_y_batch)\n\n            # Concatenate probabilities from all batches\n        pred_y = np.concatenate(probabilities, axis=0)\n        return pred_y\n</code></pre>"},{"location":"pytorch/API%20reference/Trainer/#trainer.convert_to_rgb","title":"<code>convert_to_rgb(image)</code>","text":"<p>Converts an image to RGB mode if it is not already in that mode.</p> <p>Parameters: image (PIL.Image): The image to be converted.</p> <p>Returns: PIL.Image: The converted image in RGB mode.</p> Source code in <code>temp_dir/pytorch/openml_pytorch/trainer.py</code> <pre><code>def convert_to_rgb(image):\n    \"\"\"\n        Converts an image to RGB mode if it is not already in that mode.\n\n        Parameters:\n        image (PIL.Image): The image to be converted.\n\n        Returns:\n        PIL.Image: The converted image in RGB mode.\n    \"\"\"\n    if image.mode != \"RGB\":\n        return image.convert(\"RGB\")\n    return image\n</code></pre>"},{"location":"pytorch/Examples/Create%20Dataset%20and%20Task/","title":"Create dataset and task - tiniest imagenet","text":"In\u00a0[2]: Copied! <pre>import openml\n\nimport numpy as np\nimport pandas as pd\nimport sklearn.datasets\n\nimport openml\nfrom openml.datasets.functions import create_dataset\nimport os\nimport requests\nimport zipfile\nimport glob\n</pre> import openml  import numpy as np import pandas as pd import sklearn.datasets  import openml from openml.datasets.functions import create_dataset import os import requests import zipfile import glob In\u00a0[9]: Copied! <pre>def create_tiny_imagenet():\n    dir_name = \"datasets\"\n    os.makedirs(dir_name, exist_ok=True)\n\n    # download the dataset\n    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n    r = requests.get(url, stream=True)\n\n    if not os.path.exists(f\"{dir_name}/tiny-imagenet-200.zip\"):\n        with open(f\"{dir_name}/tiny-imagenet-200.zip\", \"wb\") as f:\n            f.write(r.content)\n\n        with zipfile.ZipFile(f\"{dir_name}/tiny-imagenet-200.zip\", 'r') as zip_ref:\n            zip_ref.extractall(f\"{dir_name}/\")\n    ## recusively find all the images\n    image_paths = glob.glob(f\"{dir_name}/tiny-imagenet-200/train/*/*/*.JPEG\")\n    ## remove the first part of the path\n    image_paths = [path.split(\"/\", 1)[-1] for path in image_paths]\n    ## create a dataframe with the image path and the label\n    label_func = lambda x: x.split(\"/\")[2]\n    df = pd.DataFrame(image_paths, columns=[\"image_path\"])\n    df[\"label\"] = df[\"image_path\"].apply(label_func)\n    ## encode the labels as integers\n    # df[\"Class_encoded\"] = pd.factorize(df[\"label\"])[0]\n\n    ## encode types\n    df[\"image_path\"] = df[\"image_path\"].astype(\"string\")\n    df[\"label\"] = df[\"label\"].astype(\"string\")\n\n\n    name = \"tiny-imagenet-200\"\n    attribute_names = df.columns\n    description = \"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64 x 64 colored images. Each class has 500 training images, 50 validation images, and 50 test images. The dataset here just contains links to the images and the labels. The dataset can be downloaded from the official website ![here](http://cs231n.stanford.edu/tiny-imagenet-200.zip). /n Link to the paper - [Tiny ImageNet Classification with CNN](https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf)\"\n    paper_url = \"https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf\"\n    citation = (\"Wu, J., Zhang, Q., &amp; Xu, G. (2017). Tiny imagenet challenge. Technical report.\")\n\n    tinyim = create_dataset(\n        name = name,\n        description = description,\n        creator= \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",\n        contributor = \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",\n        collection_date = \"2017\",\n        language= \"English\",\n        licence=\"DbCL v1.0\",\n        default_target_attribute=\"label\",\n        attributes=\"auto\",\n        data=df,\n        citation=citation,\n        ignore_attribute=None\n    )\n    openml.config.apikey = ''\n    tinyim.publish()\n    print(f\"URL for dataset: {tinyim.openml_url}\")\n</pre> def create_tiny_imagenet():     dir_name = \"datasets\"     os.makedirs(dir_name, exist_ok=True)      # download the dataset     url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"     r = requests.get(url, stream=True)      if not os.path.exists(f\"{dir_name}/tiny-imagenet-200.zip\"):         with open(f\"{dir_name}/tiny-imagenet-200.zip\", \"wb\") as f:             f.write(r.content)          with zipfile.ZipFile(f\"{dir_name}/tiny-imagenet-200.zip\", 'r') as zip_ref:             zip_ref.extractall(f\"{dir_name}/\")     ## recusively find all the images     image_paths = glob.glob(f\"{dir_name}/tiny-imagenet-200/train/*/*/*.JPEG\")     ## remove the first part of the path     image_paths = [path.split(\"/\", 1)[-1] for path in image_paths]     ## create a dataframe with the image path and the label     label_func = lambda x: x.split(\"/\")[2]     df = pd.DataFrame(image_paths, columns=[\"image_path\"])     df[\"label\"] = df[\"image_path\"].apply(label_func)     ## encode the labels as integers     # df[\"Class_encoded\"] = pd.factorize(df[\"label\"])[0]      ## encode types     df[\"image_path\"] = df[\"image_path\"].astype(\"string\")     df[\"label\"] = df[\"label\"].astype(\"string\")       name = \"tiny-imagenet-200\"     attribute_names = df.columns     description = \"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64 x 64 colored images. Each class has 500 training images, 50 validation images, and 50 test images. The dataset here just contains links to the images and the labels. The dataset can be downloaded from the official website ![here](http://cs231n.stanford.edu/tiny-imagenet-200.zip). /n Link to the paper - [Tiny ImageNet Classification with CNN](https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf)\"     paper_url = \"https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf\"     citation = (\"Wu, J., Zhang, Q., &amp; Xu, G. (2017). Tiny imagenet challenge. Technical report.\")      tinyim = create_dataset(         name = name,         description = description,         creator= \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",         contributor = \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",         collection_date = \"2017\",         language= \"English\",         licence=\"DbCL v1.0\",         default_target_attribute=\"label\",         attributes=\"auto\",         data=df,         citation=citation,         ignore_attribute=None     )     openml.config.apikey = ''     tinyim.publish()     print(f\"URL for dataset: {tinyim.openml_url}\")  In\u00a0[10]: Copied! <pre>create_tiny_imagenet()\n# https://www.openml.org/d/46346\n</pre> create_tiny_imagenet() # https://www.openml.org/d/46346 In\u00a0[19]: Copied! <pre>def create_tiniest_imagenet():\n    dir_name = \"datasets\"\n    os.makedirs(dir_name, exist_ok=True)\n\n    # download the dataset\n    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n    r = requests.get(url, stream=True)\n\n    if not os.path.exists(f\"{dir_name}/tiny-imagenet-200.zip\"):\n        with open(f\"{dir_name}/tiny-imagenet-200.zip\", \"wb\") as f:\n            f.write(r.content)\n\n        with zipfile.ZipFile(f\"{dir_name}/tiny-imagenet-200.zip\", 'r') as zip_ref:\n            zip_ref.extractall(f\"{dir_name}/\")\n    ## recusively find all the images\n    image_paths = glob.glob(f\"{dir_name}/tiny-imagenet-200/train/*/*/*.JPEG\")\n    ## remove the first part of the path\n    image_paths = [path.split(\"/\", 1)[-1] for path in image_paths]\n    image_paths[-1]\n    ## create a dataframe with the image path and the label\n    label_func = lambda x: x.split(\"/\")[2]\n    df = pd.DataFrame(image_paths, columns=[\"image_path\"])\n    df[\"label\"] = df[\"image_path\"].apply(label_func)\n    ## encode types\n    df[\"image_path\"] = df[\"image_path\"].astype(\"string\")\n    df[\"label\"] = df[\"label\"].astype(\"string\")\n\n    # keep only first 20 images for each label\n    df = df.groupby(\"label\").head(20)\n\n\n    name = \"tiniest-imagenet-200\"\n    attribute_names = df.columns\n    description = \"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64 x 64 colored images. !!! This dataset only links to 20 images per class (instead of the usual 500) and is ONLY for quickly testing a framework. !!! Each class has 500 training images, 50 validation images, and 50 test images. The dataset here just contains links to the images and the labels. The dataset can be downloaded from the official website ![here](http://cs231n.stanford.edu/tiny-imagenet-200.zip). /n Link to the paper - [Tiny ImageNet Classification with CNN](https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf)\"\n    paper_url = \"https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf\"\n    citation = (\"Wu, J., Zhang, Q., &amp; Xu, G. (2017). Tiny imagenet challenge. Technical report.\")\n\n    tinyim = create_dataset(\n        name = name,\n        description = description,\n        creator= \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",\n        contributor = \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",\n        collection_date = \"2017\",\n        language= \"English\",\n        licence=\"DbCL v1.0\",\n        default_target_attribute=\"label\",\n        attributes=\"auto\",\n        data=df,\n        citation=citation,\n        ignore_attribute=None\n    )\n    openml.config.apikey = ''\n    tinyim.publish()\n    print(f\"URL for dataset: {tinyim.openml_url}\")\n</pre> def create_tiniest_imagenet():     dir_name = \"datasets\"     os.makedirs(dir_name, exist_ok=True)      # download the dataset     url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"     r = requests.get(url, stream=True)      if not os.path.exists(f\"{dir_name}/tiny-imagenet-200.zip\"):         with open(f\"{dir_name}/tiny-imagenet-200.zip\", \"wb\") as f:             f.write(r.content)          with zipfile.ZipFile(f\"{dir_name}/tiny-imagenet-200.zip\", 'r') as zip_ref:             zip_ref.extractall(f\"{dir_name}/\")     ## recusively find all the images     image_paths = glob.glob(f\"{dir_name}/tiny-imagenet-200/train/*/*/*.JPEG\")     ## remove the first part of the path     image_paths = [path.split(\"/\", 1)[-1] for path in image_paths]     image_paths[-1]     ## create a dataframe with the image path and the label     label_func = lambda x: x.split(\"/\")[2]     df = pd.DataFrame(image_paths, columns=[\"image_path\"])     df[\"label\"] = df[\"image_path\"].apply(label_func)     ## encode types     df[\"image_path\"] = df[\"image_path\"].astype(\"string\")     df[\"label\"] = df[\"label\"].astype(\"string\")      # keep only first 20 images for each label     df = df.groupby(\"label\").head(20)       name = \"tiniest-imagenet-200\"     attribute_names = df.columns     description = \"Tiny ImageNet contains 100000 images of 200 classes (500 for each class) downsized to 64 x 64 colored images. !!! This dataset only links to 20 images per class (instead of the usual 500) and is ONLY for quickly testing a framework. !!! Each class has 500 training images, 50 validation images, and 50 test images. The dataset here just contains links to the images and the labels. The dataset can be downloaded from the official website ![here](http://cs231n.stanford.edu/tiny-imagenet-200.zip). /n Link to the paper - [Tiny ImageNet Classification with CNN](https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf)\"     paper_url = \"https://cs231n.stanford.edu/reports/2017/pdfs/930.pdf\"     citation = (\"Wu, J., Zhang, Q., &amp; Xu, G. (2017). Tiny imagenet challenge. Technical report.\")      tinyim = create_dataset(         name = name,         description = description,         creator= \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",         contributor = \"Jiayu Wu, Qixiang Zhang, Guoxi Xu\",         collection_date = \"2017\",         language= \"English\",         licence=\"DbCL v1.0\",         default_target_attribute=\"label\",         attributes=\"auto\",         data=df,         citation=citation,         ignore_attribute=None     )     openml.config.apikey = ''     tinyim.publish()     print(f\"URL for dataset: {tinyim.openml_url}\")  In\u00a0[20]: Copied! <pre>create_tiniest_imagenet()\n# https://www.openml.org/d/46347\n</pre> create_tiniest_imagenet() # https://www.openml.org/d/46347 <pre>URL for dataset: https://www.openml.org/d/46347\n</pre> In\u00a0[27]: Copied! <pre>def create_task():\n    # Define task parameters\n    task_type = openml.tasks.TaskType.SUPERVISED_CLASSIFICATION\n    dataset_id = 46347 # Obtained from the dataset creation step\n    evaluation_measure = 'predictive_accuracy'\n    target_name = 'label'\n    class_labels = list(pd.read_csv(\"datasets/tiniest_imagenet.csv\")[\"label\"].unique())\n    cost_matrix = None\n\n    # Create the task\n    new_task = openml.tasks.create_task(\n        task_type=task_type,\n        dataset_id=dataset_id, \n        estimation_procedure_id = 1,\n        evaluation_measure=evaluation_measure,\n        target_name=target_name,\n        class_labels=class_labels,\n        cost_matrix=cost_matrix\n    )\n    openml.config.apikey = ''\n    new_task.publish()\n    print(f\"URL for task: {new_task.openml_url}\")\n</pre> def create_task():     # Define task parameters     task_type = openml.tasks.TaskType.SUPERVISED_CLASSIFICATION     dataset_id = 46347 # Obtained from the dataset creation step     evaluation_measure = 'predictive_accuracy'     target_name = 'label'     class_labels = list(pd.read_csv(\"datasets/tiniest_imagenet.csv\")[\"label\"].unique())     cost_matrix = None      # Create the task     new_task = openml.tasks.create_task(         task_type=task_type,         dataset_id=dataset_id,          estimation_procedure_id = 1,         evaluation_measure=evaluation_measure,         target_name=target_name,         class_labels=class_labels,         cost_matrix=cost_matrix     )     openml.config.apikey = ''     new_task.publish()     print(f\"URL for task: {new_task.openml_url}\") In\u00a0[28]: Copied! <pre>create_task()\n# https://www.openml.org/t/362128\n</pre> create_task() # https://www.openml.org/t/362128 <pre>URL for task: https://www.openml.org/t/362128\n</pre>"},{"location":"pytorch/Examples/Create%20Dataset%20and%20Task/#create-dataset-and-task-tiniest-imagenet","title":"Create dataset and task - tiniest imagenet\u00b6","text":"<ul> <li>An example of how to create a custom dataset and task using the OpenML API and upload it to the OpenML server.</li> <li>Note that you must have an API key from the OpenML website to upload datasets and tasks.</li> </ul>"},{"location":"pytorch/Examples/Create%20Dataset%20and%20Task/#create-dataset-on-openml","title":"Create dataset on OpenML\u00b6","text":"<ul> <li>Instead of making our own, we obtain a subset of the ImageNet dataset from Stanford. This dataset has 200 classes.</li> </ul>"},{"location":"pytorch/Examples/Create%20Dataset%20and%20Task/#another-even-tinier-dataset","title":"Another, even tinier dataset\u00b6","text":"<ul> <li>We subset the previous dataset to 20 images per class.</li> </ul>"},{"location":"pytorch/Examples/Create%20Dataset%20and%20Task/#create-task-on-openml","title":"Create task on OpenML\u00b6","text":"<ul> <li>Now to actually use the OpenML Pytorch API, we need to have a task associated with the dataset. This is how we create it.</li> </ul>"},{"location":"pytorch/Examples/Image%20Classification%20Task/","title":"Image classification task","text":"In\u00a0[\u00a0]: Copied! <pre>import torch.nn\nimport torch.optim\n\nimport openml_pytorch.config\nimport openml\nimport logging\nimport warnings\n\n# Suppress FutureWarning messages\nwarnings.simplefilter(action='ignore')\n\n############################################################################\n# Enable logging in order to observe the progress while running the example.\nopenml.config.logger.setLevel(logging.DEBUG)\nopenml_pytorch.config.logger.setLevel(logging.DEBUG)\n############################################################################\n\n############################################################################\nfrom openml_pytorch.trainer import OpenMLTrainerModule\nfrom openml_pytorch.trainer import OpenMLDataModule\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nimport torchvision\n\nfrom openml_pytorch.trainer import convert_to_rgb\n</pre> import torch.nn import torch.optim  import openml_pytorch.config import openml import logging import warnings  # Suppress FutureWarning messages warnings.simplefilter(action='ignore')  ############################################################################ # Enable logging in order to observe the progress while running the example. openml.config.logger.setLevel(logging.DEBUG) openml_pytorch.config.logger.setLevel(logging.DEBUG) ############################################################################  ############################################################################ from openml_pytorch.trainer import OpenMLTrainerModule from openml_pytorch.trainer import OpenMLDataModule from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda import torchvision  from openml_pytorch.trainer import convert_to_rgb In\u00a0[\u00a0]: Copied! <pre>model = torchvision.models.efficientnet_b0(num_classes=200)\n</pre> model = torchvision.models.efficientnet_b0(num_classes=200) In\u00a0[\u00a0]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(\n            convert_to_rgb\n        ),  # Convert PIL Image to RGB if it's not already.\n        Resize(\n            (64, 64)\n        ),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\ndata_module = OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size = 64,\n    transform=transform\n)\n</pre> transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(             convert_to_rgb         ),  # Convert PIL Image to RGB if it's not already.         Resize(             (64, 64)         ),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] ) data_module = OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size = 64,     transform=transform ) In\u00a0[\u00a0]: Copied! <pre>trainer = OpenMLTrainerModule(\n    data_module=data_module,\n    verbose = True,\n    epoch_count = 1,\n    callbacks=[],\n)\nopenml_pytorch.config.trainer = trainer\n</pre> trainer = OpenMLTrainerModule(     data_module=data_module,     verbose = True,     epoch_count = 1,     callbacks=[], ) openml_pytorch.config.trainer = trainer In\u00a0[\u00a0]: Copied! <pre># Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(362128)\n</pre> # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(362128) In\u00a0[\u00a0]: Copied! <pre>run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>run.publish()\n</pre> run.publish() In\u00a0[\u00a0]: Copied! <pre>trainer.runner.cbs[1].plot_loss()\n</pre> trainer.runner.cbs[1].plot_loss() In\u00a0[\u00a0]: Copied! <pre>trainer.runner.cbs[1].plot_lr()\n</pre> trainer.runner.cbs[1].plot_lr() In\u00a0[\u00a0]: Copied! <pre>trainer.learn.model_classes\n</pre> trainer.learn.model_classes In\u00a0[\u00a0]: Copied! <pre>run.publish()\n</pre> run.publish()"},{"location":"pytorch/Examples/Image%20Classification%20Task/#image-classification-task","title":"Image classification task\u00b6","text":"<ul> <li>Image classification on OpenML Task (362128), tiniest ImageNet dataset.</li> </ul>"},{"location":"pytorch/Examples/Image%20Classification%20Task/#define-the-model","title":"Define the Model\u00b6","text":""},{"location":"pytorch/Examples/Image%20Classification%20Task/#configure-the-data-module","title":"Configure the Data Module\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"pytorch/Examples/Image%20Classification%20Task/#configure-the-trainer-module","title":"Configure the Trainer Module\u00b6","text":""},{"location":"pytorch/Examples/Image%20Classification%20Task/#download-the-task","title":"Download the task\u00b6","text":""},{"location":"pytorch/Examples/Image%20Classification%20Task/#run-the-model-on-the-task","title":"Run the model on the task\u00b6","text":""},{"location":"pytorch/Examples/Image%20Classification%20Task/#view-loss","title":"View loss\u00b6","text":""},{"location":"pytorch/Examples/Image%20Classification%20Task/#view-learning-rate","title":"View learning rate\u00b6","text":""},{"location":"pytorch/Examples/Image%20Classification%20Task/#view-the-classes-in-the-model","title":"View the classes in the model\u00b6","text":""},{"location":"pytorch/Examples/Image%20Classification%20Task/#publish-the-run-to-openml","title":"Publish the run to OpenML\u00b6","text":""},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/","title":"Pretrained Image classification example - Transformer","text":"In\u00a0[\u00a0]: Copied! <pre>import torch.nn\nimport torch.optim\n\nimport openml\nimport openml_pytorch\nimport openml_pytorch.layers\nimport openml_pytorch.config\nfrom openml import OpenMLTask\nimport logging\nimport warnings\nfrom torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda\nfrom openml_pytorch.trainer import convert_to_rgb\n# Suppress FutureWarning messages\nwarnings.simplefilter(action='ignore')\n\n############################################################################\n# Enable logging in order to observe the progress while running the example.\nopenml.config.logger.setLevel(logging.DEBUG)\nopenml_pytorch.config.logger.setLevel(logging.DEBUG)\n############################################################################\n\n############################################################################\nimport torch.nn as nn\nimport torch.nn.functional as F\n</pre> import torch.nn import torch.optim  import openml import openml_pytorch import openml_pytorch.layers import openml_pytorch.config from openml import OpenMLTask import logging import warnings from torchvision.transforms import Compose, Resize, ToPILImage, ToTensor, Lambda from openml_pytorch.trainer import convert_to_rgb # Suppress FutureWarning messages warnings.simplefilter(action='ignore')  ############################################################################ # Enable logging in order to observe the progress while running the example. openml.config.logger.setLevel(logging.DEBUG) openml_pytorch.config.logger.setLevel(logging.DEBUG) ############################################################################  ############################################################################ import torch.nn as nn import torch.nn.functional as F In\u00a0[\u00a0]: Copied! <pre># openml.config.apikey = 'key'\nfrom openml_pytorch.trainer import OpenMLTrainerModule\nfrom openml_pytorch.trainer import OpenMLDataModule\nfrom openml_pytorch.trainer import Callback\n</pre> # openml.config.apikey = 'key' from openml_pytorch.trainer import OpenMLTrainerModule from openml_pytorch.trainer import OpenMLDataModule from openml_pytorch.trainer import Callback In\u00a0[\u00a0]: Copied! <pre># Example model. You can do better :)\nimport torchvision.models as models\n\n# Load the pre-trained ResNet model\nmodel = models.efficientnet_b0(pretrained=True)\n\n# Modify the last fully connected layer to the required number of classes\nnum_classes = 200\nin_features = model.classifier[-1].in_features\n# model.fc = nn.Linear(in_features, num_classes)\nmodel.classifier = nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features, num_classes),\n)\n\n# Optional: If you're fine-tuning, you may want to freeze the pre-trained layers\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# # If you want to train the last layer only (the newly added layer)\n# for param in model.fc.parameters():\n#     param.requires_grad = True\n</pre> # Example model. You can do better :) import torchvision.models as models  # Load the pre-trained ResNet model model = models.efficientnet_b0(pretrained=True)  # Modify the last fully connected layer to the required number of classes num_classes = 200 in_features = model.classifier[-1].in_features # model.fc = nn.Linear(in_features, num_classes) model.classifier = nn.Sequential(     nn.Dropout(p=0.2, inplace=True),     nn.Linear(in_features, num_classes), )  # Optional: If you're fine-tuning, you may want to freeze the pre-trained layers # for param in model.parameters(): #     param.requires_grad = False  # # If you want to train the last layer only (the newly added layer) # for param in model.fc.parameters(): #     param.requires_grad = True In\u00a0[\u00a0]: Copied! <pre>transform = Compose(\n    [\n        ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.\n        Lambda(\n            convert_to_rgb\n        ),  # Convert PIL Image to RGB if it's not already.\n        Resize(\n            (64, 64)\n        ),  # Resize the image.\n        ToTensor(),  # Convert the PIL Image back to a tensor.\n    ]\n)\ndata_module = OpenMLDataModule(\n    type_of_data=\"image\",\n    file_dir=\"datasets\",\n    filename_col=\"image_path\",\n    target_mode=\"categorical\",\n    target_column=\"label\",\n    batch_size = 64,\n    transform=transform\n)\n</pre> transform = Compose(     [         ToPILImage(),  # Convert tensor to PIL Image to ensure PIL Image operations can be applied.         Lambda(             convert_to_rgb         ),  # Convert PIL Image to RGB if it's not already.         Resize(             (64, 64)         ),  # Resize the image.         ToTensor(),  # Convert the PIL Image back to a tensor.     ] ) data_module = OpenMLDataModule(     type_of_data=\"image\",     file_dir=\"datasets\",     filename_col=\"image_path\",     target_mode=\"categorical\",     target_column=\"label\",     batch_size = 64,     transform=transform ) In\u00a0[\u00a0]: Copied! <pre>def custom_optimizer_gen(model: torch.nn.Module, task: OpenMLTask) -&gt; torch.optim.Optimizer:\n    return torch.optim.Adam(model.fc.parameters())\n\ntrainer = OpenMLTrainerModule(\n    data_module=data_module,\n    verbose = True,\n    epoch_count = 1,\n    optimizer = custom_optimizer_gen,\n    callbacks=[],\n)\nopenml_pytorch.config.trainer = trainer\n</pre> def custom_optimizer_gen(model: torch.nn.Module, task: OpenMLTask) -&gt; torch.optim.Optimizer:     return torch.optim.Adam(model.fc.parameters())  trainer = OpenMLTrainerModule(     data_module=data_module,     verbose = True,     epoch_count = 1,     optimizer = custom_optimizer_gen,     callbacks=[], ) openml_pytorch.config.trainer = trainer In\u00a0[\u00a0]: Copied! <pre># Download the OpenML task for tiniest imagenet\ntask = openml.tasks.get_task(362128)\n</pre>  # Download the OpenML task for tiniest imagenet task = openml.tasks.get_task(362128) In\u00a0[\u00a0]: Copied! <pre>#\n# Run the model on the task (requires an API key).m\nrun = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> # # Run the model on the task (requires an API key).m run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>trainer.runner.cbs[1].plot_loss()\n</pre> trainer.runner.cbs[1].plot_loss() In\u00a0[\u00a0]: Copied! <pre>trainer.runner.cbs[1].plot_lr()\n</pre> trainer.runner.cbs[1].plot_lr() In\u00a0[\u00a0]: Copied! <pre>run.publish()\n</pre> run.publish()"},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/#pretrained-image-classification-example-transformer","title":"Pretrained Image classification example - Transformer\u00b6","text":"<ul> <li>Pretrained image classification using a Transformer architecture, \"custom\" Optimizer for OpenML Task (362128) , tiniest ImageNet dataset.</li> </ul>"},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/#define-the-model","title":"Define the Model\u00b6","text":""},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/#configure-the-data-module","title":"Configure the Data Module\u00b6","text":"<ul> <li>Make sure the data is present in the <code>file_dir</code> directory, and the <code>filename_col</code> is correctly set along with this column correctly pointing to where your data is stored.</li> </ul>"},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/#configure-the-trainer-module","title":"Configure the Trainer Module\u00b6","text":""},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/#download-the-task","title":"Download the task\u00b6","text":""},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/#run-the-model-on-the-task","title":"Run the model on the task\u00b6","text":""},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/#view-loss","title":"View loss\u00b6","text":""},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/#view-learning-rate","title":"View learning rate\u00b6","text":""},{"location":"pytorch/Examples/Pretrained%20Transformer%20Image%20Classification%20Task/#publish-the-run-to-openml","title":"Publish the run to OpenML\u00b6","text":""},{"location":"pytorch/Examples/Sequential%20Classification%20Task/","title":"Sequential classification","text":"In\u00a0[\u00a0]: Copied! <pre>import torch.nn\nimport torch.optim\n\nimport openml_pytorch.config\nimport openml\nimport logging\nimport warnings\n\n# Suppress FutureWarning messages\nwarnings.simplefilter(action='ignore')\n\n############################################################################\n# Enable logging in order to observe the progress while running the example.\nopenml.config.logger.setLevel(logging.DEBUG)\nopenml_pytorch.config.logger.setLevel(logging.DEBUG)\n############################################################################\n</pre>  import torch.nn import torch.optim  import openml_pytorch.config import openml import logging import warnings  # Suppress FutureWarning messages warnings.simplefilter(action='ignore')  ############################################################################ # Enable logging in order to observe the progress while running the example. openml.config.logger.setLevel(logging.DEBUG) openml_pytorch.config.logger.setLevel(logging.DEBUG) ############################################################################ In\u00a0[\u00a0]: Copied! <pre>from openml_pytorch.trainer import OpenMLTrainerModule\nfrom openml_pytorch.trainer import OpenMLDataModule\n</pre> from openml_pytorch.trainer import OpenMLTrainerModule from openml_pytorch.trainer import OpenMLDataModule In\u00a0[\u00a0]: Copied! <pre>############################################################################\n# Define a sequential network that does the initial image reshaping\n# and normalization model.\nprocessing_net = torch.nn.Sequential(\n    openml_pytorch.layers.Functional(function=torch.Tensor.reshape,\n                                                shape=(-1, 1, 28, 28)),\n    torch.nn.BatchNorm2d(num_features=1)\n)\n############################################################################\n\n############################################################################\n# Define a sequential network that does the extracts the features from the\n# image.\nfeatures_net = torch.nn.Sequential(\n    torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n    torch.nn.LeakyReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n    torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),\n    torch.nn.LeakyReLU(),\n    torch.nn.MaxPool2d(kernel_size=2),\n)\n############################################################################\n\n############################################################################\n# Define a sequential network that flattens the features and compiles the\n# results into probabilities for each digit.\nresults_net = torch.nn.Sequential(\n    openml_pytorch.layers.Functional(function=torch.Tensor.reshape,\n                                                shape=(-1, 4 * 4 * 64)),\n    torch.nn.Linear(in_features=4 * 4 * 64, out_features=256),\n    torch.nn.LeakyReLU(),\n    torch.nn.Dropout(),\n    torch.nn.Linear(in_features=256, out_features=10),\n)\n############################################################################\n# openml.config.apikey = 'key'\n\n############################################################################\n# The main network, composed of the above specified networks.\nmodel = torch.nn.Sequential(\n    processing_net,\n    features_net,\n    results_net\n)\n############################################################################\n</pre>  ############################################################################ # Define a sequential network that does the initial image reshaping # and normalization model. processing_net = torch.nn.Sequential(     openml_pytorch.layers.Functional(function=torch.Tensor.reshape,                                                 shape=(-1, 1, 28, 28)),     torch.nn.BatchNorm2d(num_features=1) ) ############################################################################  ############################################################################ # Define a sequential network that does the extracts the features from the # image. features_net = torch.nn.Sequential(     torch.nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),     torch.nn.LeakyReLU(),     torch.nn.MaxPool2d(kernel_size=2),     torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),     torch.nn.LeakyReLU(),     torch.nn.MaxPool2d(kernel_size=2), ) ############################################################################  ############################################################################ # Define a sequential network that flattens the features and compiles the # results into probabilities for each digit. results_net = torch.nn.Sequential(     openml_pytorch.layers.Functional(function=torch.Tensor.reshape,                                                 shape=(-1, 4 * 4 * 64)),     torch.nn.Linear(in_features=4 * 4 * 64, out_features=256),     torch.nn.LeakyReLU(),     torch.nn.Dropout(),     torch.nn.Linear(in_features=256, out_features=10), ) ############################################################################ # openml.config.apikey = 'key'  ############################################################################ # The main network, composed of the above specified networks. model = torch.nn.Sequential(     processing_net,     features_net,     results_net ) ############################################################################  In\u00a0[\u00a0]: Copied! <pre>data_module = OpenMLDataModule(\n    type_of_data=\"dataframe\",\n    filename_col=\"class\",\n    target_mode=\"categorical\",\n)\n</pre> data_module = OpenMLDataModule(     type_of_data=\"dataframe\",     filename_col=\"class\",     target_mode=\"categorical\", ) In\u00a0[\u00a0]: Copied! <pre>trainer = OpenMLTrainerModule(\n    data_module=data_module,\n    verbose = True,\n    epoch_count = 1,\n    callbacks=[],\n)\nopenml_pytorch.config.trainer = trainer\n</pre>  trainer = OpenMLTrainerModule(     data_module=data_module,     verbose = True,     epoch_count = 1,     callbacks=[], ) openml_pytorch.config.trainer = trainer In\u00a0[\u00a0]: Copied! <pre># Download the OpenML task for the mnist 784 dataset.\ntask = openml.tasks.get_task(3573)\n</pre> # Download the OpenML task for the mnist 784 dataset. task = openml.tasks.get_task(3573) In\u00a0[\u00a0]: Copied! <pre>run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>trainer.runner.cbs[1].plot_loss()\n</pre> trainer.runner.cbs[1].plot_loss() In\u00a0[\u00a0]: Copied! <pre>trainer.runner.cbs[1].plot_lr()\n</pre> trainer.runner.cbs[1].plot_lr() In\u00a0[\u00a0]: Copied! <pre>run.publish()\n</pre> run.publish()"},{"location":"pytorch/Examples/Sequential%20Classification%20Task/#sequential-classification","title":"Sequential classification\u00b6","text":"<ul> <li>Sequential classification of a tabular MNIST dataset (Task 3573) using a simple neural network.</li> </ul>"},{"location":"pytorch/Examples/Sequential%20Classification%20Task/#define-the-model","title":"Define the Model\u00b6","text":""},{"location":"pytorch/Examples/Sequential%20Classification%20Task/#configure-the-data-module","title":"Configure the Data Module\u00b6","text":"<ul> <li>Make sure the <code>target_col</code> is correctly set.</li> </ul>"},{"location":"pytorch/Examples/Sequential%20Classification%20Task/#configure-the-trainer-module","title":"Configure the Trainer Module\u00b6","text":""},{"location":"pytorch/Examples/Sequential%20Classification%20Task/#download-the-task","title":"Download the task\u00b6","text":""},{"location":"pytorch/Examples/Sequential%20Classification%20Task/#run-the-model-on-the-task","title":"Run the model on the task\u00b6","text":""},{"location":"pytorch/Examples/Sequential%20Classification%20Task/#view-loss","title":"View loss\u00b6","text":""},{"location":"pytorch/Examples/Sequential%20Classification%20Task/#view-learning-rate","title":"View learning rate\u00b6","text":""},{"location":"pytorch/Examples/Sequential%20Classification%20Task/#publish-the-run-to-openml","title":"Publish the run to OpenML\u00b6","text":""},{"location":"pytorch/Examples/Tabular%20Classification/","title":"Tabular classification","text":"In\u00a0[\u00a0]: Copied! <pre>import torch.nn\nimport torch.optim\n\nimport openml\nimport openml_pytorch\nimport openml_pytorch.layers\nimport openml_pytorch.config\nimport logging\n\n\n############################################################################\n# Enable logging in order to observe the progress while running the example.\nopenml.config.logger.setLevel(logging.DEBUG)\nopenml_pytorch.config.logger.setLevel(logging.DEBUG)\n############################################################################\n</pre>  import torch.nn import torch.optim  import openml import openml_pytorch import openml_pytorch.layers import openml_pytorch.config import logging   ############################################################################ # Enable logging in order to observe the progress while running the example. openml.config.logger.setLevel(logging.DEBUG) openml_pytorch.config.logger.setLevel(logging.DEBUG) ############################################################################ In\u00a0[\u00a0]: Copied! <pre>from openml_pytorch.trainer import OpenMLTrainerModule\nfrom openml_pytorch.trainer import OpenMLDataModule\nfrom openml_pytorch.trainer import Callback\n</pre> from openml_pytorch.trainer import OpenMLTrainerModule from openml_pytorch.trainer import OpenMLDataModule from openml_pytorch.trainer import Callback In\u00a0[\u00a0]: Copied! <pre>class TabularClassificationmodel(torch.nn.Module):\n    def __init__(self, input_size, output_size):\n        super(TabularClassificationmodel, self).__init__()\n        self.fc1 = torch.nn.Linear(input_size, 128)\n        self.fc2 = torch.nn.Linear(128, 64)\n        self.fc3 = torch.nn.Linear(64, output_size)\n        self.relu = torch.nn.ReLU()\n        self.softmax = torch.nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        x = self.softmax(x)\n        return x\n</pre> class TabularClassificationmodel(torch.nn.Module):     def __init__(self, input_size, output_size):         super(TabularClassificationmodel, self).__init__()         self.fc1 = torch.nn.Linear(input_size, 128)         self.fc2 = torch.nn.Linear(128, 64)         self.fc3 = torch.nn.Linear(64, output_size)         self.relu = torch.nn.ReLU()         self.softmax = torch.nn.Softmax(dim=1)      def forward(self, x):         x = self.fc1(x)         x = self.relu(x)         x = self.fc2(x)         x = self.relu(x)         x = self.fc3(x)         x = self.softmax(x)         return x In\u00a0[\u00a0]: Copied! <pre>model = TabularClassificationmodel(20, 2)\n</pre> model = TabularClassificationmodel(20, 2) In\u00a0[\u00a0]: Copied! <pre># supervised credit-g classification\ntask = openml.tasks.get_task(31)\n</pre> # supervised credit-g classification task = openml.tasks.get_task(31) In\u00a0[\u00a0]: Copied! <pre>data_module = OpenMLDataModule(\n    type_of_data=\"dataframe\",\n    target_column=\"class\",\n    target_mode=\"categorical\",\n)\n</pre> data_module = OpenMLDataModule(     type_of_data=\"dataframe\",     target_column=\"class\",     target_mode=\"categorical\", ) In\u00a0[\u00a0]: Copied! <pre>trainer = OpenMLTrainerModule(\n    data_module=data_module,\n    verbose = True,\n    epoch_count = 5,\n)\nopenml_pytorch.config.trainer = trainer\n</pre>  trainer = OpenMLTrainerModule(     data_module=data_module,     verbose = True,     epoch_count = 5, ) openml_pytorch.config.trainer = trainer In\u00a0[\u00a0]: Copied! <pre>run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False)\n</pre> run = openml.runs.run_model_on_task(model, task, avoid_duplicate_runs=False) In\u00a0[\u00a0]: Copied! <pre>run.publish()\n</pre> run.publish() In\u00a0[\u00a0]: Copied! <pre># openml.config.apikey = ''\n</pre> # openml.config.apikey = '' In\u00a0[\u00a0]: Copied! <pre>trainer.runner.cbs[1].plot_loss()\n</pre> trainer.runner.cbs[1].plot_loss() In\u00a0[\u00a0]: Copied! <pre>trainer.runner.cbs[1].plot_lr()\n</pre> trainer.runner.cbs[1].plot_lr()"},{"location":"pytorch/Examples/Tabular%20Classification/#tabular-classification","title":"Tabular classification\u00b6","text":"<ul> <li>Supervised credit-g classification</li> </ul>"},{"location":"pytorch/Examples/Tabular%20Classification/#define-the-model","title":"Define the Model\u00b6","text":""},{"location":"pytorch/Examples/Tabular%20Classification/#configure-the-data-module","title":"Configure the Data Module\u00b6","text":"<ul> <li>Make sure the <code>target_col</code> is correctly set.</li> </ul>"},{"location":"pytorch/Examples/Tabular%20Classification/#configure-the-trainer-module","title":"Configure the Trainer Module\u00b6","text":""},{"location":"pytorch/Examples/Tabular%20Classification/#run-the-model-on-the-task","title":"Run the model on the task\u00b6","text":""},{"location":"pytorch/Examples/Tabular%20Classification/#view-loss","title":"View loss\u00b6","text":""},{"location":"pytorch/Examples/Tabular%20Classification/#view-learning-rate","title":"View learning rate\u00b6","text":""},{"location":"julia/","title":"Home","text":""},{"location":"julia/#openmljl-documentation","title":"OpenML.jl Documentation","text":"<p>This is the reference documentation of <code>OpenML.jl</code>.</p> <p>The OpenML platform provides an integration platform for carrying out and comparing machine learning solutions across a broad collection of public datasets and software platforms.</p> <p>Summary of OpenML.jl functionality:</p> <ul> <li><code>OpenML.list_tags</code><code>()</code>: for listing all dataset tags</li> <li><code>OpenML.list_datasets</code><code>(; tag=nothing, filter=nothing, output_format=...)</code>: for listing available datasets</li> <li><code>OpenML.describe_dataset</code><code>(id)</code>: to describe a particular dataset</li> <li><code>OpenML.load</code><code>(id; parser=:arff)</code>: to download a dataset</li> </ul> <p></p> <p></p>"},{"location":"julia/#installation","title":"Installation","text":"<pre><code>using Pkg\nPkg.add(\"OpenML\")\n</code></pre> <p>If running the demonstration below:</p> <pre><code>Pkg.add(\"DataFrames\") \nPkg.add(\"ScientificTypes\")\n</code></pre> <p></p> <p></p>"},{"location":"julia/#sample-usage","title":"Sample usage","text":"<pre><code>julia&gt; using OpenML # or using MLJ\n\n\njulia&gt; using DataFrames\n\n\njulia&gt; OpenML.list_tags()\n300-element Vector{Any}:\n \"study_41\"\n \"uci\"\n \"study_34\"\n \"study_37\"\n \"mythbusting_1\"\n \"OpenML-CC18\"\n \"study_99\"\n \"artificial\"\n \"BNG\"\n \"study_16\"\n \u22ee\n \"Earth Science\"\n \"Social Media\"\n \"Meteorology\"\n \"Geography\"\n \"Language\"\n \"Computational Universe\"\n \"History\"\n \"Culture\"\n \"Sociology\"\n</code></pre> <p>Listing all datasets with the \"OpenML100\" tag which also have <code>n</code> instances and <code>p</code> features, where <code>100 &lt; n &lt; 1000</code> and <code>1 &lt; p &lt; 10</code>:</p> <pre><code>julia&gt; ds = OpenML.list_datasets(\n                 tag = \"OpenML100\",\n                 filter = \"number_instances/100..1000/number_features/1..10\",\n                 output_format = DataFrame)\n12\u00d713 DataFrame\n Row \u2502 id     name                              status  MajorityClassSize  Max \u22ef\n     \u2502 Int64  String                            String  Int64?             Int \u22ef\n\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   1 \u2502    11  balance-scale                     active                288      \u22ef\n   2 \u2502    15  breast-w                          active                458\n   3 \u2502    37  diabetes                          active                500\n   4 \u2502    50  tic-tac-toe                       active                626\n   5 \u2502   333  monks-problems-1                  active                278      \u22ef\n   6 \u2502   334  monks-problems-2                  active                395\n   7 \u2502   335  monks-problems-3                  active                288\n   8 \u2502   451  irish                             active                278\n   9 \u2502   469  analcatdata_dmft                  active                155      \u22ef\n  10 \u2502   470  profb                             active                448\n  11 \u2502  1464  blood-transfusion-service-center  active                570\n  12 \u2502 40496  LED-display-domain-7digit         active                 57\n                                                               9 columns omitted\n</code></pre> <p>Describing and loading one of these datasets:</p> <pre><code>julia&gt; OpenML.describe_dataset(15)\n  Author: Dr. William H. Wolberg, University of Wisconsin Source: UCI\n  (https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)),\n  University of Wisconsin (http://pages.cs.wisc.edu/~olvi/uwmp/cancer.html) -\n  1995 Please cite: See below, plus UCI\n  (https://archive.ics.uci.edu/ml/citation_policy.html)\n\n  Breast Cancer Wisconsin (Original) Data Set. Features are computed from a\n  digitized image of a fine needle aspirate (FNA) of a breast mass. They\n  describe characteristics of the cell nuclei present in the image. The target\n  feature records the prognosis (malignant or benign). Original data available\n  here (ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/)\n\n  Current dataset was adapted to ARFF format from the UCI version. Sample code\n  ID's were removed.\n\n  ! Note that there is also a related Breast Cancer Wisconsin (Diagnosis) Data\n  Set with a different set of features, better known as wdbc\n  (https://www.openml.org/d/1510).\n\n  Relevant Papers\n  \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\n\n  W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n  for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on\n  Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San\n  Jose, CA, 1993.\n\n  O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n  prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n  July-August 1995.\n\n  Citation request\n  \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\n\n  This breast cancer database was obtained from the University of Wisconsin\n  Hospitals, Madison from Dr. William H. Wolberg. If you publish results when\n  using this database, then please include this information in your\n  acknowledgments. Also, please cite one or more of:\n\n    1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear\n       programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1\n       &amp; 18.\n\n    2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of\n       pattern separation for medical diagnosis applied to breast\n       cytology\", Proceedings of the National Academy of Sciences,\n       U.S.A., Volume 87, December 1990, pp 9193-9196.\n\n    3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern\n       recognition via linear programming: Theory and application to\n       medical diagnosis\", in: \"Large-scale numerical optimization\",\n       Thomas F. Coleman and Yuying Li, editors, SIAM Publications,\n       Philadelphia 1990, pp 22-30.\n\n    4. K. P. Bennett &amp; O. L. Mangasarian: \"Robust linear programming\n       discrimination of two linearly inseparable sets\", Optimization\n       Methods and Software 1, 1992, 23-34 (Gordon &amp; Breach Science\n       Publishers).\n\njulia&gt; table = OpenML.load(15)\nTables.DictColumnTable with 699 rows, 10 columns, and schema:\n :Clump_Thickness        Float64\n :Cell_Size_Uniformity   Float64\n :Cell_Shape_Uniformity  Float64\n :Marginal_Adhesion      Float64\n :Single_Epi_Cell_Size   Float64\n :Bare_Nuclei            Union{Missing, Float64}\n :Bland_Chromatin        Float64\n :Normal_Nucleoli        Float64\n :Mitoses                Float64\n :Class                  CategoricalArrays.CategoricalValue{String, UInt32}\n</code></pre> <p>Converting to a data frame:</p> <pre><code>julia&gt; df = DataFrame(table)\n699\u00d710 DataFrame\n Row \u2502 Clump_Thickness  Cell_Size_Uniformity  Cell_Shape_Uniformity  Marginal_ \u22ef\n     \u2502 Float64          Float64               Float64                Float64   \u22ef\n\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   1 \u2502             5.0                   1.0                    1.0            \u22ef\n   2 \u2502             5.0                   4.0                    4.0\n   3 \u2502             3.0                   1.0                    1.0\n   4 \u2502             6.0                   8.0                    8.0\n   5 \u2502             4.0                   1.0                    1.0            \u22ef\n   6 \u2502             8.0                  10.0                   10.0\n   7 \u2502             1.0                   1.0                    1.0\n   8 \u2502             2.0                   1.0                    2.0\n  \u22ee  \u2502        \u22ee                  \u22ee                      \u22ee                    \u22ee \u22f1\n 693 \u2502             3.0                   1.0                    1.0            \u22ef\n 694 \u2502             3.0                   1.0                    1.0\n 695 \u2502             3.0                   1.0                    1.0\n 696 \u2502             2.0                   1.0                    1.0\n 697 \u2502             5.0                  10.0                   10.0            \u22ef\n 698 \u2502             4.0                   8.0                    6.0\n 699 \u2502             4.0                   8.0                    8.0\n                                                  7 columns and 684 rows omitted\n</code></pre> <p>Inspecting it's schema:</p> <pre><code>julia&gt; using ScientificTypes\n\n\njulia&gt; schema(table)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502 names                 \u2502 scitypes                   \u2502 types                   \u22ef\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\u2502 Clump_Thickness       \u2502 Continuous                 \u2502 Float64                 \u22ef\n\u2502 Cell_Size_Uniformity  \u2502 Continuous                 \u2502 Float64                 \u22ef\n\u2502 Cell_Shape_Uniformity \u2502 Continuous                 \u2502 Float64                 \u22ef\n\u2502 Marginal_Adhesion     \u2502 Continuous                 \u2502 Float64                 \u22ef\n\u2502 Single_Epi_Cell_Size  \u2502 Continuous                 \u2502 Float64                 \u22ef\n\u2502 Bare_Nuclei           \u2502 Union{Missing, Continuous} \u2502 Union{Missing, Float64} \u22ef\n\u2502 Bland_Chromatin       \u2502 Continuous                 \u2502 Float64                 \u22ef\n\u2502 Normal_Nucleoli       \u2502 Continuous                 \u2502 Float64                 \u22ef\n\u2502 Mitoses               \u2502 Continuous                 \u2502 Float64                 \u22ef\n\u2502 Class                 \u2502 Multiclass{2}              \u2502 CategoricalValue{String \u22ef\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                                                                1 column omitted\n</code></pre> <p></p> <p></p>"},{"location":"julia/#public-api","title":"Public API","text":""},{"location":"julia/#openmllist_tags","title":"<code>OpenML.list_tags</code>","text":"<pre><code>list_tags()\n</code></pre> <p>List all available tags.</p>"},{"location":"julia/#openmllist_datasets","title":"<code>OpenML.list_datasets</code>","text":"<pre><code>list_datasets(; tag = nothing, filters = \"\", output_format = NamedTuple)\n</code></pre> <p>Lists all active OpenML datasets, if <code>tag = nothing</code> (default). To list only datasets with a given tag, choose one of the tags in <code>list_tags()</code>. An alternative <code>output_format</code> can be chosen, e.g. <code>DataFrame</code>, if the <code>DataFrames</code> package is loaded.</p> <p>A filter is a string of <code>&lt;data quality&gt;/&lt;range&gt;</code> or <code>&lt;data quality&gt;/&lt;value&gt;</code> pairs, concatenated using <code>/</code>, such as</p> <pre><code>    filter = \"number_features/10/number_instances/500..10000\"\n</code></pre> <p>The allowed data qualities include <code>tag</code>, <code>status</code>, <code>limit</code>, <code>offset</code>, <code>data_id</code>, <code>data_name</code>, <code>data_version</code>, <code>uploader</code>, <code>number_instances</code>, <code>number_features</code>, <code>number_classes</code>, <code>number_missing_values</code>.</p> <p>For more on the format and effect of <code>filters</code> refer to the openml API.</p> <p>Examples</p> <pre><code>julia&gt; using DataFrames\n\njulia&gt; ds = OpenML.list_datasets(\n               tag = \"OpenML100\",\n               filter = \"number_instances/100..1000/number_features/1..10\",\n               output_format = DataFrame\n)\n\njulia&gt; sort!(ds, :NumberOfFeatures)\n</code></pre>"},{"location":"julia/#openmldescribe_dataset","title":"<code>OpenML.describe_dataset</code>","text":"<pre><code>describe_dataset(id)\n</code></pre> <p>Load and show the OpenML description of the data set <code>id</code>. Use <code>list_datasets</code> to browse available data sets.</p> <p>Examples</p> <pre><code>julia&gt; OpenML.describe_dataset(6)\n  Author: David J. Slate Source: UCI\n  (https://archive.ics.uci.edu/ml/datasets/Letter+Recognition) - 01-01-1991 Please cite: P.\n  W. Frey and D. J. Slate. \"Letter Recognition Using Holland-style Adaptive Classifiers\".\n  Machine Learning 6(2), 1991\n\n    1. TITLE:\n\n  Letter Image Recognition Data\n\n  The objective is to identify each of a large number of black-and-white\n  rectangular pixel displays as one of the 26 capital letters in the English\n  alphabet.  The character images were based on 20 different fonts and each\n  letter within these 20 fonts was randomly distorted to produce a file of\n  20,000 unique stimuli.  Each stimulus was converted into 16 primitive\n  numerical attributes (statistical moments and edge counts) which were then\n  scaled to fit into a range of integer values from 0 through 15.  We\n  typically train on the first 16000 items and then use the resulting model\n  to predict the letter category for the remaining 4000.  See the article\n  cited above for more details.\n</code></pre>"},{"location":"julia/#openmlload","title":"<code>OpenML.load</code>","text":"<pre><code>OpenML.load(id; maxbytes = nothing)\n</code></pre> <p>Load the OpenML dataset with specified <code>id</code>, from those listed by <code>list_datasets</code> or on the OpenML site.</p> <p>Datasets are saved as julia artifacts so that they persist locally once loaded.</p> <p>Returns a table.</p> <p>Examples</p> <pre><code>using DataFrames\ntable = OpenML.load(61)\ndf = DataFrame(table) # transform to a DataFrame\nusing ScientificTypes\ndf2 = coerce(df, autotype(df)) # corce to automatically detected scientific types\n\npeek_table = OpenML.load(61, maxbytes = 1024) # load only the first 1024 bytes of the table\n</code></pre>"},{"location":"apiv2/","title":"OpenML Server API Software Documentation","text":"<p>This is the Python-based OpenML REST API server. It's a rewrite of our old backend built with a modern Python-based stack.</p> <p>Looking to access data on OpenML?</p> <p>If you simply want to access data stored on OpenML in a programmatic way, please have a look at connector packages in Python, Java, or R.</p> <p>If you are looking to interface directly with the REST API, and are looking for documentation on the REST API endpoints, visit the APIs page.</p> <p>This documentation is for developing and hosting your own OpenML REST API.</p>"},{"location":"apiv2/#development-roadmap","title":"Development Roadmap","text":"<p>First we will mimic current server functionality, relying on many implementation details present in the current production server. We will implement all endpoints using the SQL text queries based on PHP implementation, which should give near-identical responses to the current JSON endpoints. Minor exceptions are permitted but will be documented.</p> <p>At the same time we may also provide a work-in-progress \"new\" endpoint, but there won't be official support for it at this stage. After we verify the output of the endpoints are identical (minus any intentional documented differences), we will officially release the new API. The old API will remain available. After that, we can start working on a new version of the JSON API which is more standardized, leverages typing, and so on:</p> <ul> <li>Clean up the database: standardize value formats where possible (e.g., (un)quoting    contributor names in the dataset's contributor field), and add database level    constraints on new values.</li> <li>Redesign what the new API responses should look like and implement them,    API will be available to the public as it is developed.</li> <li>Refactor code-base to use ORM (using <code>SQLAlchemy</code>, <code>SQLModel</code>, or similar).</li> <li>Officially release the modernized API.</li> </ul> <p>There is no planned sunset date for the old API. This will depend on the progress with the new API as well as the usage numbers of the old API.</p>"},{"location":"apiv2/installation/","title":"Installation","text":"<p>Current instructions tested on Mac, but likely work on most Unix systems.</p> <p>The OpenML server will be developed and maintained for the latest minor release of Python (Python 3.12 as of writing). You can install the dependencies locally or work with docker containers.</p> Use <code>pyenv</code> to manage Python installations <p>We recommend using <code>pyenv</code> if you are working with multiple local Python versions. After following the installation instructions for <code>pyenv</code> check that you can execute it:</p> <pre><code>&gt; pyenv local\n3.12\n</code></pre> <p>If <code>pyenv</code> can't be found, please make sure to update the terminal environment (either by <code>reset</code>ing it, or by closing and opening the terminal). If you get the message <code>pyenv: no local version configured for this directory</code> first clone the repository as described below and try again from the root of the cloned repository.</p> <p>You can then install the Python version this project uses with: <code>cat .python-version | pyenv install</code></p>"},{"location":"apiv2/installation/#local-installation","title":"Local Installation","text":"<p>These instructions assume Python 3.12 and git are already installed.</p> <p>You may need to install Python3 and MySQL development headers.</p> <p>It may be necessary to first install additional headers before proceeding with a local installation of the <code>mysqlclient</code> dependency. They are documented under \"Installation\" of the <code>mysqlclient</code> documentation.</p> For UsersFor Contributors <p>If you don't plan to make code changes, you can install directly from Github. We recommend to install the OpenML server and its dependencies into a new virtual environment. Installing the project into a new virtual environment<pre><code>python -m venv venv\nsource venv/bin/activate\n\npython -m pip install git+https://github.com/openml/server-api.git\n</code></pre> If you do plan to make code changes, we recommend you follow the instructions under the \"For Contributors\" tab, even if you do not plan to contribute your changes back into the project.</p> <p>If you plan to make changes to this project, it will be useful to install the project from a cloned fork. To fork the project, go to our project page and click \"fork\". This makes a copy of the repository under your own Github account. You can then clone your own fork (replace <code>USER_NAME</code> with your Github username):</p> Cloning your fork<pre><code>git clone https://github.com/USER_NAME/server-api.git\ncd server-api\n</code></pre> <p>Then we can install the project into a new virtual environment in edit mode:</p> <p>Installing the project into a new virtual environment<pre><code>python -m venv venv\nsource venv/bin/activate\n\npython -m pip install -e \".[dev,docs]\"\n</code></pre> Note that this also installs optional dependencies for development and documentation tools. We require this for contributors, but we also highly recommend it anyone that plans to make code changes.</p>"},{"location":"apiv2/installation/#setting-up-a-database-server","title":"Setting up a Database Server","text":"<p>Depending on your use of the server, there are multiple ways to set up your own OpenML database. To simply connect to an existing database, see configuring the REST API Server below.</p>"},{"location":"apiv2/installation/#setting-up-a-new-database","title":"Setting up a new database","text":"<p>This sets up an entirely empty database with the expected OpenML tables in place. This is intended for new deployments of OpenML, for example to host a private OpenML server.</p> <p>Instructions are incomplete. See issue#78.</p>"},{"location":"apiv2/installation/#setting-up-a-test-database","title":"Setting up a test database","text":"<p>We provide a prebuilt docker image that already contains test data.</p> Docker ComposeDocker Run <p>To start the database through <code>docker compose</code>, run:</p> <pre><code>docker compose up database\n</code></pre> <p>which starts a database.</p> <p>To start a test database as stand-alone container, run:</p> <pre><code>docker run  --rm -e MYSQL_ROOT_PASSWORD=ok -p 3306:3306 -d --name openml-test-database openml/test-database:latest\n</code></pre> <p>You may opt to add the container to a network instead, to make it reachable from other docker containers:</p> <pre><code>docker network create openml\ndocker run  --rm -e MYSQL_ROOT_PASSWORD=ok -p 3306:3306 -d --name openml-test-database --network openml openml/test-database:latest\n</code></pre> <p>The container may take a minute to initialise, but afterwards you can connect to it. Either from a local <code>mysql</code> client at <code>127.0.0.1:3306</code> or from a docker container on the same network. For example:</p> <p><pre><code>docker run --network NETWORK --rm -it mysql mysql -hopenml-test-database -uroot -pok\n</code></pre> where <code>NETWORK</code> is <code>openml</code> when using <code>docker run</code> when following the example, and <code>NETWORK</code> is <code>server-api_default</code> if you used <code>docker compose</code> (specifically, it is <code>DIRECTORY_NAME</code> + <code>_default</code>, so if you renamed the <code>server-api</code> directory to something else, the network name reflects that).</p>"},{"location":"apiv2/installation/#configuring-the-rest-api-server","title":"Configuring the REST API Server","text":"<p>The REST API is configured through a TOML file.</p> <p>Instructions are incomplete. Please have patience while we are adding more documentation.</p>"},{"location":"apiv2/migration/","title":"Migration","text":"<p>The Python reimplementation provides the same endpoints as the old API, which are largely functioning the same way. However, there are a few major deviations:</p> <ul> <li>Use of typed JSON: e.g., when a value represents an integer, it is returned as integer.</li> <li>Lists when multiple values are possible: if a field can have none, one, or multiple entries (e.g., authors), we always return a list.</li> <li>Restriction or expansion of input types as appropriate.</li> <li>Standardizing authentication and access messages, and consistently execute those checks   before fetching data or providing error messages about the data.</li> </ul> <p>The list above is not exhaustive. Minor changes include, for example, bug fixes and the removal of unnecessary nesting. There may be undocumented changes, especially in edge cases which may not have occurred in the test environment. As the PHP API was underspecified, the re-implementation is based on a mix of reading old code and probing the API. If there is a behavioral change which was not documented but affects you, please open a bug report.</p>"},{"location":"apiv2/migration/#all-endpoints","title":"All Endpoints","text":"<p>The following changes affect all endpoints.</p>"},{"location":"apiv2/migration/#error-on-invalid-input","title":"Error on Invalid Input","text":"<p>When providing input of invalid types (e.g., a non-integer dataset id) the HTTP header and JSON content will be different.</p> HTTP Header<pre><code>- 412 Precondition Failed\n+ 422 Unprocessable Entity\n</code></pre> JSON Content<pre><code>- {\"error\":{\"code\":\"100\",\"message\":\"Function not valid\"}}\n+ {\"detail\":[{\"loc\":[\"query\",\"_dataset_id\"],\"msg\":\"value is not a valid integer\",\"type\":\"type_error.integer\"}]}\n</code></pre> <p>Input validation has been added to many end points</p> <p>There are endpoints which previously did not do any input validation.    These endpoints now do enforce stricter input constraints.    Constraints for each endpoint parameter are documented in the API docs.</p>"},{"location":"apiv2/migration/#other-errors","title":"Other Errors","text":"<p>For any other error messages, the response is identical except that outer field will be <code>\"detail\"</code> instead of <code>\"error\"</code>:</p> JSON Content<pre><code>- {\"error\":{\"code\":\"112\",\"message\":\"No access granted\"}}\n+ {\"detail\":{\"code\":\"112\",\"message\":\"No access granted\"}}\n</code></pre> <p>In some cases the JSON endpoints previously returned XML (example), the new API always returns JSON.</p> XML replaced by JSON<pre><code>- &lt;oml:error xmlns:oml=\"http://openml.org/openml\"&gt;\n-   &lt;oml:code&gt;103&lt;/oml:code&gt;\n-   &lt;oml:message&gt;Authentication failed&lt;/oml:message&gt;\n- &lt;/oml:error&gt;\n+ {\"detail\": {\"code\":\"103\", \"message\": \"Authentication failed\" } }\n</code></pre>"},{"location":"apiv2/migration/#datasets","title":"Datasets","text":""},{"location":"apiv2/migration/#get-dataset_id","title":"<code>GET /{dataset_id}</code>","text":"<ul> <li>Dataset format names are normalized to be all lower-case    (<code>\"Sparse_ARFF\"</code> -&gt;  <code>\"sparse_arff\"</code>).</li> <li>Non-<code>arff</code> datasets will not incorrectly have a <code>\"parquet_url\"</code> (openml#1189).</li> <li>If <code>\"creator\"</code> contains multiple comma-separated creators it is always returned    as a list, instead of it depending on the quotation used by the original uploader.</li> <li>For (some?) datasets that have multiple values in <code>\"ignore_attribute\"</code>, this field    is correctly populated instead of omitted.</li> <li>Processing date is formatted with a <code>T</code> in the middle:   processing_date<pre><code>- \"2019-07-09 15:22:03\"\n+ \"2019-07-09T15:22:03\"\n</code></pre></li> <li>Fields which may contain lists of values (e.g., <code>creator</code>, <code>contributor</code>) now always   returns a list (which may also be empty or contain a single element).</li> <li>Fields without a set value are no longer automatically removed from the response.</li> </ul>"},{"location":"apiv2/migration/#get-datalistfilters","title":"<code>GET /data/list/{filters}</code>","text":"<p>The endpoint now accepts the filters in the body of the request, instead of as query parameters. <pre><code>-  curl -d '' 127.0.0.1:8002/api/v1/json/data/list/status/active\n+ curl -X 'POST' 'http://localhost:8001/v1/datasets/list' \\\n+  -H 'Content-Type: application/json' \\\n+  -d '{}'\n</code></pre> This endpoint is now also available via a <code>POST</code> request, and will exhibit the same behavior regardless of how it is accessed.</p> <p>When accessing this endpoint when authenticated as administrator, it now correctly includes datasets which are private.</p> <p>The <code>limit</code> and <code>offset</code> parameters can now be used independently, you no longer need to provide both if you wish to set only one.</p>"},{"location":"apiv2/migration/#post-datasetstag","title":"<code>POST /datasets/tag</code>","text":"<p>When successful, the \"tag\" property in the returned response is now always a list, even if only one tag exists for the entity. For example, after tagging dataset 21 with the tag <code>\"foo\"</code>: <pre><code>{\n   data_tag\": {\n      \"id\": \"21\",\n-      \"tag\": \"foo\"\n+      \"tag\": [\"foo\"]\n   }\n}\n</code></pre></p>"},{"location":"apiv2/migration/#studies","title":"Studies","text":""},{"location":"apiv2/migration/#get-id_or_alias","title":"<code>GET /{id_or_alias}</code>","text":"<p>Old-style \"legacy\" studies which are solely based on tags are no longer supported.</p> Affected Legacy Studies <p>Only 24 old studies were affected by this change, listed below. There is currently not yet a migration plan for these studies.</p> id name 1 A large-scale comparison of classification algorit... 2 Fast Algorithm Selection using Learning Curves 3 Multi-Task Learning with a Natural Metric for Quan... 5 Local and Global Feature Selection on Multilabel T... 7 Massive machine learning experiments using mlr and... 8 Decision tree comparaison 10 Collaborative primer 11 Having a Blast: Meta-Learning and Heterogeneous En... 12 Subspace Clustering via Seeking Neighbors with Min... 13 Meta-QSAR: learning how to learn QSARs 17 Subgroup Discovery 20 Mythbusting data mining urban legends through larg... 22 Identifying critical paths in undergraduate progra... 24 OpenML R paper 25 Bernd Demo Study for Multiclass SVMs OML WS 2016 27 Compare three different SVM versions of R package ... 30 OpenML Paper Study 31 Iris Data set Study 32 Data Streams and more 34 Massively Collaborative Machine Learning 37 Speeding up Algorithm Selection via Meta-learning ... 38 Performance of new ctree implementations on classi... 41 ASLib OpenML Scenario 50 Hyper-parameter tuning of Decision Trees 51 ensemble on diabetes"},{"location":"apiv2/migration/#flows","title":"Flows","text":""},{"location":"apiv2/migration/#get-flowexistsnameexternal_version","title":"<code>GET /flow/exists/{name}/{external_version}/</code>","text":"<p>Behavior has changed slightly. When a flow is found:</p> <pre><code>- { \"flow_exists\": { \"exists\": \"true\", \"flow_id\": \"123\" } }\n+ { \"flow_id\": 123 }\n</code></pre> <p>and when a flow is not found: <pre><code>- { \"flow_exists\": { \"exists\": \"false\", \"flow_id\": \"-1\" } }\n+ { \"detail\": \"Flow not found.\" }\n</code></pre> and the HTTP header status code is <code>404</code> (NOT FOUND) instead of <code>200</code> (OK).</p> <p>In the future the successful case will more likely just return the flow immediately instead (see #170).</p>"},{"location":"apiv2/migration/#others","title":"Others","text":""},{"location":"apiv2/migration/#get-estimationprocedurelist","title":"<code>GET /estimationprocedure/list</code>","text":"<p>The <code>ttid</code> field has been renamed to <code>task_type_id</code>. All values are now typed. Outer levels of nesting have been removed.</p>"},{"location":"apiv2/migration/#get-evaluationmeasureslist","title":"<code>GET /evaluationmeasures/list</code>","text":"<p>Outer levels of nesting have been removed.</p>"},{"location":"apiv2/contributing/","title":"Contributing","text":"<p>Like many other open source projects, contributions from the community are an essential piece in making OpenML successful. We very much appreciate that you want to make a contribution. To try to make this as smooth as possible for everyone involved, we wrote this contribution guide. There are many kinds of contributions, such as bug reports, participating in discussions, making code contributions, or updating documentation.</p>"},{"location":"apiv2/contributing/#bug-reports","title":"Bug reports","text":"<p>When you encounter an issue using or deploying the Python-based REST API, the first step is to search our issue tracker to see if others have already reported the issue. If this is the case, please indicate that you encountered the issue by leaving a  on the issue. Feel free to leave a comment if you have additional information that may be useful to track down or fix the bug.</p> <p>If the bug has not been reported before, please open a new issue to report it. When you open a bug report, please include:</p> <ul> <li>a minimal reproducible example,</li> <li>a description of the expected behavior,</li> <li>a description of the encountered behavior,</li> <li>and any additional information that you think may be relevant, such as which environment you encountered it in, or when you first encountered the bug.</li> </ul>"},{"location":"apiv2/contributing/#code","title":"Code","text":"<p>If you want to make code contributions, please first make sure there is an open issue that describes what the contribution should look like. This may be a proposal that you write yourself, or an existing open issue that you want to help us with. Please indicate on that issue that you would like to contribute to it. This way, we can ensure that everything is clear, it's not out of date, and \"officially\" assign you to the issue so that others know its being worked on. Making sure there is a clear description and clear assignment helps prevent a situation where someone makes a large contribution that is unwanted, or is simultaneously developed by someone else. With an issue assigned, please head over to the \"Development\" section.</p>"},{"location":"apiv2/contributing/#documentation","title":"Documentation","text":"<p>For minor fixes, it's fine to make the changes and submit them through a pull request. For larger changes, please make sure you are assigned to an issue first as is described in the \"Code\" section of this page. Then, visit the \"Documentation\" page to learn how to contribute documentation changes.</p>"},{"location":"apiv2/contributing/#other","title":"Other","text":"<p>If you are looking to contribute to OpenML in general, visit \"Contributing\" on the OpenML website.</p>"},{"location":"apiv2/contributing/contributing/","title":"Setting up the development environment","text":"<p>First, follow the installation instructions for contributors to install a local fork with optional development dependencies. Stop when you reach the section \"Setting up a Database Server\".</p>"},{"location":"apiv2/contributing/contributing/#pre-commit","title":"Pre-commit","text":"<p>We use <code>pre-commit</code> to ensure certain tools and checks are ran before each commit. These tools perform code-formatting, linting, and more. This makes the code more consistent across the board, which makes it easier to work with each other's code and also can catch common errors. After installing it, it will automatically run when you try to make a commit. Install it now and verify that all checks pass out of the box:</p> <p>Install pre-commit and verify it works<pre><code>pre-commit install\npre-commit run --all-files\n</code></pre> Running the tool the first time may be slow as tools need to be installed, and many tools will build a cache so subsequent runs are faster. Under normal circumstances, running the pre-commit chain shouldn't take more than a few seconds.</p>"},{"location":"apiv2/contributing/contributing/#docker","title":"Docker","text":"<p>With the projected forked, cloned, and installed, the easiest way to set up all required services for development is through <code>docker compose</code>.</p>"},{"location":"apiv2/contributing/contributing/#starting-containers","title":"Starting containers","text":"<pre><code>docker compose up\n</code></pre> <p>This will spin up 4 containers, as defined in the <code>docker-compose.yaml</code> file:</p> <ul> <li><code>openml-test-database</code>: this is a mysql database prepopulated with test data.     It is reachable from the host machine with port <code>3306</code>, by default it is configured     to have a root user with password <code>\"ok\"</code>.</li> <li><code>server-api-docs-1</code>: this container serves project documentation at <code>localhost:8000</code>.     These pages are built from the documents in the <code>docs/</code> directory of this repository,     whenever you edit and save a file there, the page will immediately be updated.</li> <li><code>server-api-php-api-1</code>: this container serves the old PHP REST API at <code>localhost:8002</code>.     For example, visit http://localhost:8002/api/v1/json/data/1     to fetch a JSON description of dataset 1.</li> <li><code>python-api</code>: this container serves the new Python-based REST API at <code>localhost:8001</code>.     For example, visit http://localhost:8001/docs to see     the REST API documentation. Changes to the code in <code>src/</code> will be reflected in this     container.</li> </ul> <p>You don't always need every container, often just having a database and the Python-based REST API may be enough. In that case, only specify those services:</p> <pre><code>docker compose up database python-api\n</code></pre> <p>Refer to the <code>docker compose</code> documentation for more uses.</p>"},{"location":"apiv2/contributing/contributing/#connecting-to-containers","title":"Connecting to containers","text":"<p>To connect to a container, run:</p> <pre><code>docker exec -it CONTAINER_NAME /bin/bash\n</code></pre> <p>where <code>CONTAINER_NAME</code> is the name of the container. If you are unsure of your container name, then <code>docker container ls</code> may help you find it. Assuming the default container names are used, you may connect to the Python-based REST API container using:</p> <pre><code>docker exec -it python-api /bin/bash\n</code></pre> <p>This is useful, for example, to run unit tests in the container:</p> <pre><code>python -m pytest -x -v -m \"not web\"\n</code></pre>"},{"location":"apiv2/contributing/contributing/#unit-tests","title":"Unit tests","text":"<p>Our unit tests are written with the <code>pytest</code> framework. An invocation could look like this:</p> <pre><code>python -m pytest -v -x --lf -m \"not web\"\n</code></pre> <p>Where <code>-v</code> show the name of each test ran, <code>-x</code> ensures testing stops on first failure, <code>--lf</code> will first run the test(s) which failed last, and <code>-m \"not web\"</code> specifies which tests (not) to run.</p> <p>The directory structure of our tests follows the structure of the <code>src/</code> directory. For files, we follow the convention of appending <code>_test</code>. Try to keep tests as small as possible, and only rely on database and/or web connections when absolutely necessary.</p> <p>Instructions are incomplete. Please have patience while we are adding more documentation.</p>"},{"location":"apiv2/contributing/contributing/#yaml-validation","title":"YAML validation","text":"<p>The project contains various <code>yaml</code> files, for example to configure <code>mkdocs</code> or to describe Github workflows. For these <code>yaml</code> files we can configure automatic schema validation, to ensure that the files are valid without having to run the server. This also helps with other IDE features like autocomplete. Setting this up is not required, but incredibly useful if you do need to edit these files. The following <code>yaml</code> files have schemas:</p> File(s) Schema URL mkdocs.yml https://squidfunk.github.io/mkdocs-material/schema.json .pre-commit-config.yaml https://json.schemastore.org/pre-commit-config.json .github/workflows/*.yaml https://json.schemastore.org/github-workflow PyCharmVSCode <p>In PyCharm, these can be configured from <code>settings</code> &gt; <code>languages &amp; frameworks</code> &gt; <code>Schemas and DTDs</code> &gt; <code>JSON Schema Mappings</code>. There, add mappings per file or file pattern.</p> <p>In VSCode, these can be configured from <code>settings</code> &gt; <code>Extetions</code> &gt; <code>JSON</code> &gt; <code>Edit in settings.json</code>. There, add mappings per file or file pattern. For example:</p> <pre><code>\"json.schemas\": [\n   {\n         \"fileMatch\": [\n            \"/myfile\"\n         ],\n         \"url\": \"schemaURL\"\n   }\n\n]\n</code></pre>"},{"location":"apiv2/contributing/contributing/#connecting-to-another-database","title":"Connecting to another database","text":"<p>In addition to the database setup described in the installation guide, we also host a database on our server which may be connected to that is available to OpenML core contributors. If you are a core contributor and need access, please reach out to one of the engineers in Eindhoven.</p> <p>Instructions are incomplete. Please have patience while we are adding more documentation.</p>"},{"location":"apiv2/contributing/documentation/","title":"Documentation with mkdocs-material","text":"<p>Our documentation is built using mkdocs and the mkdocs-material theme. We also use some plugins. Please refer to the \"Getting Started\" pages of <code>mkdocs-material</code> for a general overview on how to work with <code>mkdocs-material</code>. All documentation files are in the <code>docs/</code> folder, except for the configuration file which is <code>mkdocs.yml</code> at the root of the repository.</p> <p>For minor changes, it should be fine to edit the page directly on Github. That should commit to a separate branch (or fork), and you can set up a pull request. For larger changes, clone a fork of the repository as described in the \"Local Installation\" section.</p> DockerLocal installation <p>After cloning the repository, you may also build and serve the documentation through Docker:</p> <pre><code>docker compose up docs\n</code></pre> <p>Instead of installing all dependencies (with <code>python -m pip install -e \".[docs]\"</code>), you may also install just the documentation dependencies:</p> <pre><code>python -m pip install mkdocs-material mkdocs-section-index\n</code></pre> <p>You can then build and serve the documentation with</p> <pre><code>python -m mkdocs serve\n</code></pre> <p>This will serve the documentation from the <code>docs/</code> directory to http://localhost:8000/. Any updates you make to files in that directory will be reflected on the website. When you are happy with your changes, just commit and set up a pull request!</p>"},{"location":"apiv2/contributing/project_overview/","title":"Project overview","text":"<p>The Python-based REST API serves several groups of endpoints:</p> <ul> <li><code>/old/</code>: serves the old-style JSON format, this should mimic the PHP responses exactly with the only deviations recorded in the migration guide.</li> <li><code>/mldcat_ap/</code>: serves datasets in MLDCAT_AP format.</li> <li><code>/*</code>: serves new-style JSON format. At this point it is intentionally similar to the old-style format.</li> </ul> <p>The endpoints are specified in subdirectories of <code>src/routers</code>. They pull data from the database through the <code>src/database</code> module. The schemas for each entity, and possible conversions between them, are defined in the <code>src/schemas</code> directory.</p> <p>Instructions are incomplete. Please have patience while we are adding more documentation.</p>"},{"location":"apiv2/contributing/tests/","title":"Writing Tests","text":"<p>tl;dr:  - Setting up the <code>py_api</code> fixture to test directly against a REST API endpoint is really slow, only use it for migration/integration tests.  - Getting a database fixture and doing a database call is slow, consider mocking if appropriate.</p>"},{"location":"apiv2/contributing/tests/#overhead-from-fixtures","title":"Overhead from Fixtures","text":"<p>Sometimes, you want to interact with the REST API through the <code>py_api</code> fixture, or want access to a database with <code>user_test</code> or <code>expdb_test</code> fixtures. Be warned that these come with considerable relative overhead, which adds up when running thousands of tests.</p> <pre><code>@pytest.mark.parametrize('execution_number', range(5000))\ndef test_private_dataset_owner_access(\n        execution_number,\n        expdb_test: Connection,\n        user_test: Connection,\n        py_api: TestClient,\n) -&gt; None:\n    fetch_user(ApiKey.REGULAR_USER, user_test)  # accesses only the user db\n    get_estimation_procedures(expdb_test)  # accesses only the experiment db\n    py_api.get(\"/does/not/exist\")  # only queries the api\n    pass\n</code></pre> <p>When individually adding/removing components, we measure (for 5000 repeats, n=1):</p> expdb user api exp call user call api get time (s) \u274c \u274c \u274c \u274c \u274c \u274c 1.78 \u2705 \u274c \u274c \u274c \u274c \u274c 3.45 \u274c \u2705 \u274c \u274c \u274c \u274c 3.22 \u274c \u274c \u2705 \u274c \u274c \u274c 298.48 \u2705 \u2705 \u274c \u274c \u274c \u274c 4.44 \u2705 \u2705 \u2705 \u274c \u274c \u274c 285.69 \u2705 \u274c \u274c \u2705 \u274c \u274c 4.91 \u274c \u2705 \u274c \u274c \u2705 \u274c 5.81 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 307.91 <p>Adding a fixture that just returns some value adds only minimal overhead (1.91s), so the burden comes from establishing the database connection itself.</p> <p>We make the following observations:</p> <ul> <li>Adding a database fixture adds the same overhead as instantiating an entirely new test.</li> <li>Overhead of adding multiple database fixtures is not free</li> <li>The <code>py_api</code> fixture adds two orders of magnitude more overhead</li> </ul> <p>We want our tests to be fast, so we want to avoid using these fixtures when we reasonably can. We restrict usage of <code>py_api</code> fixtures to integration/migration tests, since it is very slow. These only run on CI before merges. For database fixtures</p> <p>We will write some fixtures that can be used to e.g., get a <code>User</code> without accessing the database. The validity of these users will be tested against the database in only a single test.</p>"},{"location":"apiv2/contributing/tests/#mocking","title":"Mocking","text":"<p>Mocking can help us reduce the reliance on database connections in tests. A mocked function can prevent accessing the database, and instead return a predefined value instead.</p> <p>It has a few upsides:  - It's faster than using a database fixture (see below).  - The test is not dependent on the database: you can run the test without a database.</p> <p>But it also has downsides:  - Behavior changes in the database, such as schema changes, are not automatically reflected in the tests.  - The database layer (e.g., queries) are not actually tested.</p> <p>Basically, the mocked behavior may not match real behavior when executed on a database. For this reason, for each mocked entity, we should add a test that verifies that if the database layer is invoked with the database, it returns the expected output that matches the mock. This is additional overhead in development, but hopefully it pays back in more granular test feedback and faster tests.</p> <p>On the speed of mocks, consider these two tests:</p> <p><pre><code>@pytest.mark.parametrize('execution_number', range(5000))\ndef test_private_dataset_owner_access(\n        execution_number,\n        admin,\n+        mocker,\n-        expdb_test: Connection,\n) -&gt; None:\n+    mock = mocker.patch('database.datasets.get')\n+    class Dataset(NamedTuple):\n+        uploader: int\n+        visibility: Visibility\n+    mock.return_value = Dataset(uploader=1, visibility=Visibility.PRIVATE)\n\n    _get_dataset_raise_otherwise(\n        dataset_id=1,\n        user=admin,\n-        expdb=expdb_test,\n+        expdb=None,\n    )\n</code></pre> There is only a single database call in the test. It fetches a record on an indexed field and does not require any joins. Despite the database call being very light, the database-included test is ~50% slower than the mocked version (3.50s vs 5.04s).</p>"}]}